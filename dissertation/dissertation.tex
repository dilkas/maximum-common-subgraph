\pdfoutput=1
\documentclass{l4proj}

\usepackage[UKenglish]{babel}
\usepackage[UKenglish]{isodate}
\usepackage[hidelinks]{hyperref}
\usepackage{amsfonts, amsmath, amsthm, graphicx, subcaption}
\usepackage[group-separator={,}]{siunitx}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\theoremstyle{remark}
\newtheorem{remark}{Remark}[section]
\DeclareMathOperator{\dom}{dom}

\title{Algorithm Selection for Maximum Common Subgraph}
\author{Paulius Dilkas}
%\date{}

\begin{document}
\maketitle

%\begin{abstract}
%\end{abstract}

\educationalconsent
\tableofcontents

\chapter{Introduction}
\pagenumbering{arabic}
% TODO: write a proper introduction (and abstract)
\begin{definition}
  An undirected \emph{multigraph} is a pair $(V, E)$, where $V$ is a set of
  vertices and $E$ is a set of edges, together with a map $E \to V \cup V^2$,
  which assigns one or two vertices to each edge
  \cite{DBLP:books/daglib/0030488}. If an edge is assigned to a single vertex,
  it is called a \emph{loop}. When several edges map to the same pair of
  vertices, they are referred to as \emph{multiple edges}. 
\end{definition}

We will refer to undirected multigraphs simply as graphs. The following 3
definitions are adapted from \cite{DBLP:journals/jcamd/RaymondW02a} for
multigraphs.

\begin{definition}
  Two graphs $G_1 = (V_1, E_1)$ and $G_2 = (V_2, E_2)$ are said to be
  \emph{isomorphic} if there is a bijection $f \colon V_1 \to V_2$ such that
  for all $v \in V_1 \cup V_1^2$, the number of edges in $E_1$ that are mapped
  to $v$ is equal to the number of edges in $E_2$ that are mapped to $f(v)$,
  where $f(v) = (f(v_1), f(v_2))$ if $v$ is a tuple $(v_1, v_2) \in V_1^2$.
\end{definition}

\begin{definition} \label{def:induced_subgraph}
  An \emph{induced subgraph} of a graph $G = (V, E)$ is a graph $H = (S, E')$,
  where $S \subseteq V$ is a set of vertices and $E' \subseteq E$ is a set of
  edges that are mapped to $S \cup S^2$.
\end{definition}

\begin{definition}
  A \emph{maximum common (induced) subgraph} between graphs $G_1$ and $G_2$ is a
  graph $G_3 = (V_3, E_3)$ such that $G_3$ is isomorphic to induced subgraphs of
  both $G_1$ and $G_2$ with $|V_3|$ maximised. The \emph{maximum common
    (induced) subgraph problem} is the problem of finding a maximum common
  subgraph between two given graphs $G_1 = (V_1, E_1)$ and $G_2 = (V_2, E_2)$,
  usually expressed as a bijection between two subsets of vertices $U_1
  \subseteq V_1$ and $U_2 \subseteq V_2$.
\end{definition}

Similarly to Definition \ref{def:induced_subgraph}, we define a more general
notion of a subgraph.

\begin{definition} A graph $G_2 = (V_2, E_2)$ is a \emph{subgraph} of graph $G_1
  = (V_1, E_1)$ if $V_2 \subseteq V_1$ and $E_2 \subseteq E_1$
  \cite{DBLP:books/daglib/0030488}.
\end{definition}

In this paper we will be dealing with the maximum common induced subgraph
problem defined for undirected multigraphs, even though most of the benchmark
instances do not have multiple edges. We will also refer to a closely related
graph problem, the subgraph isomorphism problem.

\begin{definition}
  Given two (finite) graphs $G_1$ and $G_2$, the \emph{subgraph isomorphism
    problem} is the decision problem of determining whether $G_1$ is isomorphic
  to a subgraph of $G_2$ \cite{DBLP:conf/stoc/Cook71}.
\end{definition}

The purpose of this project is to create and explore machine-learning-based
algorithm \emph{portfolios}, i.e., machine learning models whose main goal is to
provide a mapping, assigning the best-performing algorithm to each problem
instance \cite{DBLP:journals/ai/BischlKKLMFHHLT16, DBLP:journals/ac/Rice76}.

\chapter{The Algorithms}
The clique encoding \cite{DBLP:conf/cp/McCreeshNPS16} solves the maximum common
subgraph problem by creating a new (association) graph and transforming the
problem into an instance of maximum clique, which is then solved by a sequential
version of the maximum clique solver by McCreesh and Prosser
\cite{DBLP:journals/topc/McCreeshP15}, which is a branch and bound algorithm
that uses bitsets and greedy colouring. Colouring is used to provide a quick
upper bound: if a subgraph can be coloured with $k$ colours, then it cannot have
a clique of size more than $k$.

$k\downarrow$ algorithm \cite{DBLP:conf/aaai/HoffmannMR17} starts by trying to
solve the subgraph isomorphism problem, i.e. finding the pattern graph in the
target graph. If that fails, it allows a single vertex of the pattern graph to
not match any of the target graph vertices and tries again, allowing smaller and
smaller pattern graphs until it finds a solution. The number of vertices of the
pattern graph that are allowed this additional freedom is represented by $k$.
More specifically, the algorithm creates a domain for each pattern graph vertex,
which initially includes all vertices of the target graph and $k$ wildcards. The
domains are filtered with various propagation techniques. Then the search begins
with a smallest domain (not counting wildcards), a value is chosen, and domains
are filtered again to eliminate the chosen value.

\textsc{McSplit} \cite{DBLP:conf/ijcai/McCreeshPT17} is a branch and bound
algorithm that builds its own bit string labels for vertices in both pattern and
target graphs. Once it chooses to match a vertex $u$ in graph $G_1$ with a
vertex $v$ in graph $G_2$, it iterates over all unmatched vertices in both
graphs, adding a 1 to their labels if they are adjacent to $u$ or $v$ and 0
otherwise. That way a vertex can only be matched with vertices that have the
same labels. The labels are also used in the upper bound heuristic function
using the rule that if a particular label is assigned to $m$ vertices in $G_1$
and $n$ vertices in $G_2$, then up to $\min \{ m, n \}$ pairs can be matched for
that label.

$\textsc{McSplit} \downarrow$ is a variant of \textsc{McSplit} mentioned but not
explained in the original paper \cite{DBLP:conf/ijcai/McCreeshPT17}. It is meant
to be similar to $k\downarrow$ in that it starts by trying to find a subgraph
isomorphism and keeps decreasing the size of common subgraphs that it is
interested in until a solution is found. Based on the source
code\footnote{\url{https://github.com/jamestrimble/ijcai2017-partitioning-common-subgraph/blob/master/code/james-cpp/mcsp.c}},
there are a few key differences between $\textsc{McSplit} \downarrow$ and
\textsc{McSplit}:
\begin{itemize}
\item Instead of always looking for larger and larger common subgraphs, we have
  a goal size and exit early if a common subgraph of that size is found.
\item The goal size is decreased if the search finishes without a solution.
\item Having a big goal size allows the heuristic to be more selective and prune
  more of the search tree branches.
\end{itemize}

\chapter{Problem Instances} \label{chapter:problems}
We use two graph databases that contain a large variety of graphs differing in
size, various characteristics, and the way they were generated. The
\textsc{McSplit} paper \cite{DBLP:conf/ijcai/McCreeshPT17} used the same
data to compare these (and a few constraint programming) algorithms and
found \textsc{McSplit} to win with unlabelled graphs described in Section
\ref{sec:labelled} of this paper, the clique encoding to win with labelled
graphs, and $\textsc{McSplit}\downarrow$ to win with unlabelled graphs from
Section \ref{sec:unlabelled}. However, in some cases the difference in
performance between \textsc{McSplit} and the clique encoding or between
$\textsc{McSplit}\downarrow$ and $k\downarrow$ was very small.

\section{Labelled Graphs} \label{sec:labelled}
All of the labelled graphs are taken from the ARG Database \cite{foggia2001-2,
  DBLP:journals/prl/SantoFSV03}, which is a large collection of graphs for
benchmarking various graph-matching algorithms. The graphs are generated using
several algorithms:

\begin{itemize}
\item randomly generated,
\item 2D, 3D, and 4D meshes,
\item and bounded valence graphs.
\end{itemize}

Furthermore, each algorithm is executed with several (3--5) different parameter
values. The database includes 81400 pairs of labelled graphs. Their unlabelled
versions are used as well.

\subsection{Characteristics of Graph Labelling}
For the purposes of this paper, we look at two types of labelled graphs: those
that have their vertices labelled and those that have both vertices and edges
labelled. We define them as follows (the definitions are loosely inspired by
\cite{abu-aisheh_2016}):

\begin{definition}
  A graph $G = (V, E)$ is a \emph{(vertex) labelled graph} if it has an associated
  vertex labelling function $\mu \colon V \to \{ 0, \dots, N - 1 \}$ for some $N
  \in \{2, \dots, |V| \}$.
\end{definition}

\begin{definition}
  A graph $G = (V, E)$ is a \emph{fully labelled graph} if it is a vertex labelled
  graph and it has an associated edge labelling function $\zeta \colon E \to
  \{ 0, \dots, M - 1 \}$ for some $M \in \{ 2, \dots, |E| \}$.
\end{definition}

Specifically, note that:

\begin{itemize}
\item If a graph is labelled, then all its vertices (and possibly edges) are
  assigned a label.
\item We are only considering finite sets of labels, represented by non-negative integers.
\end{itemize}

Now we need a way to choose $N$ and $M$. For that we formally define how
labelling is implemented in the ARG database:

\begin{definition} \label{def:percent_labelling}
  A graph $G = (V, E)$ is said to have a \emph{$p\%$ (vertex) labelling} if
  \[ N = \max \left\{ 2^n : n \in \mathbb{N},\, 2^n < \left\lfloor \frac{p}{100\%}
        \times |V| \right\rfloor \right\}. \]
\end{definition}

The default value for $p$ is 33\%.

\begin{figure}
  \centering
  \includegraphics[scale=0.5]{images/labelling_histogram.png}
  \caption{Histogram of the difference between the expected number of vertices
    assigned each label and the actual number (for all labelled graphs)}
  \label{figure:labelling_histogram}
\end{figure}

The publications associated with the database \cite{foggia2001-2, DBLP:journals/prl/SantoFSV03}
say nothing about how the labels are distributed among the $N$ values. We
calculate the number of vertices that were assigned each label for each graph
(represented by $C$) and compare those values with the numbers we would expect
from a uniform distribution (represented by $E(C)$). We plot a histogram of the
difference $E(C) - C$ in Figure \ref{figure:labelling_histogram} and observe
that the difference is normally distributed around 0.

\section{Unlabelled Graphs} \label{sec:unlabelled}
We also include a collection of benchmark instances for the subgraph isomorphism
problem\footnote{\url{http://liris.cnrs.fr/csolnon/SIP.html}} (with the
biochemical reactions dataset excluded since we are not dealing with directed
graphs). It contains only unlabelled graphs and consists of the following sets:

\begin{description}
\item[images-CVIU11] Graphs generated from segmented images. 43 pattern graphs
  and 146 target graphs, giving a total of \num{6278} instances.
\item[meshes-CVIU11] Graphs generated from meshes modelling 3D
  objects. 6 pattern graphs and 503 target graphs, giving a total of \num{3018}
  instances. Both \texttt{images-CVIU11} and \texttt{meshes-CVIU11} datasets are
  described in \cite{DBLP:journals/cviu/DamiandSHJS11}.
\item[images-PR15] Graphs generated from segmented images
  \cite{DBLP:journals/pr/SolnonDHJ15}. 24 pattern graphs and a single target
  graph, giving 24 instances.
\item[LV] Graphs with various properties (connected, biconnected, triconnected,
  bipartite, planar, etc.). 49 graphs are paired up in all possible ways, giving
  $49^2=\num{2401}$ instances.
\item[scalefree] Scale-free networks generated using a power law distribution of
  degrees (100 instances).
\item[si] Bounded valence graphs, 4D meshes, and randomly generated graphs
  (\num{1170} instances). This is the unlabelled part of the ARG database.
  \texttt{LV}, \texttt{scalefree}, and \texttt{si} datasets are described in
  \cite{DBLP:journals/ai/Solnon10, DBLP:journals/constraints/ZampelliDS10}.
\item[phase] Random graphs generated to be close to the
  satisfiable-unsatisfiable phase transition (200 instances)
  \cite{DBLP:conf/ijcai/McCreeshPT16}.
\item[largerGraphs] Larger instances of the \texttt{LV} dataset. There are 70
  graphs, giving $70^2=\num{4900}$ instances. The separation was made and used
  in \cite{DBLP:conf/aaai/HoffmannMR17, DBLP:conf/lion/KotthoffMS16,
    DBLP:conf/ijcai/McCreeshPT17}.
\end{description}

\begin{remark}
This set of instances was taken from the
repository\footnote{\url{https://github.com/jamestrimble/ijcai2017-partitioning-common-subgraph}}
for the \textsc{McSplit} paper \cite{DBLP:conf/ijcai/McCreeshPT17} and has some
minor differences from the version on Christine Solnon's website.
\end{remark}

\begin{remark}
  Since $k\downarrow$ comes from the subgraph isomorphism problem background, it
  treats the two (pattern and target) graphs differently. Therefore, when graphs
  are not divided into patterns and targets, we run the algorithms with both
  orders ($(G_1, G_2)$ and $(G_2, G_1)$).
\end{remark}
% TODO: mention algorithm portfolios, SAT, constraint solving, TSP, AI planning

\chapter{Generating Data}
A machine learning (ML) model requires data to learn from. We are using an R
package called \textsc{Llama} \cite{kotthoff_llama_2013}, which helps to train
and evaluate ML models in order to compare algorithms and was used to create
algorithm portfolios for the travelling salesperson problem
\cite{DBLP:conf/lion/KotthoffKHT15} and the subgraph isomorphism problem
\cite{DBLP:conf/lion/KotthoffMS16}. First, we run each algorithm on all pairs of
pattern-target graphs and record the running times (described in Section
\ref{sec:runtimes}). Then, we adapt a graph feature extractor program used in
\cite{DBLP:conf/lion/KotthoffMS16} to handle the binary format of the ARG
Database \cite{foggia2001-2, DBLP:journals/prl/SantoFSV03}, run it on all
graphs, and record the features in a way described in Section
\ref{sec:features}.

\section{Running Time of Algorithms} \label{sec:runtimes}

The algorithms were compiled with gcc 6.3.0 and run on Intel Xeon E5-2697A v4
(2.60 GHz) processors with a \num{1000} s time limit. A Makefile was created to
run multiple experiments in parallel with, e.g., \texttt{make -j 64}, which
generates pairs of graph filenames for all datasets, runs the selected
algorithms with various command line arguments, redirects their output to files
that are later parsed using \texttt{sed} and regular expressions into the CSV
format. For each algorithm, we keep the full names of pattern and target graphs,
the number of vertices in the returned maximum common subgraph, running time as
reported by the algorithms themselves, and the number of explored nodes in the
search tree. Entries with running time greater than or equal to the timeout
value are considered to have timed out. The aforementioned node counts are
collected but not currently used. Afterwards, the answers of different
algorithms are checked for equality (for algorithms that did not time out).

The clique algorithm requires $O(n^2m^2)$ memory for a pair of graphs with $n$
amd $m$ vertices \cite{DBLP:conf/aaai/HoffmannMR17, DBLP:conf/cp/McCreeshNPS16}.
To avoid segmentation faults, its virtual memory usage was limited to 7 GB with
\texttt{ulimit -v} and the instances from Section \ref{sec:unlabelled} (which
contain much larger graphs) were restricted to $m \times n < \num{16000}$.

$k\downarrow$ was further modified to accept graphs with vertex labels by adding
an additional constraint for matching labels on line 8 of the
\texttt{klessSubgraphIsomorphism} function \cite{DBLP:conf/aaai/HoffmannMR17}.

In the rest of this section we explore and compare how the algorithms performed
on the three different subproblems under consideration, those of having no
labels, vertex labels, and both vertex and edge labels. We introduce
\emph{empirical cumulative distribution function (ECDF)}
plots \cite{10.2307/2334448}: for each unit of time on the $x$ axis, the value on
the $y$ axis represents what part of the problem instances was solved in that
amount of time or less.
% TODO: mention how many instances were solved by at least one algorithm in each dataset.

\subsection{Unlabelled Graphs}

\begin{figure} % NOTE: updated with MCS data
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[scale=0.7]{images/ecdf_mcs.png}
    \caption{Data from Section \ref{sec:labelled}, the ARG Database}
    \label{fig:ecdf_unlabelled_mcs}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[scale=0.7]{images/ecdf_sip.png}
    \caption{Data from Section \ref{sec:unlabelled}}
    \label{fig:ecdf_unlabelled_sip}
  \end{subfigure}
  \begin{subfigure}[t]{\textwidth}
    \centering
    \includegraphics[scale=0.7]{images/ecdf_unlabelled.png}
    \caption{All unlabelled data}
    \label{fig:ecdf_unlabelled_both}
  \end{subfigure}
  \caption{Comparison of the runtimes of algorithms on unlabelled data}
  \label{fig:ecdf_unlabelled}
\end{figure}

\begin{figure} % NOTE: updated with MCS data
  \centering
  \includegraphics{images/runtime_heatmap.png}
  \caption{A heatmap of $\log_{10}$ runtimes: light colours for low running
    times and black for timing out}
  \label{fig:runtime_heatmap}
\end{figure}

We plot the ECDF plots for unlablled graphs in both databases in Figure
\ref{fig:ecdf_unlabelled}. We can check that the orderings of the algorithms in
parts (a) and (b) of Figure \ref{fig:ecdf_unlabelled} are the same as in Figures
3a and 4 in the \textsc{McSplit} paper \cite{DBLP:conf/ijcai/McCreeshPT17}.
Namely, \textsc{McSplit} outperforms $k\downarrow$ in Figure
\ref{fig:ecdf_unlabelled_mcs}, and the opposite happens in Figure
\ref{fig:ecdf_unlabelled_sip}. In Figure \ref{fig:ecdf_unlabelled_both} we also
plot a curve for the \emph{virtual best solver (VBS)}, i.e., a perfect algorithm
portfolio that always chooses the best-performing algorithm for each problem
instance. Note that the difference between $\textsc{McSplit}\downarrow$ and VBS
is very small. Therefore, a portfolio cannot provide significant performance
benefits for this subproblem. We also provide a heatmap in Figure
\ref{fig:runtime_heatmap} to compare the runtimes of the algorithms on a
per-instance basis.

\begin{remark}
  Not every problem instance gets a line of pixels on the heatmap. Therefore,
  the column for the clique encoding may look darker than it actually is.
  Furthermore, there are problem instances where $k\downarrow$ performs better
  than the other algorithms, even though it may not be apparent from the heatmap.
\end{remark}

\begin{table} % NOTE: updated with mcs data
  \centering
  \begin{tabular}{l r r r r}
    Dataset & clique & $k\downarrow$ & \textsc{McSplit} & $\textsc{McSplit}\downarrow$ \\
    \hline
    \texttt{images-CVIU11} & 0 & 32 & 78 & 1080 \\
    \texttt{images-PR15} & 0 & 0 & 0 & 24 \\
    \texttt{largerGraphs} & 0 & 14 & 6 & 143 \\
    \texttt{LV} & 85 & 12 & 256 & 206 \\
    \texttt{meshes-CVIU11} & 0 & 13 & 0 & 23 \\
    \texttt{phase} & 0 & 0 & 0 & 0 \\
    \texttt{scalefree} & 0 & 0 & 0 & 80 \\
    \texttt{si} & 0 & 10 & 11 & 1044 \\
    ARG Database & 1178 & 0 & 13511 & 18858 \\
    \hline
    Total & 1263 & 81 & 13862 & 21458
  \end{tabular}
  \caption{The number of times each algorithm was the best, for each dataset}
  \label{table:best}
\end{table}

Table \ref{table:best} shows that most of the datasets have multiple algorithms
that managed to outperform the others for some problem instances. Thus, looking
at the differences between different datasets will not be enough to predict the
best algorithm.

\begin{remark}
  More specifically, Table \ref{table:best} shows the numbers of times that each
  algorithm's runtime was lower than the runtimes of other algorithms.
  Therefore, if 2 or more lowest runtimes are equal (as can often happen with
  single-digit runtimes), neither algorithm is marked as winning in the table.
\end{remark}

\begin{remark}
Note that even though $k\downarrow$ performs considerably better than the clique
encoding according to Figures \ref{fig:ecdf_unlabelled} and
\ref{fig:runtime_heatmap}, it is almost never the best algorithm.
\end{remark}

Given this information, we would expect the ML algorithm to suggest using
\textsc{McSplit} and $\textsc{McSplit}\downarrow$ most of the time, only
suggesting $k\downarrow$ and the clique encoding in very specific situations.

\subsection{Vertex-Labelled Graphs}
\subsection{Vertex- and Edge-Labelled Graphs}

\section{Graph Features} \label{sec:features}
%TODO: formulas for features?
The initial set of features is based on the algorithm selection paper for the
subgraph isomorphism problem \cite{DBLP:conf/lion/KotthoffMS16} and consists of
the following:

\begin{enumerate}
\item number of vertices,
\item number of edges,
\item mean degree,
\item maximum degree,
\item density,
\item mean distance between all pairs of vertices,
\item maximum distance between all pairs of vertices,
\item standard deviation of degrees,
\item number of loops,
\item proportion of all vertex pairs that have a distance of at least 2, 3, and 4,
\item whether the graph is connected.
%\item uniformity of the distribution of edges.
\end{enumerate}

\begin{definition}
For a graph $G$ with $n$ vertices and $m$ edges, the \emph{(edge) density} is
defined to be the proportion of potential edges that $G$ actually has
\cite{DBLP:books/daglib/0030488}. The standard formula used for simple graphs is
\[ \frac{m}{\binom{n}{2}} = \frac{2m}{n(n-1)}. \]
\end{definition}
Even though some of our graphs contain multiple edges and loops, we stick to
this formula as it was used in \cite{DBLP:conf/lion/KotthoffMS16} and it does
not break the ML algorithm in any way to have the theoretical possibility of
density greater than 1.

We exclude feature extraction running time as a viable feature by itself since
it would not provide any insight into what properties of the graph affect which
algorithm is likely to achieve the best performance. Since $k\downarrow$ and
$\textsc{McSplit}\downarrow$ both start by looking for (complete) subgraph
isomorphisms, they are likely to outperform other algorithms when both graphs
are very similar and the maximum common subgraph has (almost) as many vertices
as the smaller of the two graphs. Thus, for each feature $f$ in features 1--7
(excluding the rest to avoid division by 0), we also add a feature for the ratio
$\frac{f(G_p)}{f(G_t)}$, where $G_p$ and $G_t$ are the pattern and target
graphs, respectively.

We analyse three different types of labelling and treat them as separate
problems: no labels, vertex labels, vertex and edge labels. For the last two
types, we add a feature corresponding to $p$ defined in Definition
\ref{def:percent_labelling} and collect data for the following values of $p$:
50\%, 33\%, 25\%, 20\%, 15\%, 10\%, 5\%. The values correspond to having about
2, 3, 4, 5, 10, and 20 vertices/edges with the same label on average,
respectively.

\begin{remark} % TODO: could add more citations here
  When working with both vertex and edge labels, we only consider using the same
  value of $p$ for both vertices and edges. Although this may not be ideal, it
  reflects how the ARG Database was constructed. Furthermore, there are many
  ways to label a graph and it is unclear which types of labelling are worth
  investigating.
\end{remark}

\subsection{Distributions of Features}

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/mcs_vertices.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/mcs_edges.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/mcs_meandeg.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/mcs_maxdeg.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/mcs_stddeg.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/mcs_density.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/mcs_meandist.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/mcs_maxdist.png}
  \end{subfigure}
  \caption{Plots of how various features are distributed for the labelled graphs}
  \label{fig:mcs_features1}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/mcs_prop2.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/mcs_prop3.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/mcs_prop4.png}
  \end{subfigure}
  \caption{Plots showing typical distances between pairs of vertices for the
    labelled graphs}
  \label{fig:mcs_features2}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/sip_vertices.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/sip_edges.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/sip_meandeg.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/sip_maxdeg.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/sip_stddeg.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/sip_density.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/sip_meandist.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/sip_maxdist.png}
  \end{subfigure}
  \caption{Plots of how various features are distributed for the unlabelled
    graphs}
  \label{fig:sip_features1}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/sip_prop2.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/sip_prop3.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/sip_prop4.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/sip_loops.png}
  \end{subfigure}
  \caption{Plots showing typical distances between pairs of vertices and the
    number of loops for the unlabelled graphs}
  \label{fig:sip_features2}
\end{figure}

\begin{figure}
  \centering
  \includegraphics{images/features_heatmap.png}
  \caption{A heatmap for normalised features with black denoting the maximum
    value and white denoting the minimum for each feature}
  \label{fig:features_heatmap}
\end{figure}

In this section we plot and discuss how the selected features are distributed in
both databases. Most of the data is plotted, except for the number of loops
of the ARG Database and connectedness of both databases. For connectedness,
99.81\% of the labelled graphs are connected, compared to 93.19\% of the
unlabelled graphs. As both numbers are quite high, they may not be ideal for
establishing if connectedness is a significant factor in determining which
algorithm performs the best, however, the numbers seem quite representative of
real data, where connected graphs are a lot more common. In fact, applications
in chemistry are often only interested in connected graphs \cite{WCMS:WCMS5}.

With graphs from Section \ref{sec:labelled}, all graphs are assigned into pairs
(A and B) with no significant differences between them. Thus all of the
statistical information is provided only for pattern graphs and is plotted in
Figures \ref{fig:mcs_features1} and \ref{fig:mcs_features2}. As the number of
loops varies between two values, it is not plotted: 0.98\% of the graphs have a
single loop, other graphs have no loops. Other than the plot for number of
vertices, which is manually chosen by the creators of the database, all the
distributions in Figure \ref{fig:mcs_features1} are centered around lower
values, with some instances providing significantly higher values. More
importantly, we have some graphs that are quite dense and some graphs with
higher mean distance values.

With graphs from Section \ref{sec:unlabelled}, we will only consider graphs that
are part of a pair of graphs solved by at least one algorithm. Since most of the
data is heavily skewed, Figures \ref{fig:sip_features1} and
\ref{fig:sip_features2} show the distributions as density plots. Because of this
choice of which graphs to include, all of the easy instances are solved and thus
end up in the sample, while only some of the harder instances are solved by at
least one algorithm. Harder instances typically have more vertices, which means
they are also capable of higher values for many other features, hence all of the
density plots in Figure \ref{fig:sip_features1} are right skewed. The same
applies to the number of loops in Figure \ref{fig:sip_features2}: almost all
graphs have a small number of them, while some (presumably larger) graphs have
significantly more. The same story is represented in the heatmap in Figure
\ref{fig:features_heatmap} (which is plotted for all of the data), where the
average values are coloured grey, largest values black, and smallest values
white. The rows that look almost completely white represent features that have
several significantly higher values that skew the average.

To sum up, even though the distributions are far from normal, most of them still
provide a good range of different values. There is one important difference
between the plots of proportions of vertex pairs with distance $\ge k$ for $k =
2, 3, 4$ for the graphs from Sections \ref{sec:labelled} and \ref{sec:unlabelled} (in Figures
\ref{fig:mcs_features2} and \ref{fig:sip_features2}, respectively). With
the ARG Database, the mean value keeps shifting to the left, while with the
other database, the plot for $k = 4$ still has its highest peak around 0.9,
which means adding features for $k \ge 5$ could be valuable.

\chapter{Machine Learning}
After running the algorithms on all of the data for different types of labelling
and $p$ values, an ML algorithm can be trained to predict which algorithm should
be chosen for each pair of graphs. For each pair of graphs, \textsc{Llama} can
take:

\begin{itemize}
\item A list of features. We treat the features of pattern and target graphs
  separately, giving more than 20 features per problem instance.
\item A list of performance measures for all algorithms, i.e., the values that
  we are trying to optimise. In this case (as in most cases), this corresponds
  to running time. The values are capped at the timeout value (\num{1000000} ms).
  Furthermore, instances that were not run on the clique algorithm are also set
  to the timeout value. Finally, we filter out instances where all of the
  algorithms timed out.
\item A list of boolean values, denoting whether each algorithm successfully
  finished or not. Timeouts, the clique algorithm running out of memory, and
  instances that were not run with the clique algorithm because of their size
  are all marked as false.
\item A dataframe, measuring the running time taken to compute each feature for
  each problem instance. Alternatively, a single number for the approximate time
  taken to compute all features for any instance. This parameter is taken into
  account when comparing the algorithm portfolio against specific algorithms. As
  the main goal of this work is to gain insight about how the algorithms compare
  rather than to prove an algorithm portfolio as a superior approach, this
  parameter is not used.
\end{itemize}

After constructing the required dataframes as described above, the data needs to
be split into training and test sets. We use a technique called 10-fold
\emph{cross-validation}, which splits the data into 10 parts \cite{citeulike:1304145}.
9/10\textsuperscript{ths} of the data is used to train the ML algorithm, while
the remaining 1/10\textsuperscript{th} is used to evaluate how good the trained
model is. This process of training and evaluation is repeated 10 times, letting
each of the 10 parts be used for evaluation exactly once. The goodness-of-fit
criteria are then averaged out between the 10 runs.

The 10 folds could, of course, be chosen completely randomly. However, research
suggests that stratified cross-validation typically outperforms
random-sampling-based cross-validation and results in a better model
\cite{DBLP:conf/ijcai/Kohavi95}. Suppose we have a dataset of $N$ elements.
\emph{Stratified sampling} partitions it into a number of subpopulations $s_1,
\dots, s_n$ with $n_1, \dots, n_N$ elements, respectively (typically based on
the value of some feature or collection of features). It then draws from each
subpopulation independently, ensuring that approximately $n_i/N$ of the sample
comes from subpopulation $s_i$ for $i = 1, \dots, n$ \cite{lohr2009sampling}. In
this case the data is partitioned into four groups based on which algorithm
performed best.

The cross-validation folds are then passed to the ML algorithm. \textsc{Llama}
supports algorithm portfolios based on three different types of ML algorithms:

\begin{description}
  \item[Classification] The ML algorithm predicts which algorithm is likely to
    perform the best on each problem instance.
  \item[Regression] Each algorithm's data is used to train a separate ML model,
    predicting the algorithm's performance. The winning algorithm can then be
    chosen based on those predictions.
  \item[Clustering] All instances of the training data are clustered and the
    best algorithm is determined for each cluster. New problem instances can
    then be assigned to the closest cluster.
\end{description}

\begin{figure}
  \centering
  \includegraphics[scale=0.5]{images/tree_sizes.png}
  \caption{A histogram of the number of nodes in each tree}
  \label{fig:tree_size}
\end{figure}

We are using a classification algorithm called random forests. The ideas behind
the main algorithm is described in \cite{DBLP:journals/ml/Breiman01}. More
information about its implementation in R can be found in \cite{randomforest}. We
chose this algorithm as it is recommended in the \textsc{Llama} manual
\cite{kotthoff_llama_2013} and successfully used in a similar study
\cite{DBLP:conf/lion/KotthoffMS16}. We use the default number of trees (500),
and the number of nodes per tree is plotted in Figure \ref{fig:tree_size}.

In order to discuss and analyse the ML algorithm in more detail, we introduce
some new terminology. Each problem instance with features and running times in
the training dataset is called an \emph{observation}. The \emph{class} of an
observation is the algorithm with lowest running time for that problem instance.
Previously discussed features of graphs are sometimes referred to as
\emph{(independent) variables}. The \emph{(problem) instance space} is the
Cartesian product of the domains of features \cite{DBLP:series/smpai/RokachM14},
where a \emph{domain} of $X$ (denoted $\dom X$) is a set of all possible values
that $X$ can take.

A \emph{(classification) decision tree} is ``a classifier expressed as a
recursive partition of the instance space'' \cite{DBLP:series/smpai/RokachM14}.
Typically, it can be represented as a rooted binary tree, where each internal
node \emph{splits} the instance space into two regions based on the value of one
of the features. For example, for some feature $X$ and a particular value $x \in
\dom{X}$, the left child might be assigned all observations with $X < x$, while
the right child would get observations with $X \ge x$. For each leaf node, we
can count how many of its observations belong to each class and assign the most
highly represented class to that node.

\begin{remark}
  Other possibilities include a node having more than two children and a split
  being made in a more complicated way. Although trees with such properties fit
  the definition of a decision tree, standard machine learning algorithms are
  more restrictive \cite{James:2014:ISL:2517747, DBLP:series/smpai/RokachM14}.
\end{remark}

When a decision tree is used to make a classification prediction, a data point
travels from node to node (starting at the root node) according to the splitting
rules. When it reaches a leaf node, the class assigned to that node is outputted
as the tree's prediction.

A \emph{random forest} builds a collection of decision trees
\cite{James:2014:ISL:2517747}. However, each time a split is considered, we
choose the variable to split on not from all $p$ of the variables, but from a
random sample (usually of size $\sqrt{p}$ for classification problems). This
ensures some diversity among trees. % TODO: continue here
% TODO: describe random forest, vote,

As we are trying to use hundreds of megabytes of data, the R code was optimised
to reduce memory consumption by removing temporary variables as soon as they are
no longer needed and parallelised with the help of the \texttt{parallelMap}
package.

\section{Unlabelled Graphs}

\begin{figure}
  \centering
  \includegraphics[scale=0.5]{images/unlabelled_forest_errors.png}
  \caption{Convergence plot of various error measures as the number of trees in
    a random forest increases. The plot shows the OOB error and $1 -
    \text{recall}$ for each algorithm.}
  \label{fig:unlabelled_forest_errors}
\end{figure}

Random forests support a convenient way to estimate the test error without
cross-validation or any other kind of data splitting. Each tree in a random
forest uses around 2/3 of the data. The remaining 1/3 is referred to as
\emph{out-of-bag (OOB)} observations \cite{James:2014:ISL:2517747}. For each
observation in the data, we can predict the answer (the vote on which algorithm
is expected to win) using all trees that have the observation as OOB. The
majority vote is then declared to be the answer. The \emph{OOB error} is
the relative frequency of incorrect predictions. As each prediction was made
using trees that had not seen that particular observation before, OOB error is a
valid estimate of test error. The black line in Figure
\ref{fig:unlabelled_forest_errors} shows how the error converges with the number
of trees to about 17\%.

% TODO: count how many times each algorithm was predicted
% TODO: and explain the high error rates for clique and kdown
The other lines in the figure, one for each algorithm, are defined as $1 -
\text{recall}$, where for an algorithm $A$ \emph{recall}
\cite{citeulike:12882259} is
\[ \frac{\text{the number of instances that were correctly predicted as
      $A$}}{\text{the number of instances where $A$ is the correct
      prediction}}. \]
The error rates for the clique encoding and $k\downarrow$ are quite high,
converging to 47\% and 81\%, respectively. This is most likely due to having very
few observations with them as the winning algorithms. The error rates for
\textsc{McSplit} and $\textsc{McSplit}\downarrow$ converge to 27\% and 11\%,
respectively.

% TODO: cite all of this
Next we are going to explore how important each feature is in making
predictions, but for that we need to introduce some new definitions. Consider a
single tree $T$ in a random forest. The root of $T$ can be reached by any
observation, regardless of the values of its features. After passing some node
$n$, some feature is restricted, i.e., it is imposed an upper or lower limit on
the kind of values it can have for it to move towards a particular child of $n$.
The part of feature space that an observation can have while at some node $n$ is
called a \emph{region}.

\begin{definition}
  Suppose we have $K$ classes. Consider some region $m$. The \emph{Gini index}
  is then defined as
  \[ G = \sum_{k=1}^K \hat{p}_{mk}(1-\hat{p}_{mk}), \]
  where $\hat{p}_{mk}$ represents the proportion of observations in region $m$
  that are from class $k$ (i.e., have algorithm $k$ as the best algorithm)
  \cite{James:2014:ISL:2517747}.
\end{definition}

As we move down a tree, we want the region to be restricted to a single class.
Then the observations from the training data that satisfy the conditions imposed
by the parent nodes would be classified with perfect accuracy. The Gini is at
its lowest when all the proportions $\hat{p}_{mk}$ are close to either 0 or 1,
meaning that almost all the observations in the region belong to a single class.
Hence the Gini index is often used to evaluate the quality of a split.

\begin{remark}
  Note that $G=0$ when any single $\hat{p}_{mk}=0$, regardless of the values of
  other proportions. Therefore $G=0$ does not automatically imply that the tree
  is a good classifier.
\end{remark}

\begin{figure}
  \centering
  \includegraphics{images/unlabelled_variable_importance.png}
  \caption{Dotchart of variable importance calculated based on the Gini index
    and sorted from most important to least important}
  \label{fig:unlabelled_variable_importance}
\end{figure}

\begin{figure}
  \centering
  \includegraphics{images/unlabelled_var_used.png}
  \caption{How often was each variable used to make splitting decisions?}
  \label{fig:unlabelled_var_used}
\end{figure}

The variable importance measure of feature $f$ in Figure
\ref{fig:unlabelled_variable_importance} is calculated as the amount by which
the Gini index decreases after passing nodes that use feature $f$, averaged over
all trees in the random forest \cite{James:2014:ISL:2517747}. Looking at the
figure more closely, the standard deviations of degrees of both target and
pattern graphs are by far the most important predictors. Unsurprisingly, the
worst predictors are the features with very low variance: number of loops and
connectedness of both graphs. Perhaps more surprisingly, the ratio features are
not as successful as one might have hoped: the ratio of the numbers of vertices
is at the bottom 5th place and the best ratio feature, the mean distance ratio,
is only 10th. Last thing to note is that features of the pattern graph are
always behind the same features of the target graph and usually not far behind.
Perhaps this is due to some datasets having less pattern graphs, or them having
fewer vertices. The variable usage plot in Figure \ref{fig:unlabelled_var_used}
tells a similar story: the orders are not identical, but there are no big
outliers.

\begin{definition}
  Let $c_1, \dots, c_n$ be $n$ classes and let $p$ be a data point that belongs
  to class $c_p$. Let $v_1, \dots, v_n$ denote the number of votes for each
  class when given $p$ as input. The \emph{margin} of $p$ is
  \[ \frac{v_p}{\sum_{i=1}^n v_i} - \max_{i \ne p} \frac{v_i}{\sum_{j=1}^n v_j}, \]
  which is a number in $[-1, 1]$ \cite{forest}.
\end{definition}

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[scale=0.5]{images/unlabelled_margin.png}
    \caption{Sorted}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[scale=0.5]{images/unlabelled_margin2.png}
    \caption{Unsorted}
  \end{subfigure}
  \caption{Margins of all the data points. \textsc{McSplit} is in green,
    $\textsc{McSplit}\downarrow$ is in purple, $k\downarrow$ is in blue, and the
    clique encoding is in red.}
  \label{fig:unlabelled_margins}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[scale=0.5]{images/clique_hist.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[scale=0.5]{images/kdown_hist.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[scale=0.5]{images/mcsplit_hist.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[scale=0.5]{images/mcsplitdown_hist.png}
  \end{subfigure}
  \caption{Histograms of margins for each winning algorithm}
  \label{fig:unlabelled_margin_hist}
\end{figure}

A value above 0 means that the forest as a whole predicted correctly. A margin
of 1 would mean that all trees voted correctly. Figure
\ref{fig:unlabelled_margins} shows the sorted and unsorted margin of all data
points. We can recognise the same error rates as in Figure
\ref{fig:unlabelled_forest_errors} as well as areas where \textsc{McSplit} and
$\textsc{McSplit}\downarrow$ dominate. We also plot the histograms of how the
margins are distributed for each algorithm in Figure
\ref{fig:unlabelled_margin_hist}. We note that:

\begin{itemize}
\item we have no clue how to recognise when we should choose the clique
  encoding,
\item we are usually wrong about $k\downarrow$ (probably because it is the
  winning algorithm in only 0.178\% of all cases),
\item when faced with an instance that is best handled by
  $\textsc{McSplit}\downarrow$, the vast majority of the trees vote correctly,
\item \textsc{McSplit} detection rates are decent, but far behind
  $\textsc{McSplit}\downarrow$.
\end{itemize}

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[scale=0.7]{images/mcsplit_partial.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[scale=0.7]{images/clique_partial.png}
  \end{subfigure}
  \caption{Partial dependence plots for the standard deviation of degrees of
    the target graph}
  \label{fig:unlabelled_partials}
\end{figure}

Since the standard deviations of degrees in both target and pattern graphs are
the most important features, we plot partial dependence plots for the standard
deviation of degrees in the target graph for $\textsc{McSplit}\downarrow$ and
the clique encoding in Figure \ref{fig:unlabelled_partials}. The plotted
function \cite{forest} is defined as
\[ f(x) = \log{p_k(x)} - \frac{1}{K} \sum_{i=1}^K \log{p_i(x)}, \]
where:

\begin{itemize}
\item $x$ is the value of the $x$ axis (in this case a particular standard
  deviation of degrees in the target graph),
\item $p_i(x)$ is the proportion of votes for class $i$ for a problem instance
  with a standard deviation of degrees in the target graph equal to $x$,
\item $K$ is the number of classes,
\item and $k$ is the main class under consideration
  ($\textsc{McSplit}\downarrow$ and the clique encoding).
\end{itemize}

Essentially, $f(x)$ compares the proportion of votes for class $k$ with the
average value over all classes. We can deduce that a low standard deviation of
degrees is a strong sign that \textsc{McSplit} and $\textsc{McSplit}\downarrow$
should perform well. On the other hand, the clique encoding is expected to
perform better on graphs with high variance of degrees. However, $\max f(x)$ for
the clique encoding is just barely above 0 and much lower than $\min f(x)$ for
$\textsc{McSplit}\downarrow$, meaning that the standard deviation of degrees
does not provide enough information to choose the clique encoding over
\textsc{McSplit} or $\textsc{McSplit}\downarrow$.

\begin{remark}
  We omit the plots for the other two algorithms as the plot for \textsc{McSplit}
  looks the same as the one for $\textsc{McSplit}\downarrow$ and prediction success
  rate for $k\downarrow$ is so low that a plot for $k\downarrow$ would be
  meaningless.
\end{remark}

\begin{remark}
  The plots for the standard deviation of degrees of the pattern graph are
  omitted since they are identical to those of the target graph.
\end{remark}

\section{Vertex-Labelled Graphs}

\section{Vertex- and Edge-Labelled Graphs}

%\begin{appendices}
%\end{appendices}

\bibliographystyle{plain}
\bibliography{references}
\end{document}
