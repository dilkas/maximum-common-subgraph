\pdfoutput=1
\documentclass{l4proj}

\usepackage[UKenglish]{babel}
\usepackage[UKenglish]{isodate}
\PassOptionsToPackage{hyphens}{url}\usepackage[hidelinks]{hyperref}
\usepackage{amsfonts, amsmath, amsthm, graphicx, subcaption, mathtools,
  tikz, cancel, bm, etoolbox, booktabs}
\usepackage[group-separator={,}]{siunitx}
\usepackage[linesnumbered]{algorithm2e}
\usetikzlibrary{positioning}

\newtheorem{lemma}{Lemma}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\newtheorem{example}{Example}[chapter]
\theoremstyle{remark}
\newtheorem{remark}{Remark}[chapter]

\AfterEndEnvironment{lemma}{\noindent\ignorespaces}
\AfterEndEnvironment{proposition}{\noindent\ignorespaces}
\AfterEndEnvironment{definition}{\noindent\ignorespaces}
\AfterEndEnvironment{example}{\noindent\ignorespaces}
\AfterEndEnvironment{remark}{\noindent\ignorespaces}

\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\nablaop}{\nabla}
\newcommand{\kprec}[1]{\prescript{}{#1}{\preceq}\ }
\newcommand{\etal}{\textit{et al}.}

\title{Algorithm Selection for Maximum Common Subgraph}
\author{Paulius Dilkas}
%\date{}

\begin{document}
\maketitle

\begin{abstract}
  TODO: obviously
\end{abstract}

\educationalconsent
\tableofcontents

\chapter{Introduction}
\pagenumbering{arabic}

Maximum common subgraph (MCS) is a way to determine the similarity between two
graphs. More specifically, MCS algorithms find what the two graphs have in
common (these ideas will be made more concrete in
Section~\ref{sec:definitions}). Since graphs can represent many real-world
phenomena, MCS algorithms have been used in molecular science \cite{WCMS:WCMS5,
  DBLP:journals/dam/GayFMSS14, grindley, DBLP:journals/jcamd/RaymondW02a},
malware detection \cite{DBLP:journals/compsec/ParkRS13}, and substructure
discovery \cite{DBLP:journals/jair/CookH94, 617051}.

Different MCS algorithms tend to be best for different types of graphs,
depending on how many vertices they have, and whether they contain labels
\cite{DBLP:conf/ijcai/McCreeshPT17}. In some cases the differences are quite
small, and typically the overall best algorithm gets outperformed by other
algorithms as often as half the time (see Chapter \ref{chapter:generating_data}
for more details). Since there are no (known) clear boundaries that separate
instances best handled by each algorith, perhaps training a machine learning
(ML) model to quickly choose an algorithm based on features of the input graphs
would result in a algorithm faster than any individual algorithm. This is an
approach known as algorithm selection \cite{DBLP:journals/ac/Rice76} and is the
main focus of this project.

In the rest of this chapter we provide rigorous definitions for concepts such as
algorithm selection, define the kind of graphs we are interested in, continue
with some necessary graph theory definitions, building up to the definition of
an MCS. We then descibe and illustrate the algorithms used in this project. In
order to compare MCS algorithms and reliably choose an algorithm for each
problem instance, we need to run all algorithms on some pairs of graphs, and
use an ML algorithm to find how their running times (or, more precisely, which
algorithm is expected to win) depend on various features of the graphs. Thus,
Chapter \ref{chapter:problems} describes all graphs used to compare algorithms
and train ML models, looking into their origins, how they were constructed, and
undocumented but relevant features. Chapter \ref{chapter:generating_data} then
describes how the algorithms were run to generate running time data, explores
how the algorithms compare on different parts of the data, outlines the features
used for ML, and describes their distributions in the data. We then move on to
building the ML models in Chapter \ref{chapter:ml}, where we start with the
relevant definitions in ML and statistics, describing the relevant software and
the ML algorithm used. Afterwards, we describe various ways to evaluate the
resulting models and do so for three different models. Finally, Chapter
\ref{chapter:further} provides ideas on how to expand one of the algorithms to
previously unsupported instances, and introduces a new algorithm that merges two
algorithms into one.

\section{Definitions} \label{sec:definitions}

\begin{definition}
  Given a set $\mathcal{I}$ of problem instances, a space of algorithms
  $\mathcal{A}$, and a performance measure $m \colon \mathcal{I} \times
  \mathcal{A} \to \mathbb{R}$, the \emph{per-instance algorithm selection
    problem} is to find a mapping $s \colon \mathcal{I} \to \mathcal{A}$ that
  optimises $\mathbb{E}[m(i, s(i))]$ \cite{DBLP:journals/ai/BischlKKLMFHHLT16}.
  $\mathcal{A}$ (and sometimes this problem in general) is often referred to as
  an \emph{algorithm portfolio} \cite{DBLP:conf/ijcai/Leyton-BrownNAMS03,
    DBLP:journals/corr/abs-1111-2249}.
\end{definition}
%We begin with a definition of a graph suitable for the empirical data described
%in Chapter~\ref{chapter:problems} and used in
%Chapter~\ref{chapter:generating_data}.

\begin{definition}
  Let $S$ be a set and $k$ a non-negative integer. Then $[S]^k$ denotes
  the set of \emph{$k$-subsets} of $S$ \cite{subset}, i.e.,
  \[ [S]^k = \{ A \subseteq S : |A| = k \}. \]
\end{definition}

\begin{definition} \label{def:graph}
  An \emph{undirected multigraph} is a pair $(V, E)$, where $V$ is a set of
  vertices and $E$ is a set of edges, together with a map $E \to V \cup [V]^2$,
  which assigns one or two vertices to each edge
  \cite{DBLP:books/daglib/0030488}. If an edge is assigned to a single vertex,
  it is called a \emph{loop}. When several edges map to the same pair of
  vertices, they are referred to as \emph{multiple edges}. 
\end{definition}
For the purposes of this project, we look at two kinds of labelled graphs: those
with vertex labels, and those with both vertex and edge labels. Labels can be
used to represent important properties of graphs such as types of atoms and
bonds in chemistry. We define them as follows (the definitions are loosely
inspired by \cite{abu-aisheh_2016}):

\begin{definition} \label{def:vertex_label}
  A \emph{(vertex-)labelled graph} is a 3-tuple $G = (V, E, \mu)$, where $V$,
  $E$ are as in Definition~\ref{def:graph} and $\mu \colon V \to \{ 0, \dots, N
  - 1 \}$ is a vertex labelling function, for some $N \in \mathbb{N}$.
\end{definition}

\begin{definition} \label{def:edge_label}
  A \emph{fully labelled graph} is a 4-tuple $G = (V, E, \mu, \zeta)$, where the
  first three elements are as in Definition~\ref{def:vertex_label} and $\zeta
  \colon E \to \{ 0, \dots, M - 1 \}$ is an edge labelling function, for some $M
  \in \mathbb{N}$.
\end{definition}
Specifically, note that:

\begin{itemize}
\item If a graph is labelled, then all its vertices (and possibly edges) are
  assigned a label.
\item We are only considering finite sets of labels, represented by non-negative
  integers.
\item A vertex-labelled graph is just a special case of a fully labelled graph
  with $M = 1$.
\item An unlabelled graph is just a special case of of a vertex-labelled graph
  with $N = 1$.
\end{itemize}

The last two observations allow us to tailor subsequent definitions to fully
labelled graphs without having to redefine everything for unlabelled and
vertex-labelled graphs as well. When vertex and/or edge labels are immaterial to
a definition, we will omit the labelling function from the full 4-tuple and
denote a graph as just $G = (V, E)$.

We formulate all definitions in terms of undirected multigraphs, as some of the
graphs in Chapter~\ref{chapter:problems} contain multiple edges, and many contain
at least one loop. In the rest of the dissertation, we will use the word
``graph'' to denote undirected multigraphs. Next we adapt some basic graph
theory definitions to work with our definitions of graphs and labelling. As most
definitions in the literature are typically defined for simple graphs with no
labels, the cited sources are heavily adapted to the full generality of graphs
relevant to this dissertation.

\begin{definition}
  Two graphs $G_1 = (V_1, E_1, \mu_1, \zeta_1)$ and $G_2 = (V_2, E_2, \mu_2,
  \zeta_2)$ are said to be \emph{isomorphic} if there are bijections $f \colon
  V_1 \to V_2$, $g \colon E_1 \to E_2$ such that
  \cite{DBLP:journals/jcamd/RaymondW02a}:
  \begin{itemize}
  \item $\forall e \in E_1$, if $e$ maps to some $v \in V_1 \cup [V_1]^2$, then
    $g(e)$ maps to $f(v)$, where $f(v) = \{ f(v_1), f(v_2) \}$ if $v = \{ v_1,
    v_2 \}$ (preserving structure);
  \item $\forall v \in V_1$, $\mu_2(f(v)) = \mu_1(v)$ (preserving vertex labels)
    \cite{DBLP:journals/siamcomp/BabaiES80};
  \item $\forall e \in E_1$, $\zeta_2(g(e)) = \zeta_1(e)$ (preserving edge
    labels).
  \end{itemize}
\end{definition}

\begin{definition}
  Let $f \colon X \to Y$ be a function. Then the \emph{restriction} of $f$ to $A
  \subseteq X$ is the function $f|_A \colon A \to Y$ such that $\forall a \in A$,
  $f|_A(a) = f(a)$ \cite{stoll1979set}.
\end{definition}

\begin{definition} A graph $G' = (V', E', \mu', \zeta')$ is a
  \emph{subgraph} of graph $G = (V, E, \mu, \zeta)$ if: $V' \subseteq V$, $E'
  \subseteq E$ (with the edge-mapping function restricted to $E'$), $\mu' =
  \mu|_{V'}$, $\zeta' = \zeta|_{E'}$ \cite{DBLP:books/daglib/0030488}.
\end{definition}

\begin{definition} \label{def:induced_subgraph}
  An \emph{induced subgraph} of a graph $G = (V, E)$ is a subgraph $H = (S,
  E')$, where $E' \subseteq E$ is a set of edges mapped to $S \cup [S]^2$
  \cite{DBLP:journals/jcamd/RaymondW02a}.
\end{definition}

\begin{definition}
  A \emph{clique} $C$ in a graph $G = (V, E)$ is a subset of $V$ such that
  $\forall v_1, v_2 \in C$ with $v_1 \ne v_2$, there is an edge in $E$ mapping
  to $\{ v_1, v_2 \}$ \cite{DBLP:journals/jgo/PardalosX94a}.
\end{definition}
Finally, we define the main problem of this project:

\begin{definition}
  A \emph{maximum common (induced) subgraph} (MCS) between graphs $G_1$ and
  $G_2$ is a graph $G_3 = (V_3, E_3)$ such that $G_3$ is isomorphic to induced
  subgraphs of both $G_1$ and $G_2$ with $|V_3|$ maximised
  \cite{DBLP:journals/jcamd/RaymondW02a}. The \emph{maximum common (induced)
    subgraph problem} is the problem of finding an MCS between two given graphs
  $G_1 = (V_1, E_1)$ and $G_2 = (V_2, E_2)$, usually expressed as a bijection
  between two subsets of vertices $U_1 \subseteq V_1$ and $U_2 \subseteq V_2$.
\end{definition}
The decision version of this problem (finding a common induced subgraph of a
fixed size) is NP-complete \cite{DBLP:journals/bmcbi/HuangLJ06}, and thus the
MCS optimisation problem is NP-hard \cite{DBLP:conf/ijcai/McCreeshPT17,
  DBLP:books/ph/PapadimitriouS82}.

\begin{figure}
  \centering
  \begin{tikzpicture}
    \begin{scope}[every node/.style={circle,draw}]
      \node (u1) [fill=gray] at (0, 4) {$u_1$};
      \node (u2) at (0, 2) {$u_2$};
      \node (u3) at (0, 0) {$u_3$};
      \node (u4) at (2, 3) {$u_4$};
      \node (u5) at (2, 1) {$u_5$};
    \end{scope}
    \path (u1) [color=blue,ultra thick] edge node {} (u2);
    \path (u1) edge node {} (u4);
    \path (u1) edge node {} (u5);
    \path (u2) edge node {} (u4);
    \path (u3) edge node {} (u4);
    \path (u3) edge node {} (u5);
    \begin{scope}[every node/.style={circle, draw},xshift=6cm]
      \node (v1) at (0, 4) {$v_1$};
      \node (v2) at (0, 2) {$v_2$};
      \node (v3) at (0, 0) {$v_3$};
      \node (v4) [fill=gray] at (2, 3) {$v_4$};
      \node (v5) [fill=gray] at (2, 1) {$v_5$};
    \end{scope}
    \begin{scope}[color=blue,ultra thick]
      \path (v1) edge node {} (v2);
      \path (v1) edge node {} (v4);
      \path (v2) edge node {} (v4);
      \path (v2) edge node {} (v5);
    \end{scope}
  \end{tikzpicture}
  \caption{Two graphs with binary ($\protect{\{ 0, 1 \}}$) labels on both vertices
    and edges with pattern graph $G_p$ on the left and target graph $G_t$ on the
    right. Grey vertices and thick blue edges represent label 1, while white
    vertices and black edges have label 0.}
  \label{fig:graphs}
\end{figure}

\begin{example}
  Consider the graphs in Figure~\ref{fig:graphs}. Since $G_p$ has only one blue
  edge, while all edges of $G_t$ are blue, any common subgraph can have up to
  one edge. It is then obvious that any MCS must be isomorphic to the subgraph
  induced by $\{ u_1, u_2, u_3 \}$ in $G_p$ and to multiple induced subgraphs of $G_t$.
\end{example}

\section{Algorithms}

We consider four algorithms that were shown to be competitive in a recent paper by
McCreesh, Prosser and Trimble \cite{DBLP:conf/ijcai/McCreeshPT17}: $k{\downarrow}$
\cite{DBLP:conf/aaai/HoffmannMR17}, \textsc{McSplit}
\cite{DBLP:conf/ijcai/McCreeshPT17}, $\textsc{McSplit}{\downarrow}$
\cite{DBLP:conf/ijcai/McCreeshPT17}, and the clique encoding
\cite{DBLP:conf/cp/McCreeshNPS16}. In order to explain how they work, we need to
define two other problems:

\begin{definition}
  Given a graph $G$, the \emph{maximum clique problem} is an optimisation problem
  asking for a clique in $G$ with maximum cardinality
  \cite{DBLP:journals/jgo/PardalosX94a}.
\end{definition}

\begin{definition}
  Given two (finite) graphs $G_1$ and $G_2$, the \emph{subgraph isomorphism
    problem} is the decision problem of determining whether $G_1$ is isomorphic
  to a subgraph of $G_2$ \cite{DBLP:conf/stoc/Cook71}. $G_1$ and $G_2$ are
  usually referred to as the \emph{pattern} and \emph{target} graphs
  \cite{DBLP:journals/ai/Solnon10, Valiente97analgorithm,
    DBLP:journals/constraints/ZampelliDS10}, respectively. We will use these
  definitions in the context of both the subgraph isomorphism problem and the
  MCS problem, where the word 'pattern' will typically refer to the smaller of
  the two graphs.
\end{definition}

\noindent The $k{\downarrow}$ algorithm \cite{DBLP:conf/aaai/HoffmannMR17}
starts by trying to solve the subgraph isomorphism problem, i.e., finding the
pattern graph in the target graph. If that fails, it allows a single vertex of
the pattern graph to not match any of the target graph vertices and tries again,
allowing smaller and smaller pattern graphs until it finds a solution. The
number of vertices of the pattern graph that are allowed this additional freedom
is represented by $k$. More specifically, the algorithm creates a domain for
each pattern graph vertex, which initially includes all vertices of the target
graph and $k$ wildcards. The domains are filtered with various propagation
techniques. Then the search begins with a smallest domain (not counting
wildcards), a value is chosen, and domains are filtered again to eliminate the
chosen value.

\begin{definition}
  Although typically the \emph{image} of a set $S$ under a function $f$
  \cite{algebra} is defined as a set
  \[ f(S) = \{ f(s) : s \in S \}, \]
  we define the \emph{multiset image} of $S$ under $f$ by the same expression,
  but with $f(S)$ as a multiset, i.e., if $f(x) = f(y)$ for two different $x, y
  \in S$, then $f(x)$ is in $f(S)$ at least two times
  \cite{DBLP:books/lib/Knuth98}.
\end{definition}

\begin{definition} \label{def:nabla}
  The \emph{association graph} of an MCS problem instance between graphs $G_1 =
  (V_1, E_1, \mu_1, \zeta_1)$ and $G_2 = (V_2, E_2, \mu_2, \zeta_2)$, denoted
  $G_1 \nablaop G_2 = (V_3, E_3)$ \cite{DBLP:conf/cp/McCreeshNPS16}, is a simple
  graph that can be defined as follows:
  \begin{itemize}
  \item we add a vertex $(v_1, v_2)$ to $V_3$ if $\mu_1(v_1) = \mu_2(v_2)$, and
    the multiset image of loops of $v_1$ under $\zeta_1$ is equal to the
    multiset image of loops of $v_2$ under $\zeta_2$;
  \item $E_3$ is defined so that there is an edge between vertices $(u_1, u_2)$
    and $(v_1, v_2)$ if $u_1 \ne v_1$, $u_2 \ne v_2$, and the multiset image of
    edges in $E_1$ that are between $u_1$ and $v_1$ under $\zeta_1$ is equal to
    the multiset image of edges in $E_2$ that are between $u_2$ and $v_2$ under
    $\zeta_2$.
  \end{itemize}
\end{definition}

\noindent The clique encoding \cite{DBLP:conf/cp/McCreeshNPS16} solves the MCS
problem by creating an association graph and transforming the problem into an
instance of the maximum clique problem \cite{Levi1973}, which is then solved by
a sequential version of the maximum clique solver by McCreesh and Prosser
\cite{DBLP:journals/topc/McCreeshP15}, which is a branch and bound algorithm
that uses bitsets and greedy colouring. Colouring is used to provide a quick
upper bound: if a subgraph can be coloured with $k$ colours, then it cannot have
a clique of size more than $k$.

\textsc{McSplit} \cite{DBLP:conf/ijcai/McCreeshPT17} is a branch and bound
algorithm that builds its own bit string labels for vertices in both pattern and
target graphs. Once it chooses to match a vertex $u$ in graph $G_1$ with a
vertex $v$ in graph $G_2$, it iterates over all unmatched vertices in both
graphs, adding a 1 to their labels if they are adjacent to $u$ or $v$ and 0
otherwise. That way a vertex can only be matched with vertices that have the
same labels. The labels are also used in the upper bound heuristic function:
if a particular label is assigned to $m$ vertices in $G_1$ and $n$ vertices in
$G_2$, then up to $\min \{ m, n \}$ pairs can be matched for that label.

$\textsc{McSplit} {\downarrow}$ is a variant of \textsc{McSplit} mentioned but not
explained in the original paper \cite{DBLP:conf/ijcai/McCreeshPT17}. It is meant
to be similar to $k{\downarrow}$ in that it starts by trying to find a subgraph
isomorphism and keeps decreasing the size of common subgraphs that it is
interested in until a solution is found. Based on the source
code\footnote{\url{https://github.com/jamestrimble/ijcai2017-partitioning-common-subgraph/blob/master/code/james-cpp/mcsp.c}},
there are a few key differences between $\textsc{McSplit} {\downarrow}$ and
\textsc{McSplit}:

\begin{itemize}
\item Instead of always looking for larger and larger common subgraphs, we have
  a goal size and exit early if a common subgraph of that size is found.
\item The goal size is decreased if the search finishes without a solution.
\item Having a big goal size allows the heuristic to be more selective and prune
  more of the search tree branches.
\end{itemize}

The \textsc{McSplit} paper \cite{DBLP:conf/ijcai/McCreeshPT17} compared these
(and a few constraint programming) algorithms and found \textsc{McSplit} to win
with unlabelled graphs from Section~\ref{sec:labelled} of this dissertation, the
clique encoding to win with labelled graphs, and $\textsc{McSplit}{\downarrow}$
to win with unlabelled graphs from Section~\ref{sec:unlabelled} (closely
followed by $k{\downarrow}$).

\chapter{Problem Instances} \label{chapter:problems}

In order to build ML models that predict the winning algorithm for each pair of
graphs as well as to see how the algorithms compare for different kinds of
graphs, we need to have or generate some graphs. For that we use two graph
databases that contain a large variety of graphs differing in size, various
characteristics, and the way they were generated.

\section{Labelled Graphs} \label{sec:labelled}

All labelled graphs are taken from the ARG Database \cite{foggia2001-2,
  DBLP:journals/prl/SantoFSV03}, which is a large collection of graphs for
benchmarking various graph-matching algorithms. The database contains randomly
generated graphs; 2D, 3D, and 4D meshes; and bounded valence graphs.
Furthermore, each graph-generating algorithm is executed with several (3--5)
different parameter values. The database includes \num{81400} pairs of labelled
graphs. Their unlabelled versions are used as well.

\subsection{Characteristics of Graph Labelling} \label{sec:characteristics}

In Definitions~\ref{def:vertex_label} and \ref{def:edge_label} we used $N$ and
$M$ to denote the number of different labels for vertices and edges,
respectively. The ARG Database supports interpreting the same graphs to have
different numbers of different labels via the following
parameter\footnote{\url{http://mivia.unisa.it/datasets/graph-database/arg-database/documentation/},
``How to read labeled graphs''}:

\begin{definition} \label{def:percent_labelling}
  A graph $G = (V, E)$ is said to have a \emph{$p\%$ (vertex) labelling} if
  \[ N = \max \left\{ 2^n : n \in \mathbb{N},\, 2^n < \left\lfloor \frac{p}{100\%}
        \times |V| \right\rfloor \right\}. \]
\end{definition}

\begin{figure}
  \centering
  \includegraphics[scale=0.5]{images/labelling_histogram.png}
  \caption{Histogram of the difference between the expected number of vertices
    assigned each label and the actual number (for all labelled graphs).}
  \label{figure:labelling_histogram}
\end{figure}

The default value for $p$ is 33\%. The publications associated with the database
\cite{foggia2001-2, DBLP:journals/prl/SantoFSV03} say nothing about how the
labels are distributed among the $N$ values. We calculate the number of vertices
that were assigned each label for each graph (represented by $C$) and compare
those values with the numbers we would expect from a uniform distribution
(represented by $\mathrm{E}(C)$). We plot a histogram of the difference $\mathrm{E}(C) - C$ in
Figure~\ref{figure:labelling_histogram} and observe that the difference is
normally distributed around 0.

\section{Unlabelled Graphs} \label{sec:unlabelled}

We also include a collection of benchmark instances for the subgraph isomorphism
problem\footnote{\url{http://liris.cnrs.fr/csolnon/SIP.html}} (with the
biochemical reactions dataset excluded since we are not dealing with directed
graphs). It contains only unlabelled graphs, and consists of the following
sets\footnote{This set of instances was taken from the repository
  (\url{https://github.com/jamestrimble/ijcai2017-partitioning-common-subgraph})
  for the \textsc{McSplit} paper \cite{DBLP:conf/ijcai/McCreeshPT17} and has
  some minor differences from the version on Christine Solnon's website.}:

\begin{description}
\item[images-CVIU11] Graphs generated from segmented images. There are 43
  pattern graphs and 146 target graphs, giving a total of \num{6278} instances.
\item[meshes-CVIU11] Graphs generated from meshes modelling 3D
  objects. 6 pattern graphs and 503 target graphs, giving a total of \num{3018}
  instances. Both \texttt{images-CVIU11} and \texttt{meshes-CVIU11} datasets are
  described in a paper by Damiand \etal{} \cite{DBLP:journals/cviu/DamiandSHJS11}.
\item[images-PR15] Graphs generated from segmented images
  \cite{DBLP:journals/pr/SolnonDHJ15}. There are 24 pattern graphs and a single
  target graph, giving 24 instances.
\item[LV] Graphs with various properties (connected, biconnected, triconnected,
  bipartite, planar, etc.). 49 graphs are paired up in all possible ways, giving
  $49^2=\num{2401}$ instances.
\item[scalefree] Scale-free networks generated using a power law distribution of
  degrees (100 instances).
\item[si] Bounded valence graphs, 4D meshes, and randomly generated graphs
  (\num{1170} instances). This is the unlabelled part of the ARG database.
  \texttt{LV}, \texttt{scalefree}, and \texttt{si} datasets are described in
  two papers by Solnon \cite{DBLP:journals/ai/Solnon10} and by Zampelli,
  Deville, and Solnon \cite{DBLP:journals/constraints/ZampelliDS10}.
\item[phase] Random graphs generated to be close to the
  satisfiable-unsatisfiable phase transition (200 instances)
  \cite{DBLP:conf/ijcai/McCreeshPT16}.
\item[largerGraphs] Larger instances of the \texttt{LV} dataset. There are 70
  graphs, giving $70^2=\num{4900}$ instances. The separation was made and used
  in the $k{\downarrow}$ paper \cite{DBLP:conf/aaai/HoffmannMR17}, a paper on
  algorithm selection for the subgraph isomorphism problem by Kotthoff,
  McCreesh, and Solnon \cite{DBLP:conf/lion/KotthoffMS16}, and the
  \textsc{McSplit} paper \cite{DBLP:conf/ijcai/McCreeshPT17}.
\end{description}

\begin{remark}
  Since $k{\downarrow}$ comes from the subgraph isomorphism problem background, it
  treats the two (pattern and target) graphs differently. Therefore, when graphs
  are not divided into patterns and targets, we run the algorithms with both
  orderings ($(G_1, G_2)$ and $(G_2, G_1)$).
\end{remark}

\chapter{Generating Data} \label{chapter:generating_data}

A machine learning (ML) model requires data to learn from. We are using an R
package called \textsc{Llama} \cite{kotthoff_llama_2013, llama}, which helps to train
and evaluate ML models in order to compare algorithms and was used to create
algorithm portfolios for the travelling salesperson problem
\cite{DBLP:conf/lion/KotthoffKHT15} and the subgraph isomorphism problem
\cite{DBLP:conf/lion/KotthoffMS16}. First, we run each algorithm on all pairs of
pattern-target graphs and record the running times (described in
Section~\ref{sec:runtimes}). Then, we adapt a graph feature extractor program
used in an algorithm selection paper for the subgraph isomorphism problem by
Kotthoff, McCreesh, and Solnon \cite{DBLP:conf/lion/KotthoffMS16} to handle the
binary format of the ARG Database \cite{foggia2001-2,
  DBLP:journals/prl/SantoFSV03}, run it on all graphs, and record the features
in a way described in Section~\ref{sec:features}.

\section{Running Time of Algorithms} \label{sec:runtimes}

The algorithms were compiled with gcc 6.3.0 and run on nodes with Ubuntu 17.04
(Zesty Zapus) operating system, Intel Xeon E5-2697A v4 (2.60 GHz) processors,
and 512 GB RAM. Each algorithm was set to time out after \num{1000} s. A
Makefile was created to run multiple experiments in parallel (with, e.g.,
\texttt{make -j 64}), which generates pairs of graph filenames for all datasets,
runs the selected algorithms with various command line arguments, redirects
their output to files that are later parsed using \texttt{sed} and regular
expressions into the CSV format. For each algorithm, we keep the full names of
pattern and target graphs, the number of vertices in the returned MCS, running
time as reported by the algorithms themselves, and the number of explored nodes
in the search tree. Entries with running time greater than or equal to the
timeout value are considered to have timed out. The aforementioned node counts
are collected but not currently used. Afterwards, the answers of different
algorithms are checked for equality (for algorithms that did not time out).

Some limitations had to be enforced to avoid running out of memory. First, the
clique algorithm requires $O(n^2m^2)$ memory for a pair of graphs with $n$ and
$m$ vertices \cite{DBLP:conf/aaai/HoffmannMR17, DBLP:conf/cp/McCreeshNPS16}, so
its virtual memory usage was limited to 7 GB with \texttt{ulimit -v} and the
instances from Section~\ref{sec:unlabelled} (which contain much larger graphs)
were restricted to $m \times n < \num{16000}$. Second, having almost $10^5$
problem instances and analysing 7 different kinds of labelling (according to
Definition~\ref{def:percent_labelling}) results in too much data for the ML
algorithm. Therefore, we sample \num{30000} out of \num{81400} instances from
Section~\ref{sec:labelled}. The sample is drawn once and used to train ML
models for both vertex-labelled and fully labelled graphs. As with 50\%
labelling most instances are solved within the time limit, sampling from the
whole database still leaves us with enough instances where at least one
algorithm finished within the time limit.

In the rest of this section we explore and compare how the algorithms performed
on the three different subproblems under consideration, namely with unlabelled,
vertex-labelled and fully labelled graphs. We introduce
\emph{empirical cumulative distribution function (ECDF)}
plots \cite{10.2307/2334448} (also known as cumulative plots): for each unit of
time on the horizontal axis, the value on the vertical axis represents the
proportion of problem instances solved in that amount of time or less on an
instance-by-instance basis. For example, if an algorithm finished one instance
in 1~s and two instances in 2~s, then its ECDF curve will connect points $(0,
0)$, $(1, \frac{1}{3})$, and $(2, 1)$ (assuming that time is measured in
seconds).

\subsection{Unlabelled Graphs}

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/ecdf_mcs.png}
    \caption{Data from Section~\ref{sec:labelled}, the ARG Database}
    \label{fig:ecdf_unlabelled_mcs}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/ecdf_sip.png}
    \caption{Data from Section~\ref{sec:unlabelled}}
    \label{fig:ecdf_unlabelled_sip}
  \end{subfigure}
  \begin{subfigure}[t]{\textwidth}
    \centering
    \includegraphics[width=0.49\textwidth]{images/ecdf_unlabelled.png}
    \caption{All unlabelled data}
    \label{fig:ecdf_unlabelled_both}
  \end{subfigure}
  \caption{Comparison of the runtimes of algorithms on unlabelled data.}
  \label{fig:ecdf_unlabelled}
\end{figure}

We plot the ECDF plots for unlabelled graphs in both databases in
Figure~\ref{fig:ecdf_unlabelled}. We can check that the orderings of the
algorithms in parts (a) and (b) of Figure~\ref{fig:ecdf_unlabelled} are the same
as in Figures 3a and 4 of the \textsc{McSplit} paper
\cite{DBLP:conf/ijcai/McCreeshPT17}. Namely, \textsc{McSplit} outperforms
$k{\downarrow}$ in Figure \ref{fig:ecdf_unlabelled_mcs}, and the opposite
happens in Figure \ref{fig:ecdf_unlabelled_sip}. In
Figure~\ref{fig:ecdf_unlabelled_both} we also plot a curve for the \emph{virtual
  best solver (VBS)}, i.e., a perfect algorithm portfolio that always chooses
the best-performing algorithm for each problem instance. Note that the
difference between $\textsc{McSplit}{\downarrow}$ and the VBS is very small.
Therefore, a portfolio cannot provide significant performance benefits for
unlabelled graphs.

\begin{table}
  \centering
  \begin{tabular}{l l l l l}
    \toprule
    Dataset & clique & $k{\downarrow}$ & \textsc{McSplit} & $\textsc{McSplit}{\downarrow}$ \\
    \midrule
    \texttt{images-CVIU11} & \tablenum{0} & \tablenum{32} & \tablenum{79} & \tablenum{1081} \\
    \texttt{images-PR15} & \tablenum{0} & \tablenum{0} & \tablenum{0} & \tablenum{24} \\
    \texttt{largerGraphs} & \tablenum{0} & \tablenum{14} & \tablenum{30} & \tablenum{167} \\
    \texttt{LV} & \tablenum{90} & \tablenum{30} & \tablenum{489} & \tablenum{439} \\
    \texttt{meshes-CVIU11} & \tablenum{0} & \tablenum{13} & \tablenum{0} & \tablenum{23} \\
    \texttt{phase} & \tablenum{0} & \tablenum{0} & \tablenum{0} & \tablenum{0} \\
    \texttt{scalefree} & \tablenum{0} & \tablenum{0} & \tablenum{0} & \tablenum{80} \\
    \texttt{si} & \tablenum{0} & \tablenum{10} & \tablenum{102} & \tablenum{1135} \\
    ARG Database & \tablenum{1443} & \tablenum{141} & \tablenum{21965} & \tablenum{27305} \\
    \midrule
    Total & \tablenum{1533} & \tablenum{240} & \tablenum{22665} & \tablenum{30254} \\
    \bottomrule
  \end{tabular}
  \caption{The number of times each algorithm outperformed other algorithms, for
    each dataset.}
  \label{table:best}
\end{table}

Table~\ref{table:best} shows that most of the datasets have multiple algorithms
that managed to outperform the others for some problem instances. Thus, looking
at the differences between different datasets will not be enough to predict the
best algorithm. More specifically, Table~\ref{table:best} shows the numbers of
times that each algorithm's runtime was lower than or equal to the runtimes of
other algorithms. Therefore, if 2 or more lowest runtimes are equal (as can
often happen with single-digit runtimes), both algorithms are marked as winning
in the table.

Given this information, we would expect the ML algorithm to suggest using
\textsc{McSplit} and $\textsc{McSplit}{\downarrow}$ most of the time, occasionally
consider the clique encoding, and mostly forget about $k{\downarrow}$.

\subsection{Labelled Graphs} \label{sec:labelled_runtimes}

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/ecdf_vertex_labels.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/ecdf_both_labels.png}
  \end{subfigure}
  \caption{Cumulative plots with the vertical axis starting at 0.8.}
  \label{fig:ecdfs}
\end{figure}

We deal with the two types of labelling separately; however, since the results
are fairly similar, we describe them in the same section to highlight the
differences. We plot the ECDFs in Figure~\ref{fig:ecdfs}. The situation with
vertex-labelled graphs is quite straightforward: $\textsc{McSplit}{\downarrow}$ is
slight better than \textsc{McSplit}, which is better than the clique encoding.
Moreover, the VBS curve is significantly higher, providing plenty of room for an
ML model to outperform individual algorithms (unlike with unlabelled data). This
latter fact is true for fully labelled graphs as well, whereas
the other three curves provide a more interesting story. The clique algorithm is
briefly winning for shorter timeout values, then is between
$\textsc{McSplit}{\downarrow}$ and \textsc{McSplit} until it drops to the
3\textsuperscript{rd} place right before the final timeout at \num{1000000} ms.
This is likely because the clique encoding is better with higher labelling
percentages (we will see this shortly) and such graphs are generally easier to
solve (because there are fewer ``matchable'' combinations of vertices, each pair
of vertices is likely to have different labels).

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/vertex_labels_linechart.png}
    \includegraphics[width=\textwidth]{images/vertex_labels_linechart2.png}
    \includegraphics[width=\textwidth]{images/vertex_labels_linechart3.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/both_labels_linechart.png}
    \includegraphics[width=\textwidth]{images/both_labels_linechart2.png}
    \includegraphics[width=\textwidth]{images/both_labels_linechart3.png}
  \end{subfigure}
  \caption{For both types of labelling and for the three algorithms we plot how
    three characteristics change with respect to the labelling percentage:
    proportion of instances solved within the time limit, total runtime, and the
    number of times each algorithm outperformed the others.}
  \label{fig:linecharts}
\end{figure}

Just like with unlabelled graphs we could split the data into different subsets
based on how (and by whom) the graphs were generated, now we can analyse how the
situation changes with different labelling percentages. While some publications
explored several different values of the labelling percentage (33\%, 50\%, 75\%)
\cite{DBLP:conf/sspr/BunkeFGSV02, DBLP:journals/jgaa/ConteFV07,
  DBLP:conf/gbrpr/ConteGS03}, and some explored only one (33\%)
\cite{DBLP:conf/cp/McCreeshNPS16, DBLP:conf/ijcai/McCreeshPT17}, we explore a
much wider range from 5\% to 50\%, showing exactly when algorithms considered
best for labelled graphs stop being good, when faced with a smaller number of
labels. In Figure~\ref{fig:linecharts} we analyse two performance measures
(proportion of instances solved and total runtime) as well as the number of
times each algorithm ``wins'', for each labelling percentage. The two
performance statistics tell exactly the same story, and it is the same for both
types of labelling. The only difference is that the clique algorithm's major
drop in performance starts at 20\% for vertex-labelled graphs and at 15\% for
fully labelled graphs. With higher labelling percentages, the clique encoding is
better than \textsc{McSplit}, which is marginally better than
$\textsc{McSplit}{\downarrow}$. With lower levels of labelling, two differences
emerge: \textsc{McSplit} experiences a drop in performance below
$\textsc{McSplit}{\downarrow}$, and the performance of the clique algorithm
starts to drop exponentially. The latter is exactly what makes the clique
encoding finish last in Figure~\ref{fig:ecdfs}, even though it is in the lead
for most labelling percentages.

\begin{remark}
Note that we are only considering instances solved by at least one algorithm.
Out of the \num{30000} instances selected by our random sampling procedure, the
number of such instances ranges from 56\% with 5\% labelling to 99\% with 50\%
labelling for vertex-labelled graphs (the percentages for fully labelled graphs
are similar). Thus a (roughly) horizontal line should not be interpreted as the
algorithm performing equally well for different labelling percentages: the
performance of all algorithms improves with higher labelling percentages as the
problem becomes significantly easier. In this case we are more interested in the
differences between individual algorithms.
\end{remark}

As for the last two plots of Figure~\ref{fig:linecharts}, \textsc{McSplit} wins
more often than $\textsc{McSplit}{\downarrow}$. This is not surprising for
higher labelling percentages (based on the other two types of plots), but this
remains true for lower percentages as well. Perhaps this is because by
definition $\textsc{McSplit}{\downarrow}$ is better at handling problem
instances with a big answer. With a smaller labelling percentage,
\textsc{McSplit} is more likely to time out on those instances, contributing to
$\textsc{McSplit}{\downarrow}$ solving more instances than \textsc{McSplit}.
Perhaps $\textsc{McSplit}{\downarrow}$ is also much faster on those instances,
ensuring its lower total runtime, while consistently falling slightly behind
\textsc{McSplit} on easier instances, resulting in a lower win count. The one
significant difference between the two types of labelling is that the clique
encoding wins slightly less than $\textsc{McSplit}{\downarrow}$ with vertex
labels, but slightly more than \textsc{McSplit} with both vertex and edge
labels, for 25\%--33\% labelling. Finally, the main observation from this plot
is that the overall highest point is only at \num{16401} (\num{18031}) for
vertex-labelled graphs (both vertex and edge labels), just slightly above half
of the number of instances (\num{30000}), and other than the clique encoding
falling behind with $\le 15\%$ labelling, win rates of the three algorithms
stay similar. This makes the problem especially potent for an algorithm
selection approach.

\section{Graph Features} \label{sec:features}

The initial set of features is based on the algorithm selection paper for the
subgraph isomorphism problem \cite{DBLP:conf/lion/KotthoffMS16}, and consists of
the following:

\begin{enumerate}
\item number of vertices,
\item number of edges,
\item mean degree,
\item maximum degree,
\item density,
\item mean distance between pairs of vertices,
\item maximum distance between pairs of vertices,
\item standard deviation of degrees,
\item number of loops,
\item proportion of all vertex pairs with a distance of at least 2, 3, and 4,
\item whether a graph is connected.
\end{enumerate}

\begin{definition}
For a graph $G$ with $n$ vertices and $m$ edges, the \emph{(edge) density} is
defined to be the proportion of potential edges that $G$ actually has
\cite{DBLP:books/daglib/0030488}. The standard formula used for \emph{simple}
graphs (i.e., graphs with no multiple edges or loops
\cite{DBLP:books/ws/NishizekiR04}) is
\[ \frac{m}{\binom{n}{2}} = \frac{2m}{n(n-1)}. \]
\end{definition}
Even though some of our graphs do contain multiple edges and loops, we stick to
this formula, as it was used in the algorithm selection paper for the subgraph
isomorphism problem \cite{DBLP:conf/lion/KotthoffMS16}, and it does
not break the ML algorithm in any way to have the theoretical possibility of
density greater than 1.

We exclude feature extraction running time as a viable feature by itself (used
in the algorithm selection paper for the subgraph isomorphism problem
\cite{DBLP:conf/lion/KotthoffMS16}), since it would not provide any insight into
what properties of the graph affect which algorithm is likely to achieve the
best performance. Since $k{\downarrow}$ and $\textsc{McSplit}{\downarrow}$ both
start by looking for (complete) subgraph isomorphisms, they are likely to
outperform other algorithms when both graphs are very similar and the maximum
common subgraph has (almost) as many vertices as the smaller of the two graphs.
Thus, for each feature $f$ in features 1--7 (excluding the rest to avoid
division by 0), we also add a feature for the ratio $\frac{f(G_p)}{f(G_t)}$,
where $G_p$ and $G_t$ are the pattern and target graphs, respectively.

We analyse three different types of labelling and treat them as separate
problems: no labels, vertex labels, vertex and edge labels. For the last two
types, we add a feature corresponding to $p$ defined in
Definition~\ref{def:percent_labelling} and collect data for the following values
of $p$: 5\%, 10\%, 15\%, 20\%, 25\%, 33\%, 50\%\footnote{When working with both
  vertex and edge labels, we only consider using the same value of $p$ for both
  vertices and edges. This ``convention'' seems to have originated in a paper by
  the creators of the ARG Database \cite{DBLP:journals/jgaa/ConteFV07} and was
  replicated in subsequent papers on MCS algorithms
  \cite{DBLP:conf/cp/McCreeshNPS16, DBLP:conf/cp/NdiayeS11}.}. The values
correspond to having about 20, 10, 5, 4, 3, and 2 vertices/edges with the same
label on average, respectively.

\subsection{Distributions of Features}

In this section we discuss how the selected features are distributed in both
databases. In order to save space, all plots are in Appendix
\ref{appendix:plots}. As the graphs from Section~\ref{sec:unlabelled} contain
some very hard instances, we only consider graphs that are part of a pair of
graphs solved by at least one algorithm. In order to visualise highly skewed
data, we sometimes use density plots. Furthermore, we take $\log$
transformations of ratio features.

Firstly, one feature is not plotted as by definition it has only two possible
values. 99.81\% of graphs from Section~\ref{sec:labelled} are connected,
compared to 93.19\% of graphs from Section~\ref{sec:unlabelled}. As both numbers
are quite high, they may not be ideal for establishing if connectedness is a
significant factor in determining which algorithm performs the best, but might
be representative of real data in application domains such as chemistry
\cite{WCMS:WCMS5}. Similarly, the number of loops for graphs from
Section~\ref{sec:labelled} is not plotted as it varies between two values:
0.98\% of the graphs have a single loops, while the remaining majority of graphs
have no loops. On the other hand, as shown in Figure~\ref{fig:loops}, some
(although not many) graphs from Section~\ref{sec:unlabelled} have significantly
more loops.

Most of the features of graphs from Section~\ref{sec:labelled} are displayed in
Figure~\ref{fig:mcs_features1}. Other than the number of vertices, which is
manually controlled by the creators of the database, all other distributions are
centred around lower values, with some outliers on the high end. More
importantly, we have some graphs that are quite dense and some graphs with
higher mean distance values.

The same plots for graphs from Section~\ref{sec:unlabelled} in
Figure~\ref{fig:sip_features1} show a similar story, albeit for clearer reasons.
Since we filter out graphs that none of the algorithms were able to handle, our
sample consists of all the easy instances and some harder instances that were
solved by one or two algorithms. Harder instances typically have more vertices,
which means they are also capable of higher values for many other features,
hence all of the density plots are right-skewed.

Figure~\ref{fig:proportions} shows density plots of proportions of pairs of
vertices with distance at least $k$ for $k = 2, 3, 4$. For both databases, as
$k$ increases, the distributions shift to the left as expected. However, there
is one important difference: even with $k = 4$ the plot for graphs from
Section~\ref{sec:unlabelled} has its highest peak around 0.9, which means that
adding features for $k \ge 5$ could be valuable. 

Finally, the density plots of $\log$-transformed ratio features are in
Figures~\ref{fig:ratio_vertices}, \ref{fig:mcs_ratio}, and \ref{fig:sip_ratio}.
Almost all of these plots for graphs from Section~\ref{sec:unlabelled} (with the
exception of the ratio of mean degree) are clearly bimodal with one of the two
modes centred around 0. Hence we can infer the existence of two subpopulations:
one where pattern and target graphs have very similar properties (the majority
for most features) and one where pattern and target graphs are very different.
As for the graphs from Section~\ref{sec:labelled}, the plot of the ratio of the
number of vertices in Figure~\ref{fig:ratio_vertices} is perfectly symmetrical
and centred around 0, since the number of vertices is a controlled variable for
this database. All of the remaining plots for ratio features have most of the
data very close to 0. Thus, the differences between pattern and target graphs
are very small. Furthermore, all distributions are symmetrical---the pattern
graph is just as likely to be larger/denser/etc. as the opposite.

\subsection{Correlations}

Another important question is how the features are correlated with each other.
Even though research suggests that Pearson's correlation coefficient can be
quite robust to violations of the data normality assumption \cite{article}
(which is clearly violated with our data), we choose Spearman's correlation
coefficient as the more appropriate alternative:

\begin{definition}
  \emph{Spearman's rank correlation coefficient} \cite{mood1974introduction}
  between vectors $\bm{x}, \bm{y} \in \mathbb{R}^n$ is defined as
\[ S = \frac{\sum_i [r(x_i) - \overline{r}(\bm{x})][r(y_i) -
    \overline{r}(\bm{y})]}{\sqrt{\sum_i [r(x_i) -
      \overline{r}(\bm{x})]^2\sum_i [r(y_i) -
      \overline{r}(\bm{y})]^2}} \],
where $\overline{r}(\bm{x}) = \sum_i r(x_i)/n$, $\overline{r}(\bm{y})
= \sum_i r(y_i)/n$, and $r$ is a rank function, which, for each of the two
vectors, assigns a natural number to each element such that $x_i > x_j \implies
r(x_i) > r(x_j)$, for any two elements of $\bm{x}$.
\end{definition}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{images/feature_correlations.png}
  \caption{Spearman's rank correlation coefficients between all pairs of
    features, for both databases of graphs. Each correlation coefficient is
    represented by colour, with dark red for strong negative correlations,
    dark blue for strong positive correlations, and white for a correlation
    coefficient that is close to zero.}
  \label{fig:feature_correlations}
\end{figure}

The resulting correlation coefficients are pictured in Figure
\ref{fig:feature_correlations}. Ratio features, connectedness, and numbers of
loops have almost no significant correlations, unlike the rest of the features
that have significant positive and negative correlations. There is also a
noticeable symmetry between features of the pattern and target graphs, i.e., if
feature $f$ of the pattern graph correlates with features $g$ and $h$ of the
pattern graph, then it also correlates with features $f, g, h$ of the target
graph.

\chapter{Machine Learning Models \& Their Evaluation} \label{chapter:ml}

After running the algorithms  with different types and percentages of labelling
and recording their running times, an ML model can be trained to predict which
algorithm should be chosen for each pair of graphs. For each pair of graphs,
\textsc{Llama} \cite{llama} (the package that facilitates constructing algorithm
portfolios) can take:

\begin{itemize}
\item A list of features. With separate features for pattern and target graphs
  as well as ratio features and the labelling percentage, we have 34 features in
  total.
\item A list of performance measures for all algorithms, i.e., the values that
  we are trying to optimise. In this case (as in most cases), this corresponds
  to running time. The values are capped at the timeout value (\num{1000000}~ms).
  Furthermore, instances that were not run on the clique algorithm are also set
  to the timeout value. Finally, we filter out instances where all algorithms
  timed out.
\item A list of Boolean values, denoting whether each algorithm successfully
  finished or not. Timeouts, the clique algorithm running out of memory, and
  instances that were not run with the clique algorithm because of their size
  are all marked as false.
\item A data frame, measuring the running time taken to compute each feature for
  each problem instance, a single number for the approximate time taken to
  compute all features for any instance, or a list with one or more costs per
  problem instance. We use the last option with a single cost per instance. This
  parameter is used to ensure a fair comparison when comparing the portfolio
  against other algorithms: the runtime of the portfolio is defined as the
  runtime of its chosen algorithm together with feature extraction time.
  While costs for graphs from Section~\ref{sec:unlabelled} reached up to 65 s,
  the maximum cost for graphs from Section~\ref{sec:labelled} is only 6 ms, with
  distance-based features being more expensive to compute.
\end{itemize}

After constructing the required data frames as described above, the data needs to
be split into training and test sets. We use a technique called 10-fold
\emph{cross-validation}, which splits the data into 10 parts \cite{citeulike:1304145}.
9/10\textsuperscript{ths} of the data is used to train the ML algorithm, while
the remaining 1/10\textsuperscript{th} is used to evaluate how good the trained
model is. This process of training and evaluation is repeated 10 times, letting
each of the 10 parts be used for evaluation exactly once. The goodness-of-fit
criteria are then averaged out between the 10 runs.

The 10 folds could, of course, be chosen completely randomly. However, research
suggests that stratified cross-validation typically outperforms
random-sampling-based cross-validation and results in a better model
\cite{DBLP:conf/ijcai/Kohavi95}. Suppose we have a dataset of $N$ elements.
\emph{Stratified sampling} partitions it into a number of subpopulations $s_1,
\dots, s_k$ with $n_1, \dots, n_k$ elements, respectively (typically based on
the value of some feature or collection of features). It then draws from each
subpopulation independently, ensuring that approximately $n_i/N$ of the sample
comes from subpopulation $s_i$ for $i = 1, \dots, k$ \cite{lohr2009sampling}. In
this case the data is partitioned into four groups based on which algorithm
performed best.

The cross-validation folds are then used to generate predictions on all data,
where each prediction is made by a model that did not have that observation in
its training data set. The predictions are then used to compare the ML model with
individual algorithms and the VBS using ECDF plots and numeric statistics.
\textsc{Llama} \cite{kotthoff_llama_2013, llama} supports algorithm portfolios
based on three different types of ML algorithms:

\begin{description}
  \item[Classification] The ML algorithm predicts which algorithm is likely to
    perform best on each problem instance.
  \item[Regression] Each algorithm's data is used to train a separate ML model,
    predicting the algorithm's performance. The winning algorithm can then be
    chosen based on those predictions.
  \item[Clustering] All instances of the training data are clustered and the
    best algorithm is determined for each cluster. New problem instances can
    then be assigned to the nearest cluster.
\end{description}

We are using a classification algorithm called random forests
\cite{DBLP:journals/ml/Breiman01}, implemented in R \cite{randomforest}. We
chose this algorithm as it is recommended in the \textsc{Llama} manual
\cite{kotthoff_llama_2013} and successfully used in a similar study
\cite{DBLP:conf/lion/KotthoffMS16}. We use the default number of trees (500),
and our trees have about \num{6000}, \num{20000}, and \num{21000} vertices on
average for unlabelled, vertex-labelled, and fully labelled graphs,
respectively.

In order to discuss and analyse the ML algorithm in more detail, we introduce
some new terminology. Each problem instance with features and running times in
the training dataset is called an \emph{observation}. The \emph{class} of an
observation is the algorithm with lowest running time for that problem instance.
Previously discussed features of graphs are sometimes referred to as
\emph{(independent) variables}. The \emph{(problem) instance space} is the
Cartesian product of the domains of features \cite{DBLP:series/smpai/RokachM14},
where a \emph{domain} of $X$ (denoted $\dom X$) is a set of all possible values
that $X$ can take.

A \emph{(classification) decision tree} is ``a classifier expressed as a
recursive partition of the instance space'' \cite{DBLP:series/smpai/RokachM14}.
Typically, it can be represented as a rooted binary tree, where each internal
node \emph{splits} the instance space into two regions based on the value of one
of the features\footnote{Other possibilities include a node having more than two
  children and a split being made in a more complicated way. Although trees with
  such properties fit the definition of a decision tree, standard machine
  learning algorithms tend to be more restrictive \cite{James:2014:ISL:2517747,
    DBLP:series/smpai/RokachM14}.}. For example, for some feature $X$ and a
particular value $x \in \dom{X}$, the left child might be assigned all
observations with $X < x$, while the right child would get observations with $X
\ge x$. For each leaf node, we can count how many of its observations belong to
each class and assign the most common class to that node.

When a decision tree is used to make a classification prediction, a data point
travels from node to node (starting at the root node) according to the splitting
rules. When it reaches a leaf node, the class assigned to that node is outputted
as the tree's prediction.

A \emph{random forest} builds a collection of decision trees
\cite{James:2014:ISL:2517747}. Given $p$ variables, each time a split is
considered, the variable to split on is chosen from $\sqrt{p}$ rather than all
$p$ variables. Therefore, the strongest predictors are sometimes not even
considered, ensuring a level of diversity among trees. An individual tree's
prediction is called a \emph{vote}. A classifying random forest predicts by
collecting the votes from all its trees and predicting the class with the
highest number of votes.

As we saw in Section~\ref{sec:features}, some of the feature data is highly
skewed. Thus we should formally address the question of using transformations
(such as $\log$ used for some of the plots or $x \mapsto 1/x$, giving
the labelling percentage a different interpretation) before feeding the
data into the ML algorithm. While it is a commonly held belief that
predictor transformations are unnecessary for decision tree-based learning
algorithms \cite{DBLP:journals/classification/Friedman06,
  DBLP:books/lib/HastieTF09, cart}, they can affect the predictions of
previously unseen data \cite{DBLP:journals/corr/GaliliM16}. To see this,
consider a split between some values $x_1, x_2 > 0$ at $b =
\frac{x_1+x_2}{2}$ (which is the type of split used by the
\texttt{randomForest} package we are using
\cite{DBLP:journals/corr/GaliliM16}) and consider the same situation after
applying the transformation $x \mapsto x^2$. The new boundary is $b' =
\frac{x_1^2+x_2^2}{2}$ and it is easy to find $x > b$ such that $x^2 < b'$.
Then, if this is the final split and $x_1$ and $x_2$ have different predictions,
$x$ will have different predictions with and without the transformation. In
this project we do not use transformations as having over \num{100000} data
points and 500 trees reduces this effect and good algorithm portfolios are
created without even mentioning transformations
\cite{DBLP:conf/lion/KotthoffKHT15, DBLP:conf/lion/KotthoffMS16}. However, we
would encourage future researchers to consider $\log$-transforming highly skewed
features, especially if using an algorithm more sensitive to the distribution of
data.

Lastly, we use the \texttt{parallelMap} package to train the model using
multiple threads and the R code was heavily optimised to remove temporary
variables as soon as they are no longer needed in order to reduce memory
consumption. 

\section{Unlabelled Graphs}

In this section we introduce various metrics and plots that can be used to
examine a random forest, and use them to evaluate our model for unlabelled
graphs. We examine how well the model predicts the winning of each algorithm,
how important each feature is in making a prediction, how convinced the model is
of each prediction, which algorithms prefer what values of various features, and
how well the model performs compared to individual algorithms and the VBS.

\subsection{Error Rates} \label{sec:unlabelled_error_rates}

\begin{figure}
  \centering
  \includegraphics[scale=0.5]{images/unlabelled_forest_errors.png}
  \caption{Convergence plot of various error measures as the number of trees in
    a random forest increases. The plot shows the OOB error and $1 -
    \text{recall}$ for each algorithm.}
  \label{fig:unlabelled_forest_errors}
\end{figure}

Random forests support a convenient way to estimate the test error without
cross-validation or any other kind of data splitting. Each tree in a random
forest uses around 2/3 of the data \cite{James:2014:ISL:2517747}. The remaining
1/3 is referred to as \emph{out-of-bag (OOB)} observations . For each
observation in the data, we can predict the answer (the vote on which algorithm
is expected to win) using all trees that have the observation as OOB. The
majority vote is then declared to be the answer. The \emph{OOB error} is
the relative frequency of incorrect predictions \cite{James:2014:ISL:2517747}.
As each prediction was made using trees that had not seen that particular
observation before, OOB error is a valid estimate of test error. The black line
in Figure~\ref{fig:unlabelled_forest_errors} shows how the error converges to
about 17\% as we number of trees reaches 500.

The other lines in the figure, one for each algorithm, are defined as $1 -
\text{recall}$, where, for an algorithm $A$, \emph{recall}
\cite{citeulike:12882259} is
\[ \frac{\text{the number of instances that were correctly predicted as
      $A$}}{\text{the number of instances where $A$ is the correct
      prediction}}. \]

The error rates for $\textsc{McSplit}{\downarrow}$, \textsc{McSplit}, the clique
encoding, and $k{\downarrow}$ converge to 11\%, 29\%, 30\%, and 80\%,
respectively. Unlike the errors of other classes, the error of $k{\downarrow}$ has
an upward trend and converges to a very high value. Perhaps the model eventually
learns not to predict $k{\downarrow}$ and the data points with $k{\downarrow}$
winning are treated as randomness in the data more so than a statistically
significant trend.

\subsection{Variable Importance}

Next we are going to explore how important each feature is in making
predictions, but for that we need to introduce some new definitions. Consider a
single tree $T$ in a random forest. The root of $T$ can be reached by any
observation, regardless of the values of its features. After passing some node
$n$, some feature is restricted, i.e., it is imposed an upper or lower limit on
the kind of values it can have for it to move towards a particular child of $n$
\cite{James:2014:ISL:2517747}. We will refer to a part of feature space that an
observation can be in while at some node $n$ as a \emph{region}.

\begin{definition}
  Suppose we have $K$ classes. Consider some region $m$. The \emph{Gini index}
  is then defined as
  \[ G = \sum_{k=1}^K \hat{p}_{mk}(1-\hat{p}_{mk}), \]
  where $\hat{p}_{mk}$ represents the proportion of observations in region $m$
  that are from class $k$ (i.e., have algorithm $k$ as the best algorithm)
  \cite{James:2014:ISL:2517747}.
\end{definition}
As we move down a tree, we want the region to be restricted to a single class.
Then the observations from the training data satisfying the conditions imposed
by the parent nodes would be classified with perfect accuracy. The Gini index is
at its lowest when all proportions $\hat{p}_{mk}$ are close to either 0 or
1\footnote{Note that $G=0$ when any single $\hat{p}_{mk}=0$, regardless of the
  values of other proportions. Therefore, $G=0$ does not automatically make a
  tree into a good classifier.}, meaning that almost all observations in the
region belong to a single class. Hence the Gini index is often used to evaluate
the quality of a split.

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/unlabelled_variable_importance.png}
    \caption{Dot chart of variable importance calculated based on the Gini index
      and sorted from most important to least important.}
    \label{fig:unlabelled_variable_importance}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/unlabelled_var_used.png}
    \caption{How often was each variable used to make splitting decisions?}
    \label{fig:unlabelled_var_used}
  \end{subfigure}
\end{figure}

The variable importance measure of feature $f$ in
Figure~\ref{fig:unlabelled_variable_importance} is calculated as the amount by
which the Gini index decreases after passing nodes that use feature $f$,
averaged over all trees in the random forest \cite{James:2014:ISL:2517747}.
Looking at the figure more closely, the standard deviations of degrees of both
target and pattern graphs are by far the most important predictors.
Unsurprisingly, the worst predictors are the features with very low variance:
number of loops and connectedness of both graphs. Perhaps more surprisingly, the
ratio features are not as successful as one might have hoped: the ratio of the
numbers of vertices is at the bottom 5\textsuperscript{th} place and the best
ratio feature, the mean distance ratio, is only 10\textsuperscript{th}. The last
thing to note is that features of the pattern graph are always behind the same
features of the target graph and usually not far behind. Perhaps this is due to
some datasets having fewer pattern graphs, or pattern graphs having fewer
vertices. The variable usage plot in Figure~\ref{fig:unlabelled_var_used} tells
a similar story: the orders are not identical, but there are no big outliers.

\subsection{Margins}

\begin{definition}
  Let $c_1, \dots, c_n$ be $n$ classes and let $p$ be a data point that belongs
  to class $c_p$. Let $v_1, \dots, v_n$ denote the number of votes for each
  class when given $p$ as input. The \emph{margin} of $p$ is
  \[ \frac{v_p}{\sum_{i=1}^n v_i} - \max_{i \ne p} \frac{v_i}{\sum_{j=1}^n v_j}, \]
  which is a number in $[-1, 1]$ \cite{forest}.
\end{definition}

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/unlabelled_margin.png}
    \caption{Sorted}
    \label{fig:unlabelled_margins1}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/unlabelled_margin2.png}
    \caption{Unsorted}
    \label{fig:unlabelled_margins2}
  \end{subfigure}
  \caption{Margins of the data points.}
  \label{fig:unlabelled_margins}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/clique_hist.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/kdown_hist.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/mcsplit_hist.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/mcsplitdown_hist.png}
  \end{subfigure}
  \caption{Histograms of margins for each winning algorithm.}
  \label{fig:unlabelled_margin_hist}
\end{figure}

A value above 0 means that the forest as a whole predicted correctly. A margin
of 1 would mean that all trees voted correctly. In
Figure~\ref{fig:unlabelled_margins} we plot the margins in the following way:
for each problem instance, the colour signifies the winning algorithm and the
height shows its margin. Figure~\ref{fig:unlabelled_margins1} shows the points
sorted by the margin, while in Figure~\ref{fig:unlabelled_margins2} they are
left in the original order of the data (which mostly corresponds to the order of
files in the databases, but with some variation since experiments are started in
order but end at different times). We can recognise the same error rates as in
Figure~\ref{fig:unlabelled_forest_errors} as well as areas where
\textsc{McSplit} and $\textsc{McSplit}{\downarrow}$ dominate. We also plot the
histograms of how the margins are distributed for each algorithm in
Figure~\ref{fig:unlabelled_margin_hist}. We note that:

\begin{itemize}
\item Instances best handled with the clique encoding are usually recognised, but
  with significant uncertainty.
\item We are usually wrong about $k{\downarrow}$ (probably because it is a
  winning algorithm in only 0.44\% of all cases).
\item When faced with an instance that is best handled with
  $\textsc{McSplit}{\downarrow}$, the vast majority of trees vote correctly.
\item \textsc{McSplit} detection rates are decent, but far behind those of
  $\textsc{McSplit}{\downarrow}$.
\end{itemize}

\subsection{Partial Dependence} \label{sec:unlabelled_partial}

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/mcsplit_partial.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/clique_partial.png}
  \end{subfigure}
  \caption{Partial dependence plots of the standard deviation of degrees in the
    target graph.}
  \label{fig:unlabelled_partials}
\end{figure}

Since the standard deviations of degrees in both target and pattern graphs are
the most important features, we plot partial dependence plots of the standard
deviation of degrees in the target graph\footnote{The plots for the standard
  deviation of degrees of the pattern graph are omitted since they are identical
  to those of the target graph.} for $\textsc{McSplit}{\downarrow}$ and the
clique encoding\footnote{We omit the plots for the other two algorithms as the
  plot for \textsc{McSplit} looks the same as the one for
  $\textsc{McSplit}{\downarrow}$ and prediction success rate for $k{\downarrow}$
  is so low that a plot for $k{\downarrow}$ would be meaningless.} in
Figure~\ref{fig:unlabelled_partials}. The plotted function \cite{forest} is
defined as
\[ f(x) = \log{p_k(x)} - \frac{1}{K} \sum_{i=1}^K \log{p_i(x)}, \]
where:

\begin{itemize}
\item $x$ is the value on the horizontal axis (in this case standard deviation of
  degrees in the target graph),
\item $p_i(x)$ is the proportion of votes for class $i$ for a problem instance
  with a standard deviation of degrees in the target graph equal to $x$,
\item $K$ is the number of classes,
\item and $k$ is the main class under consideration
  ($\textsc{McSplit}{\downarrow}$ and the clique encoding).
\end{itemize}

Essentially, $f(x)$ compares the proportion of votes for class $k$ with the
average value over all classes. We can deduce that a low standard deviation of
degrees is a strong sign that \textsc{McSplit} and $\textsc{McSplit}{\downarrow}$
should perform well. On the other hand, the clique encoding is expected to
perform better on graphs with high variance in degrees. However, $\max f(x)$ for
the clique encoding is just barely above 0 and much lower than $\min f(x)$ for
$\textsc{McSplit}{\downarrow}$, meaning that the standard deviation of degrees
does not provide enough information to choose the clique encoding over
\textsc{McSplit} or $\textsc{McSplit}{\downarrow}$.

\subsection{Runtime Comparison}

\begin{figure}
  \centering
  \includegraphics[scale=0.5]{images/ecdf_unlabelled_llama.png}
  \caption{\textsc{Llama} model compared to the VBS and
    $\textsc{McSplit}{\downarrow}$.}
  \label{fig:ecdf_unlabelled_llama}
\end{figure}

In order to compare our ML model with other algorithms, we treat the VBS as the
upper bound and the single best solver $\textsc{McSplit}{\downarrow}$ as the
lower bound. Out of \num{45468} instances solved by at least one algorithm, our
model managed to solve \num{45290}, compared to \num{45223} solved by
$\textsc{McSplit}{\downarrow}$. In other words, it was able to close 27.3\% of
the gap between our lower and upper bounds in terms of instances solved within
the time limit. Figure~\ref{fig:ecdf_unlabelled_llama} shows how the ML model
compares to the VBS and the single best solver $\textsc{McSplit}{\downarrow}$
(note that the vertical axis starts at 0.9 rather than 0). Unsurprisingly, the
model outperforms $\textsc{McSplit}{\downarrow}$, but does not reach the best
possible performance represented by the VBS.

\section{Labelled Graphs} \label{sec:ml_labelled}

\begin{table}
  \centering
  \begin{tabular}{l c c}
    \toprule
    Error type & Final value for vertex labels (\%) & Final value for both labels (\%) \\
    \midrule
    OOB & \tablenum{13} & \tablenum{14} \\
    clique & \tablenum{8} & \tablenum{7} \\
    \textsc{McSplit} & \tablenum{22} & \tablenum{29} \\
    $\textsc{McSplit}{\downarrow}$ & \tablenum{11} & \tablenum{11} \\
    \bottomrule
  \end{tabular}
  \caption{The values that all 4 errors (approximately) converge to, for both
    types of labelling.}
  \label{table:errors}
\end{table}

In order to save space, we leave the most important data and conclusions in this
section, while moving supplementary plots to Appendix \ref{appendix:plots2}.
Just like in the previous section, we plot the error rates (defined in
Section~\ref{sec:unlabelled_error_rates}) in Figure~\ref{fig:forest_errors}. The
final error values are summarized in Table~\ref{table:errors}. This time all
errors converge downwards and are below 50\%. Comparing vertex-labelled and
fully labelled subproblems, the only noticeable difference is that
\textsc{McSplit} has a lower error rate for vertex-labelled graphs, which is
also the worst-recognised algorithm for both types of labelling.

According to the variable importance measures plotted in
Figure~\ref{fig:variable_importance}, the standard deviations of degrees for
both pattern and target graphs still act as top predictors, however, they are
overshadowed by the labelling feature, which is to be expected considering how
impactful it is to the performance of the clique algorithm and the difficulty of
the problem in general. For graphs with both vertex and edge labels, the
vertex/edge counts make up the next most important predictors, while for
vertex-labelled graphs, the number of vertices in the target graph seems to be
less important (perhaps simply due to chance). Comparing this with the variable
usage statistic in Figure~\ref{fig:var_used} (which is almost identical for the
two subproblems), the top 3 spots remain the same, the numbers of edges drop to
the middle of the list, and the numbers of vertices drop to the
6\textsuperscript{th}--7\textsuperscript{th} places from the bottom. Apparently,
even though all four of these predictors end up doing a great job at splitting
the data to reduce the Gini index (a variation of which is used by the random
forest algorithm in choosing which predictor to split on \cite{167153}), they
are rarely used. 

We show the margin plots and histograms in Figures~\ref{fig:margins} and
\ref{fig:margin_hist}. Again, the data for both types of labelling is close to
identical. Note that there are plenty of data points in all three colours and the ML
models are usually very convinced when predicting $\textsc{McSplit}{\downarrow}$
and the clique encoding and are less sure when dealing with problem instances
best handled with \textsc{McSplit}.

This time we show the partial dependence plots for the top two most
important predictors (the labelling percentage and the standard deviation of
degrees in the target graph\footnote{The second most important predictor for the
fully labelled case is the standard deviation of degrees in the pattern graph,
but the difference is negligible and very likely to be due to randomness.}) in
Figures~\ref{fig:labelling_partials} and \ref{fig:stddeg_partials},
respectively. In both cases there is little difference between the plots for
vertex-labelled and fully labelled graphs. Consider the labelling plots first.
The results for the clique encoding are the most straightforward to interpret.
Clearly, the algorithm performs much better with higher labelling percentages
(more different labels). The interesting bit (just like back in
Figure~\ref{fig:linecharts}) is that the curve stays constant between 20\% and
50\%. In other words, anywhere above 20\% labelling, a higher labelling
percentage does not make the clique encoding more preferable than it already is.
Furthermore, once again we can recognise how the curve descends more slowly for
both labels than it does for vertex labels. On the other hand, both
\textsc{McSplit} and $\textsc{McSplit}{\downarrow}$ prefer lower labelling
percentages.

The partial dependence on the standard deviation of degrees of the target graph
says what we already knew from Section~\ref{sec:unlabelled_partial}: the clique
encoding prefers graphs with more variance in degrees. The one small difference
is that this time the change is not as steep, but this can be easily explained
by the fact that the highest standard deviation of degrees is around 7 in this
section compared to around 70 in Section~\ref{sec:unlabelled_partial}. While
$\textsc{McSplit}{\downarrow}$ prefers graphs with smaller standard deviations of
degrees, the situation with \textsc{McSplit} is suddenly less clear, which is
reflective of the fact that the ML models have less confidence about
\textsc{McSplit} (compared to the other 2 algorithms). While small standard
deviations are still preferred, there is a significant drop between the values
of 1 and 2.

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/ecdf_vertex_labels_llama.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/ecdf_both_labels_llama.png}
  \end{subfigure}
  \caption{\textsc{Llama} models compared to other algorithms and the VBS.}
  \label{fig:ecdf_llama}
\end{figure}

Finally, for the runtime comparison, we plot the ECDFs for all individual
algorithms, the VBS, and the \textsc{Llama} models in
Figure~\ref{fig:ecdf_llama}. While there are important differences among
individual algorithms' curves (discussed in
Section~\ref{sec:labelled_runtimes}), the way \textsc{Llama} behaves with
respect to the VBS and $\textsc{McSplit}{\downarrow}$ curves is about the same:
\textsc{Llama}'s curve is quite close to optimal, and the gap between
\textsc{Llama} and $\textsc{McSplit}{\downarrow}$ is significantly wider than
the gap between $\textsc{McSplit}{\downarrow}$ and \textsc{McSplit}. To add more
numbers to this picture, \textsc{Llama} closed 86\% and 88\% of the difference
between its lower and upper bounds for vertex-labelled and fully labelled
graphs, respectively.

\chapter{Further Suggestions, Investigations, and Experiments} \label{chapter:further}

In this chapter we build on the knowledge gained by developing ML models and
analysing algorithms' performance to extend current algorithms and propose new
ones. We suggest a way to extend $k{\downarrow}$ to support vertex-labelled
graphs in a non-trivial manner and introduce the idea of combining
\textsc{McSplit} and the clique encoding into a single algorithm.

\section{Modifications to \texorpdfstring{$k{\downarrow}$}{kdown}}

The $k{\downarrow}$ algorithm was modified to accept graphs with vertex
labels by adding an additional constraint for matching labels on line 8 of the
\texttt{klessSubgraphIsomorphism} function \cite{DBLP:conf/aaai/HoffmannMR17}
and extending the \texttt{Graph} class with two new fields: a \texttt{vector},
assigning a label to each vertex, and a \texttt{vector} of \texttt{vector}s,
which, for each label, list all vertices that have that label. After generating
most of the data, it was noticed that the vertex-labelled version of
$k{\downarrow}$ was winning precisely 0 times and thus was not considered in
Sections~\ref{sec:labelled_runtimes} and \ref{sec:ml_labelled}.

In order to make $k{\downarrow}$ more competitive, the neighbourhood degree
sequence filtering was improved to make use of the additional information that
labels provide. It is also part of line 8 of the same function in the original
paper \cite{DBLP:conf/aaai/HoffmannMR17}, however, it is not enabled by
default\footnote{\url{https://github.com/ciaranm/aaai17-between-subgraph-isomorphism-and-maximum-common-subgraph-paper}}
and is not used in the experiments of the \textsc{McSplit} paper
\cite{DBLP:conf/ijcai/McCreeshPT17}. We take the definition directly from the
$k{\downarrow}$ paper \cite{DBLP:conf/aaai/HoffmannMR17}:

\begin{definition}
  ``The \emph{neighbourhood degree sequence} (NDS) of a vertex $p$, $S(p)$, is the
  (non-ascending) sequence of degrees of its neighbours.''
\end{definition}

\begin{definition} \label{def:prec}
  For two sequences $S = (s_1, \dots, s_n)$ and $T = (t_1, \dots, t_m)$ and some
  non-negative integer $k$, we say that $S \kprec{k} T$ if $n - k \le m$ and
  there is a subsequence $S_k$ of $S$ with $|S_k| \le k$ such that
  \cite{DBLP:conf/aaai/HoffmannMR17}
  \[ \forall s_i \in S \setminus S_k, \text{ there exists a distinct } j \in \{
    1, \dots, m \} \text{ such that } s_i - k \le t_j. \]
\end{definition}
As a simplification of the definition above, we state the following lemma, based
on Corollary 1 from the $k{\downarrow}$ paper \cite{DBLP:conf/aaai/HoffmannMR17}.

\begin{lemma} \label{lemma1}
  Let $S = (s_1, \dots, s_n)$ and $T = (t_1, \dots, t_m)$ be two sequences and
  $k$ an integer such that $k \ge \max\{ 0, n - m \}$. Then $S \kprec{k} T$
  if and only if
  \[ s_i \le t_{i-k} + k \quad \forall i = k + 1, \dots, n. \]
\end{lemma}

\begin{proof}
  A simple extension of Corollary 1 from the $k{\downarrow}$ paper
  \cite{DBLP:conf/aaai/HoffmannMR17} by a reordering argument.
\end{proof}

We also have Proposition 3 from the same paper
\cite{DBLP:conf/aaai/HoffmannMR17}, which is useful to our proposed algorithm:

\begin{proposition} \label{prop:kdown}
  Given some $k$, if a pattern graph vertex $p$ is mapped to a target graph
  vertex $t$ in a subgraph isomorphism that excludes up to $k$ vertices from the
  pattern graph, then
  \[ S(p) \kprec{k} S(t). \]
\end{proposition}

\begin{algorithm}
  \SetKwData{pNds}{pNds}
  \SetKwData{tNds}{tNds}
  \SetKwData{neighbours}{NeighboursUniqueToPattern}
  \KwData{a non-negative integer $k$, a list of NDSs for each label
    \pNds, a list of NDSs for each label \tNds}
  \KwResult{a Boolean value, indicating whether $p$ and $t$ can be matched}
  $\neighbours \gets \sum_{i=|\tNds|}^{|\pNds| - 1} |\pNds[i]|$\;
  \lIf{$\neighbours > k$}{\Return{false}}
  \For{$i \gets 0$ \KwTo $\min\{ |\pNds|, |\tNds| \} - 1$}{
    \If{$|\pNds[i]| - |\tNds[i]| > 0$}{
      $\neighbours \gets \neighbours + |\pNds[i]| - |\tNds[i]|$\;
      \lIf{$\neighbours > k$}{\Return{false}}
    }
    \lIf{$\pNds[i] \cancel{\kprec{k}} \tNds[i]$}{\Return{false}}
  }
  \Return{true}\;
  \caption{NDS filtering with vertex label support.}
  \label{alg:nds}
\end{algorithm}

We extend these ideas in Algorithm~\ref{alg:nds} by considering an NDS for each
label. The algorithm checks whether a vertex $p$ in the pattern graph can be
matched with a vertex $t$ in the target graph.
\textsf{NeighboursUniqueToPattern} tracks the number of neighbours of $p$ that
have to be excluded, i.e., whenever the pattern graph has more neighbours with a
particular label than the target graph, we can add the difference to
\textsf{NeighboursUniqueToPattern}, and if it ever becomes larger than $k$, we
know that it is impossible to map $p$ to $t$. The other early exit condition is
the contrapositive of Proposition~\ref{prop:kdown}. If neither of these
conditions is satisfied throughout the \textbf{for} loop that starts on line 5,
then $p$ and $t$ are compatible for this value of $k$.

\begin{figure}
  \centering
  \includegraphics[scale=0.5]{images/ecdf_kdown.png}
  \caption{Cumulative plot of all algorithms with 10\% vertex labelling.}
  \label{fig:ecdf_kdown}
\end{figure}

As this improvement is an afterthought after the main experiments, the resulting
algorithm was run on a smaller subset of all data: \num{30000} instances with
10\% labelling. The ECDF plot in Figure~\ref{fig:ecdf_kdown} shows the new
$k{\downarrow}$ to be just slightly behind the clique encoding and it outperforms
other algorithms in 193 instances, making it worth consideration for an
inclusion in a portfolio, although it might achieve similar results as with
unlabelled graphs, where it wins too infrequently to be a valuable inclusion.

\section{The Clique Algorithm and its Association Graph} \label{sec:clique}
% TODO: intro the whole ordeal once it's done

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/vertices_density.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/edges_density.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/meandeg_density.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/maxdeg_density.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/stddeg_density.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/density_density.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/time_density.png}
  \end{subfigure}
  \caption{Density plots of the selected features of the association graph and
    the clique algorithm's runtime. Note that the number of edges and runtime
    were $\log$-transformed in order to improve clarity.}
  \label{fig:densities}
\end{figure}

% TODO: rewrite in a better way
The next question we want to answer is ``how does the clique algorithm's running
time depend on properties of the association graph?'' To answer this question we
record some of the features from Section~\ref{sec:features} about the
association graph. More specifically, we record the features that are easy to
compute (since association graphs can get quite big) and can be informative:
number of vertices, number of edges, mean degree, max degree, standard deviation
of degrees, and density. We record this information for association graphs
created for all unlabelled instances from Section~\ref{sec:labelled} (since the
graphs from Section~\ref{sec:unlabelled} often stretch the memory limits of the
algorithm) as well as the \num{30000} instances previously selected for
experiments with labelled graphs, for all 7 labelling percentages. As the
timed-out experiments would make interpreting any results much harder, we remove
them from the data and are left with about \num{300000} (instead of about
\num{500000}) observations. The distribution of each column of the remaining
dataset is plotted in Figure~\ref{fig:densities}.

\begin{figure}
  \centering
  \includegraphics[scale=0.5]{images/correlations.png}
  \caption{Spearman's rank correlation coefficients between the measured
    variables.}
  \label{fig:correlations}
\end{figure}

To keep things simple and not violate assumptions about independent variables,
we will only consider correlations between the variables instead of trying to
fit some kind of a regression model. The resulting Spearman's rank correlation
coefficients are in Figure~\ref{fig:correlations}. Note that all variables are
positively correlated: some correlations are very strong, some average, and one
very weak (between density and the standard deviation of degrees). Also note
that all measured characteristics of the association graph are positively
correlated with the algorithm's running time. 

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/vertices_bins.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/edges_bins.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/meandeg_bins.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/maxdeg_bins.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/stddeg_bins.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/density_bins.png}
  \end{subfigure}
  \caption{How often does the clique encoding outperform other algorithms at
    different values of properties of the association graph?}
  \label{fig:bins}
\end{figure}

The next question we ought to explore is ``can we predict the instances where
the clique encoding is better than other algorithms based on properties of the
association graph?'' For each of the 6 measured properties, we divide its range
of values to 100 bins of equal size (some of them turn out to be empty) and plot
the proportion of instances in that range where the clique encoding outperformed
other algorithms in Figure~\ref{fig:bins}. We can make three observations:
\begin{itemize}
\item For all measured properties except density, the clique algorithm is not
  just overall faster, but also faster than other algorithms (i.e., more likely
  to win) with smaller vales.
\item The situation with density is less clear. Other than higher winning rates
  with density smaller than 0.3, density of the association graph tells us
  little about the algorithm's winrate.
\item Except for the low density values described in the previous point, the
  clique algorithm's winning rate stays below 0.5, meaning that simply looking
  at each measured property of the association graph is not enough to identify
  values of those properties that would provide a clear indication that
  re-encoding the MCS instance into an instance of maximum clique would provide
  clear benefits.
\end{itemize}

\begin{remark}
  The reader might notice that some bins start at negative values, even though
  the feature itself cannot be negative. This is because the left endpoint of
  the leftmost interval is defined as
  \[ \min(X) - 0.001 \times (\max(X) - \min(X)), \]
  where $X$ is the set of observed values of some particular feature \cite{R}
  (and similarly for the right endpoint of the rightmost interval).
\end{remark}

\section{\textsc{Fusion}: \textsc{McSplit} and the Clique Encoding United}

Even though we were not able to justify re-encoding a problem instance that is
partially solve by \textsc{McSplit} into the clique encoding using the simple
techniques of Section \ref{sec:clique}, this section presents the simplest way of
merging the two algorithms, where the clique algorithm overtakes
\textsc{McSplit} after making a fixed number of choices. The hope is that making
several decision with \textsc{McSplit} would simplify the association graph
enough to warrant the extra cost in creating the association graph and switching
algorithms.

Firstly, we add an additional argument to the \texttt{Search} function of the
\textsc{McSplit} algorithm \cite{DBLP:conf/ijcai/McCreeshPT17} called
\textsf{depth} with a default (initial) value of 0. It is increased by one in
the recursive call on line 19 of Algorithm 1 in the \textsc{McSplit} paper
\cite{DBLP:conf/ijcai/McCreeshPT17} and left unchanged in a different recursive
call on line 23.

Secondly, we also add a global parameter \textsf{DEPTH}, which denotes at what value of
\textsf{depth} \textsc{McSplit} re-encodes the remaining problem to a maximum
clique instance. A special value of $\textsf{DEPTH} = -1$ makes \textsc{Fusion}
essentially equal to \textsc{McSplit} in all its behaviour. Similarly,
$\textsf{DEPTH} = 0$ is similar to running the clique algorithm, but, unlike in
the implementation of the clique algorithm, association graph's creation time is
included as part of total running time.

Lastly, similarly to the $\nabla$ operator from Definition \ref{def:nabla},
which creates the association graph from two graphs, we create a version of
$\nabla$ that works on the list of label classes called \textit{future} in the
\textsc{McSplit} paper \cite{DBLP:conf/ijcai/McCreeshPT17}. The set of vertices
of the association graph is then defined as
\[ \bigcup_{\langle G, H \rangle \in \textit{future}} G \times H, \]
and the edges are defined in exactly the same way as in the original definition.

Given the two new variables \textsf{depth} and \textsf{DEPTH}, we only need to
add the following lines of code between lines 5 and 6 of Algorithm 1 in the
\textsc{McSplit} paper \cite{DBLP:conf/ijcai/McCreeshPT17} to transform
\textsc{McSplit} into \textsc{Fusion}:

\begin{algorithm}
  \SetKwData{depthVar}{depth}
  \SetKwData{DEPTH}{DEPTH}
  \If{$\DEPTH \ne -1$ \bf{and} $\depthVar \ge \DEPTH$}{
    $|\textit{incumbent}_{\text{clique}}| \gets
    |\textit{incumbent}_{\textsc{McSplit}}| - |M|$\;
    $\texttt{search}(\nabla(\textit{future}), \emptyset, \emptyset, V_1)$\;
    \lIf{$|M| + |\textit{incumbent}_{\text{clique}}| >
      |\textit{incumbent}_{\textsc{McSplit}}|$}{$\textit{incumbent}_{\textsc{McSplit}}
      \gets \textit{incumbent}_{\textsc{McSplit}} \cup
      \textit{incumbent}_{\text{clique}}$}
  }
\end{algorithm}

Here, $\textit{incumbent}_{\textsc{McSplit}}$ and
$\textit{incumbent}_{\text{clique}}$ denote the different variables with the
same name in the two algorithms, $M$ and \textit{future} come from the
\textsc{McSplit} algorithm, $V_1$ is the set of vertices of one of the
original graphs, used in the initialisation of the clique algorithm, and
\texttt{search} is the main search function of the clique algorithm
\cite{DBLP:conf/cp/McCreeshNPS16}. Note that due to how the incumbents are
implemented in practice, we set the cardinality of
$\textit{incumbent}_{\text{clique}}$ without having to set the incumbent itself.

\begin{figure}
  \centering
  \includegraphics[scale=0.5]{images/fusion_ecdf.png}
  \caption{Cumulative plot of the main three MCS algorithms with two versions of
    \textsc{Fusion}: \textsc{Fusion 1} and \textsc{Fusion 2}, that build the
    association graph after one and two branching decisions, respectively.}
  \label{fig:fusion_ecdf}
\end{figure}

The algorithm was run on \num{10000} randomly-selected instances with
the labelling percentage varying between 5\% and 50\% (just like in Section
\ref{sec:runtimes}) and the resulting ECDF plot in Figure \ref{fig:fusion_ecdf}
shows that:

\begin{itemize}
\item \textsc{Fusion 2} is strictly worse than \textsc{Fusion 1}. Therefore we
  do not run experiments with versions of \textsc{Fusion} that switch algorithms
  after even more decisions.
\item \textsc{Fusion 1} is second best for very small runtimes, but gradually
  falls to the 4\textsuperscript{th} place.
\item At all points in the plot, \textsc{Fusion 1} is worse than the original
  clique encoding.
\end{itemize}

\begin{figure}
  \centering
  \includegraphics[scale=0.5]{images/fusion_linechart.png}
  \caption{Total runtime of each algorithm, for each labelling percentage.}
  \label{fig:fusion_lines}
\end{figure}

Figure \ref{fig:fusion_lines} helps to clarify the picture by showing
differences in performance of the old and the new algorithms, for each labelling
percentage. Keeping in mind that in this plot low total runtime values represent
well-performing algorithms, we can see that the original clique algorithm is
better than \textsc{Fusion 1}, which is better than \textsc{Fusion 2}, for all
labelling percentages, except for small differences when all three algorithms'
performance becomes terrible with 5\% labelling. Another thing to note is that
the differences between these three algorithms is highest for average
(10\%--20\%) labelling percentages, and becomes smaller when moving towards
either extreme. This conclusively shows that switching between \textsc{McSplit}
and the clique encoding at a certain depth of the search tree does not produce a
competitive algorithm for solving MCS problem instances.

\section{\textsc{SmartFusion}: Can We Switch Algorithms in a More Intelligent
  Manner?}

We consider switching algorithms at the same place in the \textsc{McSplit}
algorithm as in the previous section, but this time we want to use running-time
data and simple statistical/ML models to inform our choice for when to switch.
We introduce a way to map partially solved instances inside \textsc{McSplit}
into unsolved instances, so that...TODO: complete this sentence. For the rest of
this section, consider an MCS problem instance between graphs $\mathcal{G} =
(V_{\mathcal{G}}, E_{\mathcal{G}}, \mu_{\mathcal{G}}, \zeta_{\mathcal{G}})$ and
$\mathcal{H} = (V_{\mathcal{H}}, E_{\mathcal{H}}, \mu_{\mathcal{H}},
\zeta_{\mathcal{H}})$ with $\mu_{\mathcal{G}} \colon V_{\mathcal{G}} \to \{ 0,
\dots, N - 1 \}$ and $\mu_{\mathcal{H}} \colon V_{\mathcal{H}} \to \{ 0, \dots,
N - 1\}$. We formalise how each algorithm ``sees'' a problem instance
(obviously, the definitions are heavily based on the two papers describing these
algorithms \cite{DBLP:conf/cp/McCreeshNPS16, DBLP:conf/ijcai/McCreeshPT17}):

\begin{definition} \label{def:mcsplit_instance}
  A \emph{\textsc{McSplit} instance} is a 5-tuple
  $(\textit{future}, M, \textit{incumbent}, \mathcal{G},
  \mathcal{H})$\footnote{Note that the state of the algorithm is fully described
    by the first three elements of the tuple. The graphs are added in order to
    simplify some of the ideas presented later on.}, where:
  \begin{itemize}
  \item $\textit{future} = \{ \langle G, H \rangle : G \subseteq
    V_{\mathcal{G}}, H \subseteq V_{\mathcal{H}} \}$ represents the fact that
    every vertex in $G$ can only be matched to vertices in $H$ and vice versa.
    Initially, $\textit{future} = \{ \langle \mu_{\mathcal{G}}^{-1}(l),
    \mu_{\mathcal{H}}^{-1}(l) \rangle : l \in \mu_{\mathcal{G}}(V_{\mathcal{G}})
    \cap \mu_{\mathcal{H}}(V_{\mathcal{H}}) \}$\footnote{Technically, in order
      to account for loops, we would have to have a $\langle G, H \rangle$ pair
      for each unique combination of vertex label and multiset image of loops
      under the edge-labelling function (where $G \ne \emptyset \ne H$) (the
      same way the clique algorithm creates vertices in the association graph).
      To keep the level of complexity down to a managable level, we will stick
      with this simplification.}.
  \item $M = \{ (v, w) : v \in V_{\mathcal{G}}, w \in V_{\mathcal{H}} \}$
    contains the pairs of vertices that have already been matched.
  \item $\textit{incumbent} = \{ (v, w) : v \in V_{\mathcal{G}}, w \in
    V_{\mathcal{H}} \}$ is the maximum common subgraph known so far.
  \item $\mathcal{G}$ and $\mathcal{H}$ are the original graphs.
  \end{itemize}
  We will call this an \emph{unsolved \textsc{McSplit} instance} if $M =
  \emptyset$ and a \emph{partially solved instance} otherwise.
\end{definition}

\begin{definition}
  A \emph{clique instance} is a tuple $(G, \textit{incumbent})$, where $G =
  \mathcal{G} \nablaop \mathcal{H} = (V, E)$ is the association graph, and
  $\textit{incumbent} \subseteq V$ is the maximum clique in the association
  graph known so far.
\end{definition}

\noindent An \emph{unsolved instance} is characterised by a pair of graphs and a
non-negative integer $(\mathcal{G}, \mathcal{H}, k)$. Here, $k$ is used to set
up the initial size of the \textit{incumbent}. An unsolved instance
$(\mathcal{G}, \mathcal{H}, k)$ can be transformed to a clique instance
$(\mathcal{G} \nablaop \mathcal{H}, \textit{incumbent})$, and to a
\textsc{McSplit} instance $(\textit{future}, \emptyset, \textit{incumbent},
\mathcal{G}, \mathcal{H})$, where $\textit{incumbent}$ is any set of cardinality
$k$, and \textit{future} is initialised as in Definition
\ref{def:mcsplit_instance}. It is also important to remember from the previous
section that a \textsc{McSplit} instance $(\textit{future}, M,
\textit{incumbent}, \mathcal{G}, \mathcal{H})$ can be transformed into a clique
instance $(\nabla(\textit{future}), X)$, where $X$ is any set of cardinality
$|\textit{incumbent}| - |M|$.

\begin{definition}
  We call a partially solved instance $(\textit{future}, M, \textit{incumbent},
  \mathcal{G}, \mathcal{H})$ \emph{equivalent} to an unsolved instance
  $(\mathcal{G'}, \mathcal{H'}, k)$ if
  \begin{itemize}
  \item \textsc{McSplit} takes the same path of execution on both instances
    (transforming an unsolved instance into an unsolved \textsc{McSplit}
    instance as described earlier),
  \item and the clique algorithm takes the same path of execution on a clique
    instance constructed from $(\mathcal{G'}, \mathcal{H'}, k)$ as on
    $(\nabla(\textit{future}), X)$, where $X$ is any set of cardinality
    $|\textit{incumbent}| - |M|$.
  \end{itemize}
\end{definition}
% TODO: add info about edge labels

\begin{proposition}
  Let $(\textit{future}, M, \textit{incumbent}, \mathcal{G}, \mathcal{H})$ be an
  arbitrary partially solved instance. Order the elements of \textit{future} in
  an arbitrary way: $\langle G_1, H_1 \rangle, \dots, \langle G_n, H_n \rangle$.
  Let $V_{\mathcal{G'}} = \bigcup_{i=1}^n G_i$. Let $E_{\mathcal{G'}}$ be a
  subset of $E_{\mathcal{G}}$, where we remove each edge that has one of its
  endpoints in $V_{\mathcal{G}} \setminus V_{\mathcal{G'}}$. Let $v \in
  V_{\mathcal{G'}}$ be arbitrary. Then it belongs to exactly one of $G_1, \dots,
  G_n$, because all $G_i$'s are disjoint \cite{DBLP:conf/ijcai/McCreeshPT17}.
  Suppose it is $G_j$. Let $\mu_{\mathcal{G'}}(v) = j - 1$. Since $v \in
  V_{\mathcal{G'}}$ was arbitrary, this defines a function $\mu_{\mathcal{G'}}
  \colon V_{\mathcal{G'}} \to \{ 0, \dots, n - 1 \}$. Finally, let
  $\zeta_{\mathcal{G'}} = \zeta_{\mathcal{G}}|_{E_{\mathcal{G'}}}$. Then the
  partially solved instance $(\textit{future}, M, \textit{incumbent},
  \mathcal{G}, \mathcal{H})$ is equivalent to an unsolved instance
  $(\mathcal{G'}, \mathcal{H'}, |\textit{incumbent}| - |M|)$, where
  $\mathcal{G'} = (V_{\mathcal{G'}}, E_{\mathcal{G'}}, \mu_{\mathcal{G'}},
  \zeta_{\mathcal{G'}})$ and $\mathcal{H'}$ is set up in a similar way.
\end{proposition}

\begin{proof}
  We begin by considering how \textsc{McSplit} behaves on each of the two
  instances. The unsolved instance gets transformed into a \textsc{McSplit}
  instance $(\textit{future}', \emptyset, \textit{incumbent}', \mathcal{G'},
  \mathcal{H'})$, where $\textit{incumbent}'$ is any set of cardinality
  $|\textit{incumbent}| - |M|$, and $\textit{future}' = \{ \langle
  \mu_{\mathcal{G'}}^{-1}(l), \mu_{\mathcal{H'}}^{-1}(l) \rangle : l \in
  \mu_{\mathcal{G'}}(V_{\mathcal{G'}}) \cap \mu_{\mathcal{H'}}(V_{\mathcal{H'}})
  \}$. We will show that $\textit{future} = \textit{future'}$ in three steps:
  \begin{enumerate}
  \item First, we show that \textit{future} and \textit{future'} contain the
    same elements of $V_{\mathcal{G}}$. All vertices of $\mathcal{G}$ mentioned
    in \textit{future'} can be expressed as
    \begin{equation} \label{eq:union_of_vertices}
    \bigcup_{l \in \mu_{\mathcal{G'}}(V_{\mathcal{G'}}) \cap
        \mu_{\mathcal{H'}}(V_{\mathcal{H'}})} \mu_{\mathcal{G'}}^{-1}(l).
    \end{equation}
    Since
    \[ V_{\mathcal{G'}} = \bigcup_{i=1}^n G_i \]
    and $\mu_{\mathcal{G'}}(G_i) = \{ i-1 \}$ by the definitions of
    $V_{\mathcal{G'}}$ and $\mu_{\mathcal{G'}}$,
    \[ \mu_{\mathcal{G'}}(V_{\mathcal{G'}}) = \mu_{\mathcal{G'}}\left(
        \bigcup_{i=1}^n G_i \right) = \{ 0, 1, \dots, n - 1 \}. \]
    Similarly, $\mu_{\mathcal{H'}}(V_{\mathcal{H'}}) = \{ 0, 1, \dots, n - 1
    \}$, and thus $\mu_{\mathcal{G'}}(V_{\mathcal{G'}}) \cap
    \mu_{\mathcal{H'}}(V_{\mathcal{H'}}) = \{ 0, 1, \dots, n - 1 \}$. Then
    \eqref{eq:union_of_vertices} can be rewritten as
    \[ \bigcup_{l = 0}^{n-1} \mu_{\mathcal{G'}}^{-1}(l) = \bigcup_{l=0}^{n-1}
      G_{l+1} = \bigcup_{l=1}^n G_l, \]
    which is exactly what we wanted to prove. A similar argument can show that
    \textit{future} and \textit{future'} contain the same elements of
    $V_{\mathcal{H}}$.
  \item Next, we show that two vertices of $\mathcal{G'}$ are in the same $G_i'$
    if and only if they are in the same $G_j$. Indeed, for any $v, w \in
    V_{\mathcal{G'}}$,
    \[ v, w \in G_i' \iff \mu_{\mathcal{G}'}(v) = \mu_{\mathcal{G'}}(w) \iff v,
      w \in G_{\mu_{\mathcal{G'}}(v)+1}. \]
    Replacing $G$ with $H$ and $\mathcal{G}$ with $\mathcal{H}$ also gives us an
    equivalent property for vertices of $\mathcal{H}'$.
  \item Finally, we show that a similar structure is preserved between a pair of
    vertices belonging to different graphs. For any $v \in V_{\mathcal{G'}}$ and
    $w \in V_{\mathcal{H'}}$,
    \[ v \in G_i', w \in H_i' \text{ for some } i \iff \mu_{\mathcal{G'}}(v) =
      \mu_{\mathcal{H'}}(w) \iff v \in G_{\mu_{\mathcal{G'}}(v)+1}, w \in
      H_{\mu_{\mathcal{G'}}(v)+1}. \]
    This completes the proof that $\textit{future} = \textit{future'}$.
  \end{enumerate}
  For two \textsc{McSplit} instances to be equivalent, it remains to show that
  the algorithm behaves exactly the same way with $(\emptyset,
  \textit{incumbent'})$, as it does with $(M, \textit{incumbent})$. Note that
  the only places in the algorithm, where these two variables can affect how the
  algorithm executes, are in lines 3--5 in the original paper
  \cite{DBLP:conf/ijcai/McCreeshPT17}, where we compare $|M|$ with
  $|\textit{incumbent}|$ and $|M| + \sum_{\langle G, H \rangle \in
    \textit{future}} \min(|G|, |H|)$ with $|\textit{incumbent}|$. When replacing
  $(M, \textit{incumbent})$ with $(\emptyset, \textit{incumbent'})$, we replace
  $(|M|, |\textit{incumbent}|)$ with $(0, |\textit{incumbent}| - |M|)$, so lines
  3--5 test the same inequalities with $|M|$ subtracted from both sides.
  Therefore, \textsc{McSplit} executes the two instances in an identical way.
  
  The clique algorithm accepts the partially solved instance as
  $(\nabla(\textit{future}), X)$, and the unsolved instance as $(\mathcal{G'}
  \nablaop \mathcal{H'}, Y)$, where $X$ and $Y$ are any two sets of cardinality
  $|\textit{incumbent}| - |M|$. Similarly to the \textsc{McSplit} situation, the
  elements in $X$ and $Y$ do not affect the algorithm, as long as their
  cardinalities match. Thus it remains to show that $\nabla(\textit{future}) =
  \mathcal{G'} \nablaop \mathcal{H'}$. Let $\nabla(\textit{future}) = (V, E)$ and
  $\mathcal{G'} \nablaop \mathcal{H'} = (V', E')$. Then
  \[ V = \bigcup_{i = 1}^n G_i \times H_i \]
  and
  \begin{align*}
  V' &= \{ (v, w) \in V_{\mathcal{G'}} \times V_{\mathcal{H'}} :
       \mu_{\mathcal{G'}}(v) = \mu_{\mathcal{H'}}(w) \} \\
     &= \{ (v, w) \in V_{\mathcal{G'}} \times V_{\mathcal{H'}} : v \in G_i, w \in H_i \text{ for some } i \}.
  \end{align*}
  Now since
  \[ V_{\mathcal{G'}} = \bigcup_{i = 1}^n G_i, \quad V_{\mathcal{H'}} =
    \bigcup_{i=1}^n H_i, \]
  we get that $V' = V$. The rules for adding edges to the association graphs are
  the same. They refer to different graphs, but $E_{\mathcal{G'}},
  E_{\mathcal{H'}}, \zeta_{\mathcal{G'}}$, and $\zeta_{\mathcal{H'}}$ are only
  modified to accomodate the removal of vertices. Therefore, since $V' = V$ and
  they define what vertices are considered when adding edges to the association
  graph, $E' = E$, $\nabla(\textit{future}) = \mathcal{G'} \nablaop
  \mathcal{H'}$, and the association graph receives equivalent input with the
  unsolved instance as it does with the partially solved instance.
\end{proof}

\chapter{Conclusion and Future Work}

We took several competitive algorithms for the MCS problem, and explored how their
performance changes with the number of labels on vertices and edges. Then we
constructed an extensive list of features to represent each pair of graphs,
trained three different ML models, and showed two of them to have near-optimal
performance, significantly outperforming previous algorithms. During that
process, we uncovered new information on how labels are typically implemented,
the weaknesses of making generalisations about all labelled instances from a
very limited sample, and the distributions and correlations of features in
various benchmark datasets. ML models allowed us to gain further insight into
which features are the most important, which algorithm prefers what values of
each feature, and how confidently we can predict when each algorithm should be
chosen. We were then able to build on that knowledge in order to extend one of
the algorithms to vertex-labelled instances, and propose the idea of switching
between two algorithms during runtime. We implemented a simple version, capable
of switching algorithms according to a fixed rule, and laid the theoretical
groundwork towards making the decision for when to switch in a more intelligent,
ML-based manner. Although a great deal of work has been done, we would like to
end with a few suggestions for constructing new algorithm portfolios, analysing
the performance data of graph algorithms, and building the algorithms of
tomorrow.

Firstly, the set of features used for the ML models is far from perfect. A
better performance should be achieved by removing at least a few of the
worst-performing features. Many things can be calculated from a graph, some of
them are likely to perform well as new features. In particular, since the
standard deviations of degrees performed so well, other measures of dispersion
such as the coefficient of variation should be considered. As mentioned earlier
in the dissertation, considering transformations of feature data might not bring
massive advantages, but could be worthwhile nonetheless.

Secondly, in this project, once we fix the number of different labels in a
graph, all labels occur with equal probabilities. This may not be very
representative of real-world data. Hence, along with varying how many labels a
graphs has, one could also consider some probability distribution over the set
of possible labels, making some of them more common than others.

Finally, it would be interesting to see an implementation of
\textsc{SmartFusion} based on new runtime data. Even though our proposed model
builds two graphs every time it has to make a decision, faster ways are likely
to exist, once the gathered data enables us to see which features are important
and should be tracked. Similarly, instead of creating a new association graph
every time, the same graph could be modified. One last question that our ML
models were not particularly successful in answering is ``when is
$\textsc{McSplit}{\downarrow}$ better than \textsc{McSplit}?''

\begin{appendices}

  \chapter{Plots of Distributions of Features} \label{appendix:plots}

  \begin{figure}
    \centering
    \includegraphics[scale=0.7]{images/sip_loops.png}
    \caption{Density plot of the number of loops in graphs from
      Section~\ref{sec:unlabelled}.}
    \label{fig:loops}
  \end{figure}

  \begin{figure}
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/mcs_vertices.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/mcs_edges.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/mcs_meandeg.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/mcs_maxdeg.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/mcs_stddeg.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/mcs_density.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/mcs_meandist.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/mcs_maxdist.png}
    \end{subfigure}
    \caption{Plots of how various features are distributed for graphs from
      Section~\ref{sec:labelled}.}
    \label{fig:mcs_features1}
  \end{figure}

  \begin{figure}
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/sip_vertices.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/sip_edges.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/sip_meandeg.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/sip_maxdeg.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/sip_stddeg.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/sip_density.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/sip_meandist.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/sip_maxdist.png}
    \end{subfigure}
    \caption{Plots of how various features are distributed for graphs from Section
      ~\ref{sec:unlabelled}.}
    \label{fig:sip_features1}
  \end{figure}

  \begin{figure}
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/mcs_prop2.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/sip_prop2.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/mcs_prop3.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/sip_prop3.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/mcs_prop4.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/sip_prop4.png}
    \end{subfigure}
    \caption{Comparison of typical distances between pairs of vertices between the
      two graph databases, with graphs from Section~\ref{sec:labelled} on the left,
      and graphs from Section~\ref{sec:unlabelled} on the right.}
    \label{fig:proportions}
  \end{figure}

  \begin{figure}
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/mcs_ratio_vertices.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/sip_ratio_vertices.png}
    \end{subfigure}
    \caption{The density plots of log-transformed ratio of the number of vertices
      between pattern and target graphs for both databases with graphs from
      Section~\ref{sec:labelled} on the left and graphs from
      Section~\ref{sec:unlabelled} on the right.}
    \label{fig:ratio_vertices}
  \end{figure}

  \begin{figure}
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/mcs_ratio_edges.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/mcs_ratio_meandeg.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/mcs_ratio_maxdeg.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/mcs_ratio_density.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/mcs_ratio_meandist.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/mcs_ratio_maxdist.png}
    \end{subfigure}
    \caption{The other density plots of the log-transformed ratio features for
      graphs from Section~\ref{sec:labelled}.}
    \label{fig:mcs_ratio}
  \end{figure}

  \begin{figure}
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/sip_ratio_edges.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/sip_ratio_meandeg.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/sip_ratio_maxdeg.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/sip_ratio_density.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/sip_ratio_meandist.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/sip_ratio_maxdist.png}
    \end{subfigure}
    \caption{The other density plots of the log-transformed ratio features for
      graphs from Section~\ref{sec:unlabelled}.}
    \label{fig:sip_ratio}
  \end{figure}

  \chapter{Plots Analysing the ML Models for Labelled
    Graphs} \label{appendix:plots2}

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/vertex_labels_forest_errors.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/both_labels_forest_errors.png}
  \end{subfigure}
  \caption{Convergence plots of various error measures as the number of trees in
    a random forest increases. The plots show the OOB error and $1 -
    \text{recall}$ for each algorithm.}
  \label{fig:forest_errors}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/vertex_labels_variable_importance.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/both_labels_variable_importance.png}
  \end{subfigure}
  \caption{Variable importance for both types of labelling, sorted from most to
    least important.}
  \label{fig:variable_importance}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/vertex_labels_var_used.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/both_labels_var_used.png}
  \end{subfigure}
  \caption{How often was each variable used to make splitting decisions?}
  \label{fig:var_used}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/vertex_labels_margin.png}
    \includegraphics[width=\textwidth]{images/vertex_labels_margin2.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/both_labels_margin.png}
    \includegraphics[width=\textwidth]{images/both_labels_margin2.png}
  \end{subfigure}
  \caption{Sorted and unsorted margins of all data points, for both types of
    labelling.}
  \label{fig:margins}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/vertex_labels_clique_hist.png}
    \includegraphics[width=\textwidth]{images/vertex_labels_mcsplit_hist.png}
    \includegraphics[width=\textwidth]{images/vertex_labels_mcsplitdown_hist.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/both_labels_clique_hist.png}
    \includegraphics[width=\textwidth]{images/both_labels_mcsplit_hist.png}
    \includegraphics[width=\textwidth]{images/both_labels_mcsplitdown_hist.png}
  \end{subfigure}
  \caption{Histograms of margins, for each algorithm and type of labelling.}
  \label{fig:margin_hist}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/vertex_labels_clique_labelling.png}
    \includegraphics[width=\textwidth]{images/vertex_labels_mcsplit_labelling.png}
    \includegraphics[width=\textwidth]{images/vertex_labels_mcsplitdown_labelling.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/both_labels_clique_labelling.png}
    \includegraphics[width=\textwidth]{images/both_labels_mcsplit_labelling.png}
    \includegraphics[width=\textwidth]{images/both_labels_mcsplitdown_labelling.png}
  \end{subfigure}
  \caption{Partial dependence plots of the labelling percentage.}
  \label{fig:labelling_partials}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/vertex_labels_clique_stddeg.png}
    \includegraphics[width=\textwidth]{images/vertex_labels_mcsplit_stddeg.png}
    \includegraphics[width=\textwidth]{images/vertex_labels_mcsplitdown_stddeg.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/both_labels_clique_stddeg.png}
    \includegraphics[width=\textwidth]{images/both_labels_mcsplit_stddeg.png}
    \includegraphics[width=\textwidth]{images/both_labels_mcsplitdown_stddeg.png}
  \end{subfigure}
  \caption{Partial dependence plots of the standard deviation of degrees in the
    target graph.}
  \label{fig:stddeg_partials}
\end{figure}

\end{appendices}

\bibliographystyle{plain}
\bibliography{references}
\end{document}
