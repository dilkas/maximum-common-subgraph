\documentclass{article}

\usepackage[UKenglish]{babel}
\usepackage[UKenglish]{isodate}
\usepackage[backend=bibtex]{biblatex}
\usepackage[hidelinks]{hyperref}
\usepackage{amsfonts, amsthm, graphicx, subcaption}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\theoremstyle{remark}
\newtheorem{remark}{Remark}[section]

\bibliography{references.bib}

\author{Paulius Dilkas}
\title{Algorithm Selection for Maximum Common Subgraph}

\begin{document}
\maketitle

\section{Introduction}
% TODO: define graph colouring, induced subset, common induced subgraph
% TODO: which sets have loops, multiple edges
\begin{definition}
  An undirected \emph{multigraph} is a pair $(V, E)$, where $V$ is a set of
  vertices and $E$ is a set of edges, together with a map $E \to V \cup V^2$,
  which assigns one or two vertices to each edge
  \cite{DBLP:books/daglib/0030488}. If an edge is assigned to a single vertex,
  it is called a \emph{loop}. When several edges map to the same pair of
  vertices, they are referred to as \emph{multiple edges}. 
\end{definition}

We will refer to undirected multigraphs simply as graphs.

\begin{definition}
  The \emph{maximum common induced subgraph} between graphs $G_1$ and $G_2$ is a
  graph $G_3 = (V_3, E_3)$ such that $G_3$ is isomorphic to an induced subset of
  $G_1$ and $G_2$ and $|V_3|$ is maximised.
\end{definition}
In this paper we will be dealing with the maximum common induced subgraph
problem defined for undirected multigraphs, even though most of the benchmark
instances do not have multiple edges.

\section{Algorithms}
The clique encoding \cite{DBLP:conf/cp/McCreeshNPS16} solves the maximum common
subgraph problem by creating a new (association) graph and transforming the
problem into an instance of maximum clique, which is then solved by a sequential
version of the maximum clique solver by McCreesh and Prosser
\cite{DBLP:journals/topc/McCreeshP15}, which is a branch and bound algorithm
that uses bitsets and greedy colouring. Colouring is used to provide a quick
upper bound: if a subgraph can be coloured with $k$ colours, then it cannot have
a clique of size more than $k$. It should be noted that creating the association
graph can use a significant amount of memory. In fact, since memory usage for a
pair of graphs with $n$ and $m$ vertices is $\Theta(n^2m^2)$, pairs of graphs
with $n \times m \ge 16000 $ had to be abandoned. % TODO: prove the Theta bound

$k\downarrow$ algorithm \cite{DBLP:conf/aaai/HoffmannMR17} starts by trying to
solve the subgraph isomorphism problem, i.e. finding the pattern graph in the
target graph. If that fails, it allows a single vertex of the pattern graph to
not match any of the target graph vertices and tries again, allowing smaller and
smaller pattern graphs until it finds a solution. The number of vertices of the
pattern graph that are allowed this additional freedom is represented by $k$.
More specifically, the algorithm creates a domain for each pattern graph vertex,
which initially includes all vertices of the target graph and $k$ wildcards. The
domains are filtered with various propagation techniques. Then the search begins
with a smallest domain (not counting wildcards), a value is chosen, and domains
are filtered again to eliminate the chosen value.

\textsc{McSplit} \cite{DBLP:conf/ijcai/McCreeshPT17} is a branch and bound
algorithm that builds its own bit string labels for vertices in both pattern and
target graphs. Once it chooses to match a vertex $u$ in graph $G_1$ with a
vertex $v$ in graph $G_2$, it iterates over all unmatched vertices in both
graphs, adding a 1 to their labels if they are adjacent to $u$ or $v$ and 0
otherwise. That way a vertex can only be matched with vertices that have the
same labels. The labels are also used in the upper bound heuristic function
using the rule that if a particular label is assigned to $m$ vertices in $G_1$
and $n$ vertices in $G_2$, then up to $\min \{ m, n \}$ pairs can be matched for
that label.

$\textsc{McSplit} \downarrow$ is a variant of \textsc{McSplit} mentioned but not
explained in the original paper \cite{DBLP:conf/ijcai/McCreeshPT17}. It is meant
to be similar to $k\downarrow$ in that it starts by trying to find a subgraph
isomorphism and keeps decreasing the size of common subgraphs that it is
interested in until a solution is found. Based on the source
code\footnote{\url{https://github.com/jamestrimble/ijcai2017-partitioning-common-subgraph/blob/master/code/james-cpp/mcsp.c}},
there are a few key differences between $\textsc{McSplit} \downarrow$ and
\textsc{McSplit}:
\begin{itemize}
\item Instead of always looking for larger and larger common subgraphs, we have
  a goal size and exit early if a common subgraph of that size is found.
\item The goal size is decreased if the search finishes without a solution.
\item Having a big goal size allows the heuristic to be more selective and prune
  more of the search tree branches.
\end{itemize}

\section{Problem Instances}
In order to determine which algorithm should be used for which problem instance,
we run all algorithms on two databases that contain a large variety of graphs
differing in size, various characteristics, and the way they were generated.

The \textsc{McSplit} paper \cite{DBLP:conf/ijcai/McCreeshPT17} used the same
datasets to compare these (and a few constraint programming) algorithms and
found \textsc{McSplit} to win with unlabelled graphs, the clique encoding to
win with labelled graphs, and $\textsc{McSplit}\downarrow$ to win with the
\texttt{largerGraphs} dataset. However, in some cases the difference in
performance between \textsc{McSplit} and the clique encoding or between
$\textsc{McSplit}\downarrow$ and $k\downarrow$ was very small.

The algorithms were compiled with gcc 6.3.0 and run on Intel Xeon E5-2697A v4
(2.60 GHz) processors with 512 GB of memory and a 1000 s time limit.
$k\downarrow$ was further modified to accept graphs with vertex labels.
% TODO: mention how %many instances were solved by at least one algorithm in
% each dataset.
% TODO: what labelling was used

\subsection{Labelled graphs}
All of the labelled graphs are taken from the ARG Database \cite{foggia2001-2,
  DBLP:journals/prl/SantoFSV03}, which is a large collection of graphs for
benchmarking various graph-matching algorithms. The graphs are generated using
several algorithms:

\begin{itemize}
\item randomly generated,
\item 2D, 3D, and 4D meshes,
\item and bounded valence graphs.
\end{itemize}

Furthermore, each algorithm is executed with several (3--5) different parameter
values. The database includes 81400 pairs of labelled graphs. Their unlabelled
versions are used as well.

\subsubsection{Characteristics of Graph Labelling}
For the purposes of this paper, we look at two types of labelled graphs: those
that have their vertices labelled and those that have both vertices and edges
labelled. We define them as follows (the definitions are loosely inspired by
\cite{abu-aisheh_2016}):

\begin{definition}
  A graph $G = (V, E)$ is a \emph{(vertex) labelled graph} if it has an associated
  vertex labelling function $\mu \colon V \to \{ 0, \dots, N - 1 \}$ for some $N
  \in \{2, \dots, |V| \}$.
\end{definition}

\begin{definition}
  A graph $G = (V, E)$ is a \emph{fully labelled graph} if it is a vertex labelled
  graph and it has an associated edge labelling function $\zeta \colon E \to
  \{ 0, \dots, M - 1 \}$ for some $M \in \{ 2, \dots, |E| \}$.
\end{definition}

Specifically, note that:

\begin{itemize}
\item If a graph is labelled, then all its vertices (and possibly edges) are
  assigned a label.
\item We are only considering finite sets of labels, represented by non-negative integers.
\end{itemize}

Now we need a way to choose $N$ and $M$. For that we formally define how
labelling is implemented in the ARG database:

\begin{definition}
  A graph $G = (V, E)$ is said to have a \emph{$p\%$ (vertex) labelling} if
  \[ N = \max \left\{ 2^n : n \in \mathbb{N},\, 2^n < \left\lfloor \frac{p}{100\%}
        \times |V| \right\rfloor \right\}. \]
\end{definition}

The default value for $p$ is 33\%.

\begin{figure} % TODO: could add a qq plot as well
  \includegraphics[scale=0.5]{labelling_histogram.png}
  \caption{Histogram of the difference between the expected number of vertices
    assigned each label and the actual number (for all labelled graphs)}
  \label{figure:labelling_histogram}
\end{figure}

% TODO: could insert an example
The publications associated with the database \cite{foggia2001-2, DBLP:journals/prl/SantoFSV03}
say nothing about how the labels are distributed among the $N$ values. We
calculate the number of vertices that were assigned each label for each graph
(represented by $C$) and compare those values with the numbers we would expect
from a uniform distribution (represented by $E(C)$). We plot a histogram of the
difference $E(C) - C$ in Figure \ref{figure:labelling_histogram} and observe
that the difference is normally distributed around 0.

\subsection{Unlabelled graphs}
We also include a collection of benchmark instances for the subgraph isomorphism
problem\footnote{\url{http://liris.cnrs.fr/csolnon/SIP.html}} (with the
biochemical reactions dataset excluded since we are not dealing with directed
graphs). It contains only unlabelled graphs and consists of the following sets:

\begin{description}
\item[images-CVIU11] Graphs generated from segmented images. 43 pattern graphs
  and 146 target graphs, giving a total of 6278 instances.
\item[meshes-CVIU11] Graphs generated from meshes modelling 3D
  objects. 6 pattern graphs and 503 target graphs, giving a total of 3018
  instances. Both \texttt{images-CVIU11} and \texttt{meshes-CVIU11} datasets are
  described in \cite{DBLP:journals/cviu/DamiandSHJS11}.
\item[images-PR15] Graphs generated from segmented images
  \cite{DBLP:journals/pr/SolnonDHJ15}. 24 pattern graphs and a single target
  graph, giving 24 instances.
\item[LV] Graphs with various properties (connected, biconnected, triconnected,
  bipartite, planar, etc.). 49 graphs are paired up in all possible ways, giving
  $49^2=2401$ instances.
\item[scalefree] Scale-free networks generated using a power law distribution of
  degrees (100 instances).
\item[si] Bounded valence graphs, 4D meshes, and randomly generated graphs (1170
  instances). This is the unlabelled part of the ARG database. \texttt{LV},
  \texttt{scalefree}, and \texttt{si} datasets are described in
  \cite{DBLP:journals/ai/Solnon10, DBLP:journals/constraints/ZampelliDS10}.
\item[phase] Random graphs generated to be close to the
  satisfiable-unsatisfiable phase transition (200 instances)
  \cite{DBLP:conf/ijcai/McCreeshPT16}.
\item[largerGraphs] Large random and real-world graphs. There are 70 graphs,
  giving $70^2=4900$ instances. This set is not actually part of the main
  collection of benchmark instances, but is used in
  \cite{DBLP:conf/aaai/HoffmannMR17, DBLP:conf/lion/KotthoffMS16,
    DBLP:conf/ijcai/McCreeshPT17}.
\end{description}

\begin{remark}
This set of instances was taken from the
repository\footnote{\url{https://github.com/jamestrimble/ijcai2017-partitioning-common-subgraph}}
for the \textsc{McSplit} paper \cite{DBLP:conf/ijcai/McCreeshPT17} and has some
minor differences from the version on Christine Solnon's website.
\end{remark}

\begin{remark}
  Since $k\downarrow$ comes from the subgraph isomorphism problem background, it
  treats the two (pattern and target) graphs differently. Therefore, when graphs
  are not divided into patterns and targets, we run the algorithms with both
  orders ($(G_1, G_2)$ and $(G_2, G_1)$).
\end{remark}

\section{Features}
% TODO: formulas for features
The initial set of features is based on the algorithm selection paper for the
subgraph isomorphism problem \cite{DBLP:conf/lion/KotthoffMS16} and consists of
the following:

\begin{itemize}
\item number of vertices,
\item number of edges,
\item mean degree,
\item maximum degree,
\item standard deviation of degrees,
\item density,
\item mean distance between all pairs of vertices,
\item maximum distance between all pairs of vertices,
\item number of loops,
\item proportion of all vertex pairs that have a distance of at least 2, 3, and 4,
\item whether the graph is connected.
%\item uniformity of the distribution of edges.
\end{itemize}

We excluded feature extraction running time as a viable feature by itself since
it would not provide any insight into what properties of the graph affect which
algorithm is likely to achieve the best performance.
% TODO: labelling features

\subsection{Distributions of Features}

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{mcs_vertices.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{mcs_edges.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{mcs_meandeg.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{mcs_maxdeg.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{mcs_stddeg.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{mcs_density.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{mcs_meandist.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{mcs_maxdist.png}
  \end{subfigure}
  \caption{Plots of how various features are distributed for the labelled graphs}
  \label{fig:mcs_features1}
\end{figure}

\begin{figure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{mcs_prop2.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{mcs_prop3.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{mcs_prop4.png}
  \end{subfigure}
  \caption{Plots showing typical distances between pairs of vertices for the
    labelled graphs}
  \label{fig:mcs_features2}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{sip_vertices.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{sip_edges.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{sip_meandeg.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{sip_maxdeg.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{sip_stddeg.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{sip_density.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{sip_meandist.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{sip_maxdist.png}
  \end{subfigure}
  \caption{Plots of how various features are distributed for the unlabelled
    graphs}
  \label{fig:sip_features1}
\end{figure}

\begin{figure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{sip_prop2.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{sip_prop3.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{sip_prop4.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{sip_loops.png}
  \end{subfigure}
  \caption{Plots showing typical distances between pairs of vertices and the
    number of loops for the unlabelled graphs}
  \label{fig:sip_features2}
\end{figure}

% TODO: correlation?
% TODO: reference 4 figures, discuss the data
% TODO: how many instances were solved by one algorithm that weren't solved by
% any others?
In this section we plot and discuss how the selected features are distributed in
different datasets. Most of the data is plotted, except for the number of loops
of the labelled dataset and connectedness of both labelled and unlabelled data.
For connectedness, 99.81\% of the labelled graphs are connected, compared to
93.19\% of the unlabelled graphs. As both numbers are quite high, they may not
be ideal for establishing if connectedness is a significant factor in
determining which algorithm performs the best, however, the numbers seem quite
representative of real data, where connected graphs are a lot more common. In
fact, applications in chemistry are often only interested in connected graphs
\cite{WCMS:WCMS5}.

With the labelled graphs, all graphs are assigned into pairs (A and B) with no
significant differences between them. Thus all of the statistical information is
provided only for pattern graphs and is plotted in Figures
\ref{fig:mcs_features1} and \ref{fig:mcs_features2}. As the number of loops
varies between two values, it is not plotted: 0.98\% of the graphs have a
single loop, other graphs have no loops. Other than the plot for number of
vertices, which is manually chosen by the creators of the database, all the
distributions in Figure \ref{fig:mcs_features1} are centered around lower
values, with some instances providing significantly higher values. More
importantly, we have some graphs that are quite dense and some graphs with
higher mean distance values.

For unlabelled graphs, we will only consider graphs that are part of a pair of
graphs solved by at least one algorithm. Since most of the data is heavily
skewed, Figures \ref{fig:sip_features1} and \ref{fig:sip_features2} show the
distributions as density plots. Because of this choice of which graphs to
include, all of the easy instances are solved and thus end up in the sample,
while only some of the harder instances are solved by at least one algorithm.
Harder instances typically have more vertices, which means they are also capable
of higher values for many other features, hence all of the density plots in
Figure \ref{fig:sip_features1} are right skewed. The same applies to the number
of loops in Figure \ref{fig:sip_features2}: almost all graphs have a small
number of them, while some (presumably larger) graphs have significantly more.

To sum up, even though the distributions are far from normal, most of them still
provide a good range of different values. There is one important difference
between the plots of proportions of vertex pairs with distance $\ge k$ for $k =
2, 3, 4$ for labelled and unlabelled datasets (in Figures
\ref{fig:mcs_features2} and \ref{fig:sip_features2}, respectively). With
labelled graphs, the mean value keeps shifting to the left, while with
unlabelled graphs, the plot for $k = 4$ still has its highest peak around 0.9,
which means adding features for $k \ge 5$ could be valuable.

\section{Selection Model}
We're using \textsc{Llama} \cite{kotthoff_llama_2013}. Describe k-folding.

\printbibliography
\end{document}
