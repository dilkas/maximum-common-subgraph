\pdfoutput=1
\documentclass{l4proj}

\usepackage[UKenglish]{babel}
\usepackage[UKenglish]{isodate}
\usepackage[hidelinks]{hyperref}
\usepackage{amsfonts, amsmath, amsthm, graphicx, subcaption}
\usepackage[group-separator={,}]{siunitx}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\theoremstyle{remark}
\newtheorem{remark}{Remark}[chapter]
\DeclareMathOperator{\dom}{dom}

\title{Algorithm Selection for Maximum Common Subgraph}
\author{Paulius Dilkas}
%\date{}

\begin{document}
\maketitle

%\begin{abstract} TODO: obviously
%\end{abstract}

\educationalconsent
\tableofcontents

\chapter{Introduction}
\pagenumbering{arabic}

We start with a definition of graphs suitable for the empirical data described
in Chapter \ref{chapter:problems} and used in Chapter
\ref{chapter:generating_data}.

\begin{definition}
  Let $S$ be a set and let $k$ be a non-negative integer. Then $[S]^k$ denotes
  the set of \emph{$k$-subets} of $S$ \cite{subset}, i.e.,
  \[ [S]^k = \{ A \subseteq S : |A| = k \}. \]
\end{definition}

\begin{definition} \label{def:graph}
  An \emph{undirected multigraph} is a pair $(V, E)$, where $V$ is a set of
  vertices and $E$ is a set of edges, together with a map $E \to V \cup [V]^2$,
  which assigns one or two vertices to each edge
  \cite{DBLP:books/daglib/0030488}. If an edge is assigned to a single vertex,
  it is called a \emph{loop}. When several edges map to the same pair of
  vertices, they are referred to as \emph{multiple edges}. 
\end{definition}

We formulate all of the definitions in terms of undirected multigraphs as some
of the graphs in Chapter \ref{chapter:problems} contain multiple edges and many
contain at least one loop. In the rest of the dissertation, we will refer to
undirected multigraphs simply as graphs. Next we adapt some basic graph theory
definitions to work with our definition of a multigraph.

\begin{definition}
  Two graphs $G_1 = (V_1, E_1)$ and $G_2 = (V_2, E_2)$ are said to be
  \emph{isomorphic} if there is a bijection $f \colon V_1 \to V_2$ such that
  for all $v \in V_1 \cup [V_1]^2$, the number of edges in $E_1$ that are mapped
  to $v$ is equal to the number of edges in $E_2$ that are mapped to $f(v)$,
  where $f(v) = \{ f(v_1), f(v_2) \}$ if $v = \{ v_1, v_2 \} \in [V_1]^2$
  \cite{DBLP:journals/jcamd/RaymondW02a}.
\end{definition}

\begin{definition} A graph $G_2 = (V_2, E_2)$ is a \emph{subgraph} of graph $G_1
  = (V_1, E_1)$ if $V_2 \subseteq V_1$ and $E_2 \subseteq E_1$
  \cite{DBLP:books/daglib/0030488}.
\end{definition}

\begin{definition} \label{def:induced_subgraph}
  An \emph{induced subgraph} of a graph $G = (V, E)$ is a graph $H = (S, E')$,
  where $S \subseteq V$ is a set of vertices and $E' \subseteq E$ is a set of
  edges that are mapped to $S \cup [S]^2$
  \cite{DBLP:journals/jcamd/RaymondW02a}.
\end{definition}

\begin{definition}
  A \emph{clique} $C$ in a graph $G = (V, E)$ is a subset of $V$ such that
  $\forall v_1, v_2 \in C$ with $v_1 \ne v_2$, there is an edge in $E$ mapping
  to $\{ v_1, v_2 \}$ \cite{DBLP:journals/jgo/PardalosX94a}.
\end{definition}

Finally, we define the main problem dealt with in this project.

\begin{definition}
  A \emph{maximum common (induced) subgraph} between graphs $G_1$ and $G_2$ is a
  graph $G_3 = (V_3, E_3)$ such that $G_3$ is isomorphic to induced subgraphs of
  both $G_1$ and $G_2$ with $|V_3|$ maximised
  \cite{DBLP:journals/jcamd/RaymondW02a}. The \emph{maximum common (induced)
    subgraph problem} is the problem of finding a maximum common subgraph
  between two given graphs $G_1 = (V_1, E_1)$ and $G_2 = (V_2, E_2)$, usually
  expressed as a bijection between two subsets of vertices $U_1 \subseteq V_1$
  and $U_2 \subseteq V_2$.
\end{definition}

The purpose of this project is to create and explore machine-learning-based
algorithm \emph{portfolios}, i.e., machine learning models whose main goal is to
provide a mapping, assigning the best-performing algorithm to each problem
instance \cite{DBLP:journals/ai/BischlKKLMFHHLT16, DBLP:journals/ac/Rice76}.

\section{The Algorithms}

We consider four algorithms that were shown to be competitive in a paper by
McCreesh \textit{et al}. \cite{DBLP:conf/ijcai/McCreeshPT17}: $k\downarrow$
\cite{DBLP:conf/aaai/HoffmannMR17}, \textsc{McSplit},
$\textsc{McSplit}\downarrow$ \cite{DBLP:conf/ijcai/McCreeshPT17}, and the clique
encoding \cite{DBLP:conf/cp/McCreeshNPS16}. The way some of these algorithms
work relates to two other problems: the maximum clique problem and the subgraph
isomorphism problem, both defined as follows:

\begin{definition}
  Given a graph $G$, the \emph{maximum clique problem} is an optimisation problem
  asking for a clique in $G$ with maximum cardinality
  \cite{DBLP:journals/jgo/PardalosX94a}.
\end{definition}

\begin{definition}
  Given two (finite) graphs $G_1$ and $G_2$, the \emph{subgraph isomorphism
    problem} is the decision problem of determining whether $G_1$ is isomorphic
  to a subgraph of $G_2$ \cite{DBLP:conf/stoc/Cook71}.
\end{definition}

$k\downarrow$ algorithm \cite{DBLP:conf/aaai/HoffmannMR17} starts by trying to
solve the subgraph isomorphism problem, i.e. finding the pattern graph in the
target graph. If that fails, it allows a single vertex of the pattern graph to
not match any of the target graph vertices and tries again, allowing smaller and
smaller pattern graphs until it finds a solution. The number of vertices of the
pattern graph that are allowed this additional freedom is represented by $k$.
More specifically, the algorithm creates a domain for each pattern graph vertex,
which initially includes all vertices of the target graph and $k$ wildcards. The
domains are filtered with various propagation techniques. Then the search begins
with a smallest domain (not counting wildcards), a value is chosen, and domains
are filtered again to eliminate the chosen value.

% TODO: could explain colouring
The clique encoding \cite{DBLP:conf/cp/McCreeshNPS16} solves the
maximum common subgraph problem by creating a new (association) graph and
transforming the problem into an instance of the maximum clique problem
\cite{Levi1973}, which is then solved by a sequential version of the maximum
clique solver by McCreesh and Prosser \cite{DBLP:journals/topc/McCreeshP15},
which is a branch and bound algorithm that uses bitsets and greedy colouring.
Colouring is used to provide a quick upper bound: if a subgraph can be coloured
with $k$ colours, then it cannot have a clique of size more than $k$.

\textsc{McSplit} \cite{DBLP:conf/ijcai/McCreeshPT17} is a branch and bound
algorithm that builds its own bit string labels for vertices in both pattern and
target graphs. Once it chooses to match a vertex $u$ in graph $G_1$ with a
vertex $v$ in graph $G_2$, it iterates over all unmatched vertices in both
graphs, adding a 1 to their labels if they are adjacent to $u$ or $v$ and 0
otherwise. That way a vertex can only be matched with vertices that have the
same labels. The labels are also used in the upper bound heuristic function
using the rule that if a particular label is assigned to $m$ vertices in $G_1$
and $n$ vertices in $G_2$, then up to $\min \{ m, n \}$ pairs can be matched for
that label.

$\textsc{McSplit} \downarrow$ is a variant of \textsc{McSplit} mentioned but not
explained in the original paper \cite{DBLP:conf/ijcai/McCreeshPT17}. It is meant
to be similar to $k\downarrow$ in that it starts by trying to find a subgraph
isomorphism and keeps decreasing the size of common subgraphs that it is
interested in until a solution is found. Based on the source
code\footnote{\url{https://github.com/jamestrimble/ijcai2017-partitioning-common-subgraph/blob/master/code/james-cpp/mcsp.c}},
there are a few key differences between $\textsc{McSplit} \downarrow$ and
\textsc{McSplit}:
\begin{itemize}
\item Instead of always looking for larger and larger common subgraphs, we have
  a goal size and exit early if a common subgraph of that size is found.
\item The goal size is decreased if the search finishes without a solution.
\item Having a big goal size allows the heuristic to be more selective and prune
  more of the search tree branches.
\end{itemize}

\chapter{Problem Instances} \label{chapter:problems}
We use two graph databases that contain a large variety of graphs differing in
size, various characteristics, and the way they were generated. The
\textsc{McSplit} paper \cite{DBLP:conf/ijcai/McCreeshPT17} used the same
data to compare these (and a few constraint programming) algorithms and
found \textsc{McSplit} to win with unlabelled graphs described in Section
\ref{sec:labelled} of this paper, the clique encoding to win with labelled
graphs, and $\textsc{McSplit}\downarrow$ to win with unlabelled graphs from
Section \ref{sec:unlabelled}. However, in some cases the difference in
performance between \textsc{McSplit} and the clique encoding or between
$\textsc{McSplit}\downarrow$ and $k\downarrow$ was very small.

\section{Labelled Graphs} \label{sec:labelled}
All of the labelled graphs are taken from the ARG Database \cite{foggia2001-2,
  DBLP:journals/prl/SantoFSV03}, which is a large collection of graphs for
benchmarking various graph-matching algorithms. The graphs are generated using
several algorithms:

\begin{itemize}
\item randomly generated,
\item 2D, 3D, and 4D meshes,
\item and bounded valence graphs.
\end{itemize}

Furthermore, each algorithm is executed with several (3--5) different parameter
values. The database includes 81400 pairs of labelled graphs. Their unlabelled
versions are used as well.

\subsection{Characteristics of Graph Labelling}
For the purposes of this paper, we look at two types of labelled graphs: those
that have their vertices labelled and those that have both vertices and edges
labelled. We define them as follows (the definitions are loosely inspired by
\cite{abu-aisheh_2016}):

\begin{definition}
  A graph $G = (V, E)$ is a \emph{(vertex) labelled graph} if it has an associated
  vertex labelling function $\mu \colon V \to \{ 0, \dots, N - 1 \}$ for some $N
  \in \{2, \dots, |V| \}$.
\end{definition}

\begin{definition}
  A graph $G = (V, E)$ is a \emph{fully labelled graph} if it is a vertex labelled
  graph and it has an associated edge labelling function $\zeta \colon E \to
  \{ 0, \dots, M - 1 \}$ for some $M \in \{ 2, \dots, |E| \}$.
\end{definition}

Specifically, note that:

\begin{itemize}
\item If a graph is labelled, then all its vertices (and possibly edges) are
  assigned a label.
\item We are only considering finite sets of labels, represented by non-negative integers.
\end{itemize}

Now we need a way to choose $N$ and $M$. For that we formally define how
labelling is implemented in the ARG database:

\begin{definition} \label{def:percent_labelling}
  A graph $G = (V, E)$ is said to have a \emph{$p\%$ (vertex) labelling} if
  \[ N = \max \left\{ 2^n : n \in \mathbb{N},\, 2^n < \left\lfloor \frac{p}{100\%}
        \times |V| \right\rfloor \right\}. \]
\end{definition}

The default value for $p$ is 33\%.

\begin{figure}
  \centering
  \includegraphics[scale=0.7]{images/labelling_histogram.png}
  \caption{Histogram of the difference between the expected number of vertices
    assigned each label and the actual number (for all labelled graphs)}
  \label{figure:labelling_histogram}
\end{figure}

The publications associated with the database \cite{foggia2001-2, DBLP:journals/prl/SantoFSV03}
say nothing about how the labels are distributed among the $N$ values. We
calculate the number of vertices that were assigned each label for each graph
(represented by $C$) and compare those values with the numbers we would expect
from a uniform distribution (represented by $E(C)$). We plot a histogram of the
difference $E(C) - C$ in Figure \ref{figure:labelling_histogram} and observe
that the difference is normally distributed around 0.

\section{Unlabelled Graphs} \label{sec:unlabelled}
We also include a collection of benchmark instances for the subgraph isomorphism
problem\footnote{\url{http://liris.cnrs.fr/csolnon/SIP.html}} (with the
biochemical reactions dataset excluded since we are not dealing with directed
graphs). It contains only unlabelled graphs and consists of the following sets:

\begin{description}
\item[images-CVIU11] Graphs generated from segmented images. 43 pattern graphs
  and 146 target graphs, giving a total of \num{6278} instances.
\item[meshes-CVIU11] Graphs generated from meshes modelling 3D
  objects. 6 pattern graphs and 503 target graphs, giving a total of \num{3018}
  instances. Both \texttt{images-CVIU11} and \texttt{meshes-CVIU11} datasets are
  described in \cite{DBLP:journals/cviu/DamiandSHJS11}.
\item[images-PR15] Graphs generated from segmented images
  \cite{DBLP:journals/pr/SolnonDHJ15}. 24 pattern graphs and a single target
  graph, giving 24 instances.
\item[LV] Graphs with various properties (connected, biconnected, triconnected,
  bipartite, planar, etc.). 49 graphs are paired up in all possible ways, giving
  $49^2=\num{2401}$ instances.
\item[scalefree] Scale-free networks generated using a power law distribution of
  degrees (100 instances).
\item[si] Bounded valence graphs, 4D meshes, and randomly generated graphs
  (\num{1170} instances). This is the unlabelled part of the ARG database.
  \texttt{LV}, \texttt{scalefree}, and \texttt{si} datasets are described in
  \cite{DBLP:journals/ai/Solnon10, DBLP:journals/constraints/ZampelliDS10}.
\item[phase] Random graphs generated to be close to the
  satisfiable-unsatisfiable phase transition (200 instances)
  \cite{DBLP:conf/ijcai/McCreeshPT16}.
\item[largerGraphs] Larger instances of the \texttt{LV} dataset. There are 70
  graphs, giving $70^2=\num{4900}$ instances. The separation was made and used
  in \cite{DBLP:conf/aaai/HoffmannMR17, DBLP:conf/lion/KotthoffMS16,
    DBLP:conf/ijcai/McCreeshPT17}.
\end{description}

\begin{remark}
This set of instances was taken from the
repository\footnote{\url{https://github.com/jamestrimble/ijcai2017-partitioning-common-subgraph}}
for the \textsc{McSplit} paper \cite{DBLP:conf/ijcai/McCreeshPT17} and has some
minor differences from the version on Christine Solnon's website.
\end{remark}

\begin{remark}
  Since $k\downarrow$ comes from the subgraph isomorphism problem background, it
  treats the two (pattern and target) graphs differently. Therefore, when graphs
  are not divided into patterns and targets, we run the algorithms with both
  orders ($(G_1, G_2)$ and $(G_2, G_1)$).
\end{remark}
% TODO: mention algorithm portfolios, SAT, constraint solving, TSP, AI planning

\chapter{Generating Data} \label{chapter:generating_data}
A machine learning (ML) model requires data to learn from. We are using an R
package called \textsc{Llama} \cite{kotthoff_llama_2013, llama}, which helps to train
and evaluate ML models in order to compare algorithms and was used to create
algorithm portfolios for the travelling salesperson problem
\cite{DBLP:conf/lion/KotthoffKHT15} and the subgraph isomorphism problem
\cite{DBLP:conf/lion/KotthoffMS16}. First, we run each algorithm on all pairs of
pattern-target graphs and record the running times (described in Section
\ref{sec:runtimes}). Then, we adapt a graph feature extractor program used in
\cite{DBLP:conf/lion/KotthoffMS16} to handle the binary format of the ARG
Database \cite{foggia2001-2, DBLP:journals/prl/SantoFSV03}, run it on all
graphs, and record the features in a way described in Section
\ref{sec:features}.

\section{Running Time of Algorithms} \label{sec:runtimes}

The algorithms were compiled with gcc 6.3.0 and run on Intel Xeon E5-2697A v4
(2.60 GHz) processors with a \num{1000} s time limit. A Makefile was created to
run multiple experiments in parallel with, e.g., \texttt{make -j 64}, which
generates pairs of graph filenames for all datasets, runs the selected
algorithms with various command line arguments, redirects their output to files
that are later parsed using \texttt{sed} and regular expressions into the CSV
format. For each algorithm, we keep the full names of pattern and target graphs,
the number of vertices in the returned maximum common subgraph, running time as
reported by the algorithms themselves, and the number of explored nodes in the
search tree. Entries with running time greater than or equal to the timeout
value are considered to have timed out. The aforementioned node counts are
collected but not currently used. Afterwards, the answers of different
algorithms are checked for equality (for algorithms that did not time out).

Some limitations had to be enforced to avoid running out of memory. First, the
clique algorithm requires $O(n^2m^2)$ memory for a pair of graphs with $n$ amd
$m$ vertices \cite{DBLP:conf/aaai/HoffmannMR17, DBLP:conf/cp/McCreeshNPS16}, so
its virtual memory usage was limited to 7 GB with \texttt{ulimit -v} and the
instances from Section \ref{sec:unlabelled} (which contain much larger graphs)
were restricted to $m \times n < \num{16000}$. Second, having almost $10^5$
problem instances and analysing 7 different kinds of labelling (according to
Definition \ref{def:percent_labelling}) results in too much data for the ML
algorithm. Therefore, we sample \num{30000} out of \num{81400} instances from
Section \ref{sec:labelled}. The sample is drawn once and used for training ML
models for both types of labelling (vertex labels and both vertex and edge
labels). As with 50\% labelling most instances are solved within the time limit,
sampling from the whole database still leaves us with enough relevant data.

The $k\downarrow$ algorithm was further modified to accept graphs with vertex
labels by adding an additional constraint for matching labels on line 8 of the
\texttt{klessSubgraphIsomorphism} function \cite{DBLP:conf/aaai/HoffmannMR17}.
After generating most of the data, it was noticed that the vertex-labelled
version of $k\downarrow$ was winning precisely 0 times. Therefore, $k\downarrow$
is not included in Sections \ref{sec:runtime_both} and \ref{sec:runtime_vertex}
(dealing with the runtime analysis of algorithms for both kinds of labelling),
nor in the machine learning models.

Furthermore, different implementations of the clique algorithm had to be
combined to support both the binary format of the ARG Database and the plaintext
format of Section \ref{sec:unlabelled}.

In the rest of this section we explore and compare how the algorithms performed
on the three different subproblems under consideration, those of having no
labels, vertex labels, and both vertex and edge labels. We introduce
\emph{empirical cumulative distribution function (ECDF)}
plots \cite{10.2307/2334448}: for each unit of time on the horizontal axis, the
value on the vertical axis represents what part of the problem instances was
solved in that amount of time or less.
% TODO: mention how many instances were solved by at least one algorithm in each dataset.

\subsection{Unlabelled Graphs}

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/ecdf_mcs.png}
    \caption{Data from Section \ref{sec:labelled}, the ARG Database}
    \label{fig:ecdf_unlabelled_mcs}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/ecdf_sip.png}
    \caption{Data from Section \ref{sec:unlabelled}}
    \label{fig:ecdf_unlabelled_sip}
  \end{subfigure}
  \begin{subfigure}[t]{\textwidth}
    \centering
    \includegraphics[width=0.49\textwidth]{images/ecdf_unlabelled.png}
    \caption{All unlabelled data}
    \label{fig:ecdf_unlabelled_both}
  \end{subfigure}
  \caption{Comparison of the runtimes of algorithms on unlabelled data}
  \label{fig:ecdf_unlabelled}
\end{figure}

\begin{figure}
  \centering
  \includegraphics{images/runtime_heatmap.png}
  \caption{A heatmap of $\log_{10}$ runtimes: light colours for low running
    times and black for timing out}
  \label{fig:runtime_heatmap}
\end{figure}

We plot the ECDF plots for unlablled graphs in both databases in Figure
\ref{fig:ecdf_unlabelled}. We can check that the orderings of the algorithms in
parts (a) and (b) of Figure \ref{fig:ecdf_unlabelled} are the same as in Figures
3a and 4 in the \textsc{McSplit} paper \cite{DBLP:conf/ijcai/McCreeshPT17}.
Namely, \textsc{McSplit} outperforms $k\downarrow$ in Figure
\ref{fig:ecdf_unlabelled_mcs}, and the opposite happens in Figure
\ref{fig:ecdf_unlabelled_sip}. In Figure \ref{fig:ecdf_unlabelled_both} we also
plot a curve for the \emph{virtual best solver (VBS)}, i.e., a perfect algorithm
portfolio that always chooses the best-performing algorithm for each problem
instance. Note that the difference between $\textsc{McSplit}\downarrow$ and VBS
is very small. Therefore, a portfolio cannot provide significant performance
benefits for this subproblem. We also provide a heatmap in Figure
\ref{fig:runtime_heatmap} to compare the runtimes of the algorithms on a
per-instance basis.

\begin{remark}
  Not every problem instance gets a line of pixels on the heatmap. Therefore,
  the column for the clique encoding may look darker than it actually is.
  Furthermore, there are problem instances where $k\downarrow$ performs better
  than the other algorithms, even though it may not be apparent from the heatmap.
\end{remark}

\begin{table}
  \centering
  \begin{tabular}{l r r r r}
    Dataset & clique & $k\downarrow$ & \textsc{McSplit} & $\textsc{McSplit}\downarrow$ \\
    \hline
    \texttt{images-CVIU11} & 0 & 32 & 79 & 1081 \\
    \texttt{images-PR15} & 0 & 0 & 0 & 24 \\
    \texttt{largerGraphs} & 0 & 14 & 30 & 167 \\
    \texttt{LV} & 90 & 30 & 489 & 439 \\
    \texttt{meshes-CVIU11} & 0 & 13 & 0 & 23 \\
    \texttt{phase} & 0 & 0 & 0 & 0 \\
    \texttt{scalefree} & 0 & 0 & 0 & 80 \\
    \texttt{si} & 0 & 10 & 102 & 1135 \\
    ARG Database & 1443 & 141 & 21965 & 27305 \\
    \hline
    Total & 1533 & 240 & 22665 & 30254
  \end{tabular}
  \caption{The number of times each algorithm was the best, for each dataset}
  \label{table:best}
\end{table}

Table \ref{table:best} shows that most of the datasets have multiple algorithms
that managed to outperform the others for some problem instances. Thus, looking
at the differences between different datasets will not be enough to predict the
best algorithm.

\begin{remark}
  More specifically, Table \ref{table:best} shows the numbers of times that each
  algorithm's runtime was lower than or equal to the runtimes of other algorithms.
  Therefore, if 2 or more lowest runtimes are equal (as can often happen with
  single-digit runtimes), both algorithms are marked as winning in the table.
\end{remark}

Given this information, we would expect the ML algorithm to suggest using
\textsc{McSplit} and $\textsc{McSplit}\downarrow$ most of the time, occasionally
consider the clique encoding, and mostly forget about $k\downarrow$.

\subsection{Vertex- and Edge-Labelled Graphs} \label{sec:runtime_both}

\begin{figure}
  \centering
  \includegraphics[scale=0.7]{images/ecdf_both_labels.png}
  \caption{The cumulative plot with the vertical axis starting at 0.9}
  \label{fig:ecdf_both_labels}
\end{figure}

We note some interesting observations from the ECDF plot in Figure
\ref{fig:ecdf_both_labels}. First, the gap between the VBS curve and the curve
of any other algorithm is much wider than any gap between two algorithms. This
shows that different algorithms are winning for different problem instances and
that an ML model has plenty of room to outperform individual algorithms (cf.
unlabelled data). Second, the clique algorithm is briefly winning for shorter
timeout values, then is between $\textsc{McSplit}\downarrow$ and
\textsc{McSplit} until it drops to the 3\textsuperscript{rd} place right before
the final timeout at \num{1000000} ms. This is likely because the clique
encoding is better with higher labelling percentages (we will see this shortly)
and such graphs are generally easier to solve (because there are fewer
``matchable'' combinations of vertices, each pair of vertices is likely to have
different labels). Third, the gap between $\textsc{McSplit}\downarrow$ and
\textsc{McSplit} remains about the same throughout different timeout values and
is in favour of $\textsc{McSplit}\downarrow$.

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/both_labels_linechart.png}
    \caption{proportion of instances solved within the time limit}
    \label{fig:both_labels_linechart1}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/both_labels_linechart2.png}
    \caption{total running time}
    \label{fig:both_labels_linechart2}
  \end{subfigure}
  \caption{How the performance of algorithms changes with different labelling
    percentages}
  \label{fig:both_labels_linecharts}
\end{figure}

Just like with unlabelled graphs we could split the data into different subsets
based on how (and by whom) the graphs were generated, now we can analyse how the
situation changes with different labelling percentages. For this reason we plot
two statistics in Figure \ref{fig:both_labels_linecharts}: the proportion of
problem instances solved within the time limit and the total time taken to solve
all the instances. First note that we plotted two statistics representing
performance to get a more complete picture, however, the information is exactly
the same in both of them: Figure \ref{fig:both_labels_linechart2} is the vertical
reflection of Figure \ref{fig:both_labels_linechart1} with some changes in distances
between the lines (note that high values are desirable in the former, while low
values are desirable in the latter). With 15\% labelling and higher the
situation stays the same: the clique encoding is better than \textsc{McSplit},
which is (very slightly) better than $\textsc{McSplit}\downarrow$. Whereas with
lower levels of labelling, two differences emerge: $\textsc{McSplit}\downarrow$
surpasses \textsc{McSplit} and the difference between them grows larger, and the
performance of the clique algorithm starts to drop exponentially. The latter is
exactly what makes the clique encoding finish last in Figure
\ref{fig:ecdf_both_labels}, even though it is in the lead for most labelling
percentages.

\begin{remark}
Note that in both cases, for each labelling percentage, we are only considering
instances solved by at least one algorithm. Out of the \num{30,000} instances
selected by our random sampling procedure, the number of such instances ranges
from \num{17565} (58.55\%) for 5\% labelling to \num{29532} (98.44\%) for 50\%
labelling. Thus a (roughly) horizontal line should not be interpreted as the
algorithm performing equally well for different labelling percentages: the
performance of all algorithms improves with higher labelling percentages as the
problem becomes significantly easier. In this case we are more interested in the
differences between individual algorithms.
\end{remark}

\begin{figure}
  \centering
  \includegraphics[scale=0.7]{images/both_labels_linechart3.png}
  \caption{The number of times each algorithm was the best for different levels
    of labelling}
  \label{fig:both_labels_linechart3}
\end{figure}

Similarly to Table \ref{table:best}, in Figure \ref{fig:both_labels_linechart3}
we plot how many times each algorithm outperformed others, for each labelling
percentage. While the clique algorithm steadily rises from least freqeuently
winning to most frequently winning as the number of different labels decreases
as expected, the curves of \textsc{McSplit} and $\textsc{McSplit}\downarrow$ no
longer cross, with \textsc{McSplit} winning more often than
$\textsc{McSplit}\downarrow$, for all labelling percentages. By definition,
$\textsc{McSplit}\downarrow$ is better at handling problem instances with a big
answer. With a smaller labelling percentage, \textsc{McSplit} is more likely to
time out on those instances, contributing to $\textsc{McSplit}\downarrow$
solving more instances than \textsc{McSplit}. Perhaps
$\textsc{McSplit}\downarrow$ is also much faster on those instances, ensuring
its lower total runtime, while consistenly falling slightly behind
\textsc{McSplit} on easier instances, resulting in a lower win count. Whatever
the reason may be, the main observation from this plot is that the peak of all 3
lines is only at \num{18031}, just slightly above half of the number of
instances (\num{30000}), while there are only two points below \num{10000} (also
belonging to the clique encoding): at 577 and 4195. Thus all of the algorithms
end up winning for many instances, making this a perfect problem for an
algorithm portfolio.

\subsection{Vertex-Labelled Graphs} \label{sec:runtime_vertex}

\section{Graph Features} \label{sec:features}
The initial set of features is based on the algorithm selection paper for the
subgraph isomorphism problem \cite{DBLP:conf/lion/KotthoffMS16} and consists of
the following:

\begin{enumerate}
\item number of vertices,
\item number of edges,
\item mean degree,
\item maximum degree,
\item density,
\item mean distance between all pairs of vertices,
\item maximum distance between all pairs of vertices,
\item standard deviation of degrees,
\item number of loops,
\item proportion of all vertex pairs that have a distance of at least 2, 3, and 4,
\item whether the graph is connected.
%\item uniformity of the distribution of edges.
\end{enumerate}

\begin{definition}
For a graph $G$ with $n$ vertices and $m$ edges, the \emph{(edge) density} is
defined to be the proportion of potential edges that $G$ actually has
\cite{DBLP:books/daglib/0030488}. The standard formula used for \emph{simple}
graphs (i.e., graphs with no multiple edges or loops
\cite{DBLP:books/ws/NishizekiR04}) is
\[ \frac{m}{\binom{n}{2}} = \frac{2m}{n(n-1)}. \]
\end{definition}

Even though some of our graphs do contain multiple edges and loops, we stick to
this formula as it was used in  \cite{DBLP:conf/lion/KotthoffMS16} and it does
not break the ML algorithm in any way to have the theoretical possibility of
density greater than 1.

We exclude feature extraction running time as a viable feature by itself since
it would not provide any insight into what properties of the graph affect which
algorithm is likely to achieve the best performance. Since $k\downarrow$ and
$\textsc{McSplit}\downarrow$ both start by looking for (complete) subgraph
isomorphisms, they are likely to outperform other algorithms when both graphs
are very similar and the maximum common subgraph has (almost) as many vertices
as the smaller of the two graphs. Thus, for each feature $f$ in features 1--7
(excluding the rest to avoid division by 0), we also add a feature for the ratio
$\frac{f(G_p)}{f(G_t)}$, where $G_p$ and $G_t$ are the pattern and target
graphs, respectively.

We analyse three different types of labelling and treat them as separate
problems: no labels, vertex labels, vertex and edge labels. For the last two
types, we add a feature corresponding to $p$ defined in Definition
\ref{def:percent_labelling} and collect data for the following values of $p$:
50\%, 33\%, 25\%, 20\%, 15\%, 10\%, 5\%. The values correspond to having about
2, 3, 4, 5, 10, and 20 vertices/edges with the same label on average,
respectively.

\begin{remark}
  When working with both vertex and edge labels, we only consider using the same
  value of $p$ for both vertices and edges. This ``convention'' seems to have
  originated in a paper by the creators of the ARG Database
  \cite{DBLP:journals/jgaa/ConteFV07} and was replicated in subsequent papers on
  maximum common subgraph algorithms \cite{DBLP:conf/cp/McCreeshNPS16,
    DBLP:conf/cp/NdiayeS11}.
\end{remark}

\subsection{Distributions of Features}

\begin{figure}
  \centering
  \includegraphics{images/features_heatmap.png}
  \caption{A heatmap for normalised features with black denoting the maximum
    value and white denoting the minimum for each feature}
  \label{fig:features_heatmap}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[scale=0.7]{images/sip_loops.png}
  \caption{Density plot of the number of loops in graphs from Section
    \ref{sec:unlabelled}}
  \label{fig:loops}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/mcs_vertices.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/mcs_edges.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/mcs_meandeg.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/mcs_maxdeg.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/mcs_stddeg.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/mcs_density.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/mcs_meandist.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/mcs_maxdist.png}
  \end{subfigure}
  \caption{Plots of how various features are distributed for graphs from Section
  \ref{sec:labelled}}
  \label{fig:mcs_features1}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/sip_vertices.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/sip_edges.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/sip_meandeg.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/sip_maxdeg.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/sip_stddeg.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/sip_density.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/sip_meandist.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/sip_maxdist.png}
  \end{subfigure}
  \caption{Plots of how various features are distributed for graphs from Section
  \ref{sec:unlabelled}}
  \label{fig:sip_features1}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/mcs_prop2.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/sip_prop2.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/mcs_prop3.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/sip_prop3.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/mcs_prop4.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/sip_prop4.png}
  \end{subfigure}
  \caption{Comparison of typical distances between pairs of vertices between the
  two graph databases with graphs from Section \ref{sec:labelled} on the left
  and graphs from Section \ref{sec:unlabelled} on the right}
  \label{fig:proportions}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/mcs_ratio_vertices.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/sip_ratio_vertices.png}
  \end{subfigure}
  \caption{The density plots of log-transformed ratio of the number of vertices
    between pattern and target graphs for both databases with graphs from
    Section \ref{sec:labelled} on the left and graphs from Section
    \ref{sec:unlabelled} on the right}
  \label{fig:ratio_vertices}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/mcs_ratio_edges.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/mcs_ratio_meandeg.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/mcs_ratio_maxdeg.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/mcs_ratio_density.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/mcs_ratio_meandist.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/mcs_ratio_maxdist.png}
  \end{subfigure}
  \caption{The other density plots of the log-transformed ratio features for
    graphs from Section \ref{sec:labelled}}
  \label{fig:mcs_ratio}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/sip_ratio_edges.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/sip_ratio_meandeg.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/sip_ratio_maxdeg.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/sip_ratio_density.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/sip_ratio_meandist.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/sip_ratio_maxdist.png}
  \end{subfigure}
  \caption{The other density plots of the log-transformed ratio features for
    graphs from Section \ref{sec:unlabelled}}
  \label{fig:sip_ratio}
\end{figure}

In this section we plot and discuss how the selected features are distributed in
both databases. As the graphs from Section \ref{sec:unlabelled} contain some
very hard instances, we only consider graphs that are part of a pair of graphs
solved by at least one algorithm. In order to visualise highly skewed data, we
sometimes use density plots. Furthermore, we take $\log$ transformations for
ratio features. We also plot a heatmap of all features for all of the data in
Figure \ref{fig:features_heatmap}. The rows that are almost completely white
represent features that have several significantly higher values that skew the
mean.

Firstly, one feature is not plotted as by definition it has only two possible
values. 99.81\% of graphs from Section \ref{sec:labelled} are connected,
compared to 93.19\% of graphs from Section \ref{sec:unlabelled}. As both numbers
are quite high, they may not be ideal for establishing if connectedness is a
significant factor in determining which algorithm performs the best. However,
many applications in chemistry are only interested in connected graphs
\cite{WCMS:WCMS5}. Similarly, the number of loops for graphs from Section
\ref{sec:labelled} is not plotted as it varies between two values: 0.98\% of the
graphs have a single loops, while the remaining majority of graphs have no
loops. On the other hand, as shown in Figure \ref{fig:loops}, some (although not
many) graphs from Section \ref{sec:unlabelled} have significantly more loops.

Most of the features for graphs from Section \ref{sec:labelled} are displayed in
Figure \ref{fig:mcs_features1}. Other than the plot for the number of vertices,
which is controlled by the creators of the database, all the other distributions
are centred around lower values, with some outliers on the high end. More
importantly, we have some graphs that are quite dense and some graphs with
higher mean distance values.

The same plots for graphs from Section \ref{sec:unlabelled} in Figure
\ref{fig:sip_features1} show a similar story, albeit for clearer reasons. Since
we filter out graphs that none of the algorithms were able to handle, our sample
consists of all of the easy instances and some harder instances that were
solved by one or two algorithms. Harder instances typically have more vertices,
which means they are also capable of higher values for many other features,
hence all of the density plots are right skewed.

Figure \ref{fig:proportions} shows density plots of proportions of pairs of
vertices with distance at least $k$ for $k = 2, 3, 4$. For both databases, as
$k$ increases, the distributions shift to the left as expected. However, there
is one important difference: even with $k = 4$ the plot for graphs from Section
\ref{sec:unlabelled} has its highest peak around 0.9, which means that adding
features for $k \ge 5$ could be valuable.

Finally, the density plots of $\log$-transformed ratio features are in Figures
\ref{fig:ratio_vertices}, \ref{fig:mcs_ratio}, and \ref{fig:sip_ratio}. Almost
all of these plots for graphs from Section \ref{sec:unlabelled} (with the
exception of the ratio of mean degree) are clearly bimodal with one of the two
modes centred around 0. Hence we can infer the existance of two subpopulations:
one where pattern and target graphs have very similar properties (the majority
for most features) and one where pattern and target graphs are very different.
As for the graphs from Section \ref{sec:labelled}, the plot of the ratio of the
number of vertices in Figure \ref{fig:ratio_vertices} is perfectly symmetrical
and centred around 0, since the number of vertices is a controlled variable for
this database. All of the remaining plots for ratio features have most of the
data very close to 0. Thus, the differences between pattern and target graphs
are very small. Furthermore, all the distributions are symmetrical---the
pattern graph is just as likely to be larger/denser/etc. as the opposite.

\chapter{Machine Learning Models \& Their Evaluation}
After running the algorithms on all of the data for different types of labelling
and $p$ values, an ML algorithm can be trained to predict which algorithm should
be chosen for each pair of graphs. For each pair of graphs, \textsc{Llama}
\cite{llama} can take:

\begin{itemize}
\item A list of features. With separate features for pattern and target graphs
  as well as ratio features, we have 33 features in total.
\item A list of performance measures for all algorithms, i.e., the values that
  we are trying to optimise. In this case (as in most cases), this corresponds
  to running time. The values are capped at the timeout value (\num{1000000} ms).
  Furthermore, instances that were not run on the clique algorithm are also set
  to the timeout value. Finally, we filter out instances where all of the
  algorithms timed out.
\item A list of boolean values, denoting whether each algorithm successfully
  finished or not. Timeouts, the clique algorithm running out of memory, and
  instances that were not run with the clique algorithm because of their size
  are all marked as false.
\item A dataframe, measuring the running time taken to compute each feature for
  each problem instance, a single number for the approximate time taken to
  compute all features for any instance, or a list with one or more costs per
  problem instance. We use the last option with a single cost per instance. This
  parameter is used to ensure a fair comparison when comparing the portfolio
  against other algorithms: the runtime of the portfolio is defined as the
  runtime of its chosen algorithm together with the feature extraction time.
  While costs for graphs from Section \ref{sec:unlabelled} reached up to 65 s,
  the maximum cost for graphs from Section \ref{sec:labelled} is only 6 ms.
\end{itemize} % TODO: could expand the last sentence

After constructing the required dataframes as described above, the data needs to
be split into training and test sets. We use a technique called 10-fold
\emph{cross-validation}, which splits the data into 10 parts \cite{citeulike:1304145}.
9/10\textsuperscript{ths} of the data is used to train the ML algorithm, while
the remaining 1/10\textsuperscript{th} is used to evaluate how good the trained
model is. This process of training and evaluation is repeated 10 times, letting
each of the 10 parts be used for evaluation exactly once. The goodness-of-fit
criteria are then averaged out between the 10 runs.

The 10 folds could, of course, be chosen completely randomly. However, research
suggests that stratified cross-validation typically outperforms
random-sampling-based cross-validation and results in a better model
\cite{DBLP:conf/ijcai/Kohavi95}. Suppose we have a dataset of $N$ elements.
\emph{Stratified sampling} partitions it into a number of subpopulations $s_1,
\dots, s_n$ with $n_1, \dots, n_N$ elements, respectively (typically based on
the value of some feature or collection of features). It then draws from each
subpopulation independently, ensuring that approximately $n_i/N$ of the sample
comes from subpopulation $s_i$ for $i = 1, \dots, n$ \cite{lohr2009sampling}. In
this case the data is partitioned into four groups based on which algorithm
performed best.

The cross-validation folds are then used to generate predictions on all of the
data, where each prediction is made by a model that did not have that observation
in its training data set, and for various statistics provided by the
\textsc{Llama} package \cite{llama}. The predictions are then used to compare
the ML model with individual algorithms and the VBS using ECDF plots and numeric
statistics. \textsc{Llama} \cite{kotthoff_llama_2013, llama} supports algorithm
portfolios based on three different types of ML algorithms:

\begin{description}
  \item[Classification] The ML algorithm predicts which algorithm is likely to
    perform best on each problem instance.
  \item[Regression] Each algorithm's data is used to train a separate ML model,
    predicting the algorithm's performance. The winning algorithm can then be
    chosen based on those predictions.
  \item[Clustering] All instances of the training data are clustered and the
    best algorithm is determined for each cluster. New problem instances can
    then be assigned to the nearest cluster.
\end{description}

\begin{figure} % TODO: this is different for different cases. plot all of them
               % (or none)
  \centering
  \includegraphics[scale=0.7]{images/tree_sizes.png}
  \caption{A histogram of the number of nodes in each tree}
  \label{fig:tree_size}
\end{figure}

We are using a classification algorithm called random forests
\cite{DBLP:journals/ml/Breiman01} and its implementation in R
\cite{randomforest}. We chose this algorithm as it is recommended in the
\textsc{Llama} manual \cite{kotthoff_llama_2013} and successfully used in a
similar study \cite{DBLP:conf/lion/KotthoffMS16}. We use the default number of
trees (500), and the number of nodes per tree is plotted in Figure
\ref{fig:tree_size}.

In order to discuss and analyse the ML algorithm in more detail, we introduce
some new terminology. Each problem instance with features and running times in
the training dataset is called an \emph{observation}. The \emph{class} of an
observation is the algorithm with lowest running time for that problem instance.
Previously discussed features of graphs are sometimes referred to as
\emph{(independent) variables}. The \emph{(problem) instance space} is the
Cartesian product of the domains of features \cite{DBLP:series/smpai/RokachM14},
where a \emph{domain} of $X$ (denoted $\dom X$) is a set of all possible values
that $X$ can take.

A \emph{(classification) decision tree} is ``a classifier expressed as a
recursive partition of the instance space'' \cite{DBLP:series/smpai/RokachM14}.
Typically, it can be represented as a rooted binary tree, where each internal
node \emph{splits} the instance space into two regions based on the value of one
of the features. For example, for some feature $X$ and a particular value $x \in
\dom{X}$, the left child might be assigned all observations with $X < x$, while
the right child would get observations with $X \ge x$. For each leaf node, we
can count how many of its observations belong to each class and assign the most
highly represented class to that node.

\begin{remark}
  Other possibilities include a node having more than two children and a split
  being made in a more complicated way. Although trees with such properties fit
  the definition of a decision tree, standard machine learning algorithms are
  more restrictive \cite{James:2014:ISL:2517747, DBLP:series/smpai/RokachM14}.
\end{remark}

When a decision tree is used to make a classification prediction, a data point
travels from node to node (starting at the root node) according to the splitting
rules. When it reaches a leaf node, the class assigned to that node is outputted
as the tree's prediction.

A \emph{random forest} builds a collection of decision trees
\cite{James:2014:ISL:2517747}. Suppose we have $p$ variables. Then each time a
split is considered, the variable to split on is chosen from $\sqrt{p}$ rather
than $p$ variables. Therefore, the strongest predictors are sometimes not even
considered, ensuring a level of diversity among the trees. An individual tree's
prediction is called a \emph{vote}. A classifying random forest predicts by
collecting the votes from all of the trees and predicting the class with the
highest number of votes.

Lastly, we used the \texttt{parallelMap} package to train the model using
multiple threads. Furthermore, the R code was heavily optimised to remove
temporary variables as soon as they are no longer needed in order to reduce
memory consumption.

 % TODO: which is a strictly monotonic bijection for positive real numbers (define
 % bijection, monotonic)
As we saw in Section \ref{sec:features}, some of the feature data is highly
skewed. Thus we should formally address the question of using transformations
(such as $\log$ used for some of the plots or $x \mapsto 1/x$, giving
the labelling percentage a different interpretation) before feeding the
data into the ML algorithm. While it is a commonly held belief that
predictor transformations are unnecessary for decision tree-based learning
algorithms \cite{DBLP:journals/classification/Friedman06,
  DBLP:books/lib/HastieTF09, cart}, they can affect the predictions of
previously unseen data \cite{DBLP:journals/corr/GaliliM16}. To see this,
consider a split between some values $x_1, x_2 > 0$ at $b =
\frac{x_1+x_2}{2}$ (which is the type of split used by the
\texttt{randomForest} package we are using
\cite{DBLP:journals/corr/GaliliM16}) and consider the same situation after
applying the transformation $x \mapsto x^2$. The new boundary is $b' =
\frac{x_1^2+x_2^2}{2}$ and it is easy to find $x > b$ such that $x^2 < b'$.
Then, if this is the final split and $x_1$ and $x_2$ have different predictions,
$x$ will have different predictions with and without the transformation. In
this project we do not use transformations as having over \num{100000} data
points and 500 trees reduces this effect and good algorithm portfolios are
created without even mentioning transformations
\cite{DBLP:conf/lion/KotthoffKHT15, DBLP:conf/lion/KotthoffMS16}. However, we
would encourage future researchers to consider $\log$-transforming highly skewed
features, especially if using an algorithm more sensitive to the distribution of
data.

\section{Unlabelled Graphs}

\subsection{Error Rates} \label{sec:unlabelled_error_rates}

\begin{figure}
  \centering
  \includegraphics[scale=0.7]{images/unlabelled_forest_errors.png}
  \caption{Convergence plot of various error measures as the number of trees in
    a random forest increases. The plot shows the OOB error and $1 -
    \text{recall}$ for each algorithm.}
  \label{fig:unlabelled_forest_errors}
\end{figure}

Random forests support a convenient way to estimate the test error without
cross-validation or any other kind of data splitting. Each tree in a random
forest uses around 2/3 of the data \cite{James:2014:ISL:2517747}. The remaining
1/3 is referred to as \emph{out-of-bag (OOB)} observations . For each
observation in the data, we can predict the answer (the vote on which algorithm
is expected to win) using all trees that have the observation as OOB. The
majority vote is then declared to be the answer. The \emph{OOB error} is
the relative frequency of incorrect predictions \cite{James:2014:ISL:2517747}.
As each prediction was made using trees that had not seen that particular
observation before, OOB error is a valid estimate of test error. The black line
in Figure \ref{fig:unlabelled_forest_errors} shows how the error converges with
the number of trees to about 17\%.

The other lines in the figure, one for each algorithm, are defined as $1 -
\text{recall}$, where, for an algorithm $A$, \emph{recall}
\cite{citeulike:12882259} is
\[ \frac{\text{the number of instances that were correctly predicted as
      $A$}}{\text{the number of instances where $A$ is the correct
      prediction}}. \]

The error rates for $\textsc{McSplit}\downarrow$, \textsc{McSplit}, the clique
encoding, and $k\downarrow$ converge to 11\%, 29\%, 30\%, and 80\%,
respectively. Unlike the errors of all the other classes, the error of
$k\downarrow$ has an upward trend and converges to a very high value. Perhaps
the model eventually learns not to predict $k\downarrow$ very often and the data
points with $k\downarrow$ winning are treated as randomness in the data more so
than a statistically significant trend.

\subsection{Variable Importance}

Next we are going to explore how important each feature is in making
predictions, but for that we need to introduce some new definitions. Consider a
single tree $T$ in a random forest. The root of $T$ can be reached by any
observation, regardless of the values of its features. After passing some node
$n$, some feature is restricted, i.e., it is imposed an upper or lower limit on
the kind of values it can have for it to move towards a particular child of $n$
\cite{James:2014:ISL:2517747}. We will refer to a part of feature space that an
observation can be in while at some node $n$ as a \emph{region}.

\begin{definition}
  Suppose we have $K$ classes. Consider some region $m$. The \emph{Gini index}
  is then defined as
  \[ G = \sum_{k=1}^K \hat{p}_{mk}(1-\hat{p}_{mk}), \]
  where $\hat{p}_{mk}$ represents the proportion of observations in region $m$
  that are from class $k$ (i.e., have algorithm $k$ as the best algorithm)
  \cite{James:2014:ISL:2517747}.
\end{definition}

As we move down a tree, we want the region to be restricted to a single class.
Then the observations from the training data that satisfy the conditions imposed
by the parent nodes would be classified with perfect accuracy. The Gini index is
at its lowest when all the proportions $\hat{p}_{mk}$ are close to either 0 or
1, meaning that almost all the observations in the region belong to a single
class. Hence the Gini index is often used to evaluate the quality of a split.

\begin{remark}
  Note that $G=0$ when any single $\hat{p}_{mk}=0$, regardless of the values of
  other proportions. Therefore, $G=0$ does not automatically imply that the tree
  is a good classifier.
\end{remark}

\begin{figure}
  \centering
  \includegraphics{images/unlabelled_variable_importance.png}
  \caption{Dotchart of variable importance calculated based on the Gini index
    and sorted from most important to least important}
  \label{fig:unlabelled_variable_importance}
\end{figure}

\begin{figure}
  \centering
  \includegraphics{images/unlabelled_var_used.png}
  \caption{How often was each variable used to make splitting decisions?}
  \label{fig:unlabelled_var_used}
\end{figure}

The variable importance measure of feature $f$ in Figure
\ref{fig:unlabelled_variable_importance} is calculated as the amount by which
the Gini index decreases after passing nodes that use feature $f$, averaged over
all trees in the random forest \cite{James:2014:ISL:2517747}. Looking at the
figure more closely, the standard deviations of degrees of both target and
pattern graphs are by far the most important predictors. Unsurprisingly, the
worst predictors are the features with very low variance: number of loops and
connectedness of both graphs. Perhaps more surprisingly, the ratio features are
not as successful as one might have hoped: the ratio of the numbers of vertices
is at the bottom 5\textsuperscript{th} place and the best ratio feature, the
mean distance ratio, is only 10\textsuperscript{th}. Last thing to note is that
features of the pattern graph are always behind the same features of the target
graph and usually not far behind. Perhaps this is due to some datasets having
less pattern graphs, or pattern graphs having fewer vertices. The variable usage
plot in Figure \ref{fig:unlabelled_var_used} tells a similar story: the orders
are not identical, but there are no big outliers.

\subsection{Margins}

\begin{definition}
  Let $c_1, \dots, c_n$ be $n$ classes and let $p$ be a data point that belongs
  to class $c_p$. Let $v_1, \dots, v_n$ denote the number of votes for each
  class when given $p$ as input. The \emph{margin} of $p$ is
  \[ \frac{v_p}{\sum_{i=1}^n v_i} - \max_{i \ne p} \frac{v_i}{\sum_{j=1}^n v_j}, \]
  which is a number in $[-1, 1]$ \cite{forest}.
\end{definition}

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/unlabelled_margin.png}
    \caption{Sorted}
    \label{fig:unlabelled_margins1}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/unlabelled_margin2.png}
    \caption{Unsorted}
    \label{fig:unlabelled_margins2}
  \end{subfigure}
  \caption{Margins of all the data points}
  \label{fig:unlabelled_margins}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/clique_hist.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/kdown_hist.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/mcsplit_hist.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/mcsplitdown_hist.png}
  \end{subfigure}
  \caption{Histograms of margins for each winning algorithm}
  \label{fig:unlabelled_margin_hist}
\end{figure}

A value above 0 means that the forest as a whole predicted correctly. A margin
of 1 would mean that all trees voted correctly. In Figure
\ref{fig:unlabelled_margins} we plot the margins in the following way: for each
problem instance, the colour signifies the winning algorithm and the height
shows the margin. Figure \ref{fig:unlabelled_margins1} shows the points sorted
by the margin, while in Figure \ref{fig:unlabelled_margins2} they are left in
the original order of the data (which mostly corresponds to the order of files
in the databases, but with some variation since experiments are started in order
but end at different times). We can recognise the same error rates as in Figure
\ref{fig:unlabelled_forest_errors} as well as areas where \textsc{McSplit} and
$\textsc{McSplit}\downarrow$ dominate. We also plot the histograms of how the
margins are distributed for each algorithm in Figure
\ref{fig:unlabelled_margin_hist}. We note that:

\begin{itemize}
\item Instances best handled with the clique encoding are usually recognised, but
  with significant uncertainty.
\item We are usually wrong about $k\downarrow$ (probably because it is a
  winning algorithm in only 0.44\% of all cases).
\item When faced with an instance that is best handled with
  $\textsc{McSplit}\downarrow$, the vast majority of the trees vote correctly.
\item \textsc{McSplit} detection rates are decent, but far behind those of
  $\textsc{McSplit}\downarrow$.
\end{itemize}

\subsection{Partial Dependence} \label{sec:unlabelled_partial}

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/mcsplit_partial.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/clique_partial.png}
  \end{subfigure}
  \caption{Partial dependence plots of the standard deviation of degrees in the
    target graph}
  \label{fig:unlabelled_partials}
\end{figure}

Since the standard deviations of degrees in both target and pattern graphs are
the most important features, we plot partial dependence plots of the standard
deviation of degrees in the target graph for $\textsc{McSplit}\downarrow$ and
the clique encoding in Figure \ref{fig:unlabelled_partials}. The plotted
function \cite{forest} is defined as
\[ f(x) = \log{p_k(x)} - \frac{1}{K} \sum_{i=1}^K \log{p_i(x)}, \]
where:

\begin{itemize}
\item $x$ is the value on the horizontal axis (in this case standard deviation of
  degrees in the target graph),
\item $p_i(x)$ is the proportion of votes for class $i$ for a problem instance
  with a standard deviation of degrees in the target graph equal to $x$,
\item $K$ is the number of classes,
\item and $k$ is the main class under consideration
  ($\textsc{McSplit}\downarrow$ and the clique encoding).
\end{itemize}

Essentially, $f(x)$ compares the proportion of votes for class $k$ with the
average value over all classes. We can deduce that a low standard deviation of
degrees is a strong sign that \textsc{McSplit} and $\textsc{McSplit}\downarrow$
should perform well. On the other hand, the clique encoding is expected to
perform better on graphs with high variance in degrees. However, $\max f(x)$ for
the clique encoding is just barely above 0 and much lower than $\min f(x)$ for
$\textsc{McSplit}\downarrow$, meaning that the standard deviation of degrees
does not provide enough information to choose the clique encoding over
\textsc{McSplit} or $\textsc{McSplit}\downarrow$.

\begin{remark}
  We omit the plots for the other two algorithms as the plot for \textsc{McSplit}
  looks the same as the one for $\textsc{McSplit}\downarrow$ and prediction success
  rate for $k\downarrow$ is so low that a plot for $k\downarrow$ would be
  meaningless.
\end{remark}

\begin{remark}
  The plots for the standard deviation of degrees of the pattern graph are
  omitted since they are identical to those of the target graph.
\end{remark}

\subsection{Runtime Comparison}

\begin{figure}
  \centering
  \includegraphics{images/ecdf_unlabelled_llama.png}
  \caption{\textsc{Llama} model compared to the VBS and
    $\textsc{McSplit}\downarrow$}
  \label{fig:ecdf_unlabelled_llama}
\end{figure}

In order to compare the \textsc{Llama} model with other algorithms, we treat the
VBS as the upper bound and the single best solver $\textsc{McSplit}\downarrow$
as the lower bound. Out of 45468 instances solved by at least one algorithm, our
model managed to solve 45290, compared to 45223 solved by
$\textsc{McSplit}\downarrow$. In other words, it was able to close 27.3\% of the
gap between our lower and upper bounds in terms of instances solved within the
time limit. Figure \ref{fig:ecdf_unlabelled_llama} shows how the ML model
compares to the VBS and the single best solver $\textsc{McSplit}\downarrow$
(note that the vertical axis starts at 0.9 rather than 0). Unsurprisingly, the
model outperforms $\textsc{McSplit}\downarrow$, but does not reach the best
possible performance represented by the VBS.

\section{Vertex- and Edge-Labelled Graphs}

\begin{figure}
  \centering
  \includegraphics[scale=0.7]{images/both_labels_forest_errors.png}
  \caption{Convergence plot of various error measures as the number of trees in
    a random forest increases. The plot shows the OOB error and $1 -
    \text{recall}$ for each algorithm.}
  \label{fig:both_labels_forest_errors}
\end{figure}

Once again we plot the error rates (defined in Section
\ref{sec:unlabelled_error_rates}) in Figure \ref{fig:both_labels_forest_errors}.
This time all of the errors converge downwards and are below 50\%. The OOB error
converges to 14\%, while the errors of \textsc{McSplit},
$\textsc{McSplit}\downarrow$, and the clique encoding converge to 29\%, 11\%,
and 7\%, respectively.

\begin{figure}
  \centering
  \includegraphics{images/both_labels_variable_importance.png}
  \caption{A dotchart of variable importance, sorted from most to least important}
  \label{fig:both_labels_variable_importance}
\end{figure}

\begin{figure}
  \centering
  \includegraphics{images/both_labels_var_used.png}
  \caption{How often was each variable used to make splitting decisions?}
  \label{fig:both_labels_var_used}
\end{figure}

According to the variable importance measures plotted in Figure
\ref{fig:both_labels_variable_importance}, the standard deviation of degrees for
both pattern and target graphs still act as top predictors, however, they are
overshadowed by the labelling feature, which is to be expected considering how
impactful it is to the performance of the clique algorithm and the difficulty of
the problem in general. Another big change from the unlabelled subproblem is the
rise of vertex/edge counts as the next four most important predictors. Comparing
this with the variable usage statistic in Figure \ref{fig:both_labels_var_used},
the top 3 spots remain the same, the numbers of edges drop to the middle of the
list, and the numbers of vertices drop to the
6\textsuperscript{th}--7\textsuperscript{th} places from the bottom. Apparently,
even though all 4 of these predictors end up doing a great job at splitting the
data to reduce the Gini index (a variation of which is used by the random forest
algorithm in choosing which predictor to split on \cite{167153}), they are
rarely used.

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/both_labels_margin.png}
    \caption{Sorted}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/both_labels_margin2.png}
    \caption{Unsorted}
  \end{subfigure}
  \caption{Margins of all the data points}
  \label{fig:both_labels_margins}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/both_labels_clique_hist.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/both_labels_mcsplit_hist.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/both_labels_mcsplitdown_hist.png}
  \end{subfigure}
  \caption{Histograms of margins for each algorithm}
  \label{fig:both_labels_margin_hist}
\end{figure}

We show the margin scatter plots and histograms in Figures
\ref{fig:both_labels_margins} and \ref{fig:both_labels_margin_hist}. Note that
there are plenty of data points in all 3 colours and the ML model is usually
very convinced when predicting $\textsc{McSplit}\downarrow$ and the clique
encoding and is less sure when dealing with problem instances best handled with
\textsc{McSplit}.

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/_both_labels_clique_labelling_.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/_both_labels_mcsplit_labelling_.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/_both_labels_mcsplitdown_labelling_.png}
  \end{subfigure}
  \caption{Partial dependence plots of the labelling percentage}
  \label{fig:both_labels_partials2}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/_both_labels_clique_stddeg.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/_both_labels_mcsplit_stddeg.png}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/_both_labels_mcsplitdown_stddeg.png}
  \end{subfigure}
  \caption{Partial dependence plots of the standard deviation of degrees in the
    target graph}
  \label{fig:both_labels_partials1}
\end{figure}

This time we show the partial dependence plots for the top 2 most influential
predictors (the labelling percentage and the standard deviation of degrees in
the target graph) in Figures \ref{fig:both_labels_partials1} and
\ref{fig:both_labels_partials2}. The results for the clique encoding are the
most straightforward to interpret. Clearly, the algorithm performs much better
with higher labelling percentages (more different labels). The interesting bit
(just like back in Figure \ref{fig:both_labels_linecharts}) is that the curve
stays constant between 20\% and 50\%. In other words, anywhere above 20\%
labelling, a higher labelling percentage does not make the clique encoding more
preferable than it already is. The partial dependence on the standard deviation
of degrees of the target graph says what we already knew from Section
\ref{sec:unlabelled_partial}: the clique encoding prefers graphs with more
variance in degrees. The one smalll difference is that this time the change is
not as steep, but this can be easily explained by the fact that the highest
standard deviation of degrees is around 7 in this section compared to around 70
in Section \ref{sec:unlabelled_partial}.

On the other hand, both \textsc{McSplit} and $\textsc{McSplit}\downarrow$ prefer
lower labelling percentages. While $\textsc{McSplit}\downarrow$ prefers graphs
with smaller standard deviations of degrees, the situation with \textsc{McSplit}
is suddenly less clear, which is reflective of the fact that the ML model has less
confidence about \textsc{McSplit} (compared to the other 2 algorithms).

\begin{figure}
  \centering
  \includegraphics{images/ecdf_both_labels_llama.png}
  \caption{\textsc{Llama} model compared to the other algorithms and the VBS}
  \label{fig:ecdf_both_labels_llama}
\end{figure}

For the runtime comparison, we plot the ECDF for all the individual algorithms,
the VBS, and the \textsc{Llama} model in Figure
\ref{fig:ecdf_both_labels_llama}. Visually, the performance of the
\textsc{Llama} model is quite close to the upper bound and the gap between
\textsc{Llama} and any individual algorithm (like $\textsc{McSplit}\downarrow$)
is much wider than any gap between two individual algorithms. To add more
numbers to this picture, out of \num{169657} problem instances solved by at
least one algorithm, the single best algorithm $\textsc{McSplit}\downarrow$ was
able to solve \num{160879}, while \textsc{Llama} solved \num{168639}, closing
88\% of the difference between lower and upper bounds.

\section{Vertex-Labelled Graphs}

%\begin{appendices}
%\end{appendices}

\bibliographystyle{plain}
\bibliography{references}
\end{document}
