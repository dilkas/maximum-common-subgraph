\documentclass{beamer}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage[utf8]{inputenc}
\usepackage[UKenglish]{babel}
\usepackage[UKenglish]{isodate}
\usepackage[style=authoryear]{biblatex}
\usepackage{tikz}
\usepackage[clock]{ifsym}
\usepackage{adjustbox}
\usepackage{booktabs}
\usepackage{csquotes}
\usetikzlibrary{positioning, shapes.arrows}

% TODO: different theme
\usetheme{CambridgeUS}
\usecolortheme{orchid}
\beamertemplatenavigationsymbolsempty
\addbibresource{../dissertation/references.bib}
\author{Paulius Dilkas}
\title{Maximum Common Subgraph}
\subtitle{Algorithms and Algorithm Portfolios}
% TODO: change the date
\institute{School of Computing Science\\University of Glasgow}
% TODO: fix how school is formatted

\begin{document}

\maketitle

% TODO: fix sections
\begin{frame}{Outline}
  \tableofcontents
\end{frame}

\begin{frame}{Maximum Common Subgraph}
  \begin{definition}
    A \emph{maximum common (induced) subgraph} between graphs $G_1$ and
    $G_2$ is a graph $G_3$ such that $G_3 = (V_3, E_3)$ is isomorphic to induced
    subgraphs of both $G_1$ and $G_2$ with $|V_3|$ maximised.
  \end{definition}
\end{frame}

\section{Algorithms}

\begin{frame}{Algorithms}
  \begin{itemize}
  \item \textsc{McSplit}, $\textsc{McSplit}\downarrow$
    \begin{itemize}
    \item \parencite{DBLP:conf/ijcai/McCreeshPT17}
    \end{itemize}
  \item clique encoding
    \begin{itemize}
    \item \parencite{DBLP:conf/cp/McCreeshNPS16}
    \end{itemize}
  \item $k\downarrow$
    \begin{itemize}
    \item \parencite{DBLP:conf/aaai/HoffmannMR17}
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{\textsc{McSplit}: a Branch and Bound Algorithm}
  \begin{columns}
    \begin{column}{0.5\textwidth}
      \begin{adjustbox}{max totalsize={0.9\textwidth}{0.9\textheight},center}
        \begin{tikzpicture}
          \begin{scope}[every node/.style={circle,draw}]
            \node (u1) [fill=gray,] at (0, 5) {$u_1$};
            \node (u2) at (0, 3) {$u_2$};
            \node (u3) at (0, 1) {$u_3$};
            \node (u4) at (2, 4) {$u_4$};
            \node (u5) at (2, 2) {$u_5$};
          \end{scope}
          \node (name1) at (1, 0) {$G_1$};
          \path (u1) [color=blue,ultra thick] edge node {} (u2);
          \path (u1) edge node {} (u4);
          \path (u1) edge node {} (u5);
          \path (u2) edge node {} (u4);
          \path (u3) edge node {} (u4);
          \path (u3) edge node {} (u5);
          \begin{scope}[every node/.style={circle, draw},xshift=4cm]
            \node (v1) at (0, 5) {$v_1$};
            \node (v2) at (0, 3) {$v_2$};
            \node (v3) at (0, 1) {$v_3$};
            \node (v4) [fill=gray] at (2, 4) {$v_4$};
            \node (v5) [fill=gray] at (2, 2) {$v_5$};
          \end{scope}
          \node (name2) [xshift=4cm] at (1, 0) {$G_2$};
          \begin{scope}[color=blue,ultra thick]
            \path (v1) edge node {} (v2);
            \path (v1) edge node {} (v4);
            \path (v2) edge node {} (v4);
            \path (v2) edge node {} (v5);
          \end{scope}
          \begin{scope}[color=red,thick]
            \onslide<2->{\path[->] (u1) edge [bend left=40] (v4);}
            \onslide<8->{\path[->] (u2) edge [bend right=20] (v1);}
            \onslide<5->{\path[->] (u3) edge [] (v3);}
          \end{scope}
        \end{tikzpicture}
      \end{adjustbox}
    \end{column}

    \begin{column}{0.5\textwidth}
      \begin{overprint}
        Partial solution: \only<4>{\alert{$u_1 \mapsto v_4$}}\only<5-6>{$u_1 \mapsto v_4$}\only<7>{$u_1 \mapsto v_4$, \alert{$u_3 \mapsto v_3$}}\only<8->{$u_1 \mapsto v_4$, $u_3 \mapsto v_3$}

        Upper bound: \only<1-3>{$4$}\only<4>{\alert{$1+2$}}\only<5-6>{$1+2$}\only<7>{\alert{$2+1$}}\only<8->{$2+1$}
      \end{overprint}
      \begin{table}
        \begin{overlayarea}{\textwidth}{2cm}
          \centering
          \only<-2>{
            \begin{tabular}{c c c}
              \toprule
              Label & $G_1$ & $G_2$ \\
              \midrule
              0 & $u_2, u_3, u_4, u_5$ & $v_1, v_2, v_3$ \\
              1 & $u_1$ & $v_4, v_5$ \\
              \bottomrule
            \end{tabular}
          }
          \only<3>{
            \begin{tabular}{c c c}
              \toprule
              Label & $G_1$ & $G_2$ \\
              \midrule
              00 & $u_3$ & $v_3$ \\
              01 & $u_4, u_5$ & $\emptyset$ \\
              02 & $u_2$ & $v_1, v_2$ \\
              10 & $\emptyset$ & $v_5$ \\
              \bottomrule
            \end{tabular}
          }
          \only<4-5>{
            \begin{tabular}{c c c}
              \toprule
              Label & $G_1$ & $G_2$ \\
              \midrule
              00 & $u_3$ & $v_3$ \\
              01 & $u_2$ & $v_1, v_2$ \\
              \bottomrule
            \end{tabular}
          }
          \only<6>{
            \begin{tabular}{c c c}
              \toprule
              Label & $G_1$ & $G_2$ \\
              \midrule
              010 & $u_2$ & $v_1, v_2$ \\
              011 & $u_4, u_5$ & $\emptyset$ \\
              \bottomrule
            \end{tabular}
          }
          \only<7->{
            \begin{tabular}{c c c}
              \toprule
              Label & $G_1$ & $G_2$ \\
              \midrule
              010 & $u_2$ & $v_1, v_2$ \\
              \bottomrule
            \end{tabular}
          }
        \end{overlayarea}
      \end{table}
      \begin{overprint}
        \onslide<2>
        Decision: $u_1 \mapsto v_4$
        \onslide<5>
        Decision: $u_3 \mapsto v_3$
        \onslide<8>
        Decision: $u_2 \mapsto v_1$\\ Found a solution! \\ Backtrack to confirm optimality
      \end{overprint}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{The Clique Encoding}
    \begin{adjustbox}{max totalsize={0.9\textwidth}{0.9\textheight},center}
    \begin{tikzpicture}
      \begin{scope}[every node/.style={circle,draw}]
        \node (i1) at (360/11 * 1:7cm) {$\epsilon \to v_1$};
        \node (i2) at (360/11 * 2:7cm) {$\epsilon \to v_2$};
        \node (i3) [color=blue,ultra thick] at (360/11 * 3:7cm) {$\epsilon \to v_3$};

        \node (d1) at (360/11 * 4:7cm) {$u_1 \to \epsilon$};
        \node (s11) [color=blue,ultra thick] at (360/11 * 5:7cm) {$u_1 \to v_1$};
        \node (s12) at (360/11 * 6:7cm) {$u_1 \to v_2$};
        \node (s13) at (360/11 * 7:7cm) {$u_1 \to v_3$};

        \node (d2) at (360/11 * 8:7cm) {$u_2 \to \epsilon$};
        \node (s21) at (360/11 * 9:7cm) {$u_2 \to v_1$};
        \node (s22) [color=blue,ultra thick] at (360/11 * 10:7cm) {$u_2 \to v_2$};
        \node (s23) at (360:7cm) {$u_2 \to v_3$};
      \end{scope}

      %\begin{scope}[color=blue]
      %  \path (s11) [ultra thick] edge node {} (s22) edge node {} (i3);
      %  \path (s22) [ultra thick] edge node {} (i3);
      %\end{scope}

      \path (i1) edge node {} (i2) edge node {} (i3) edge node {} (d1) edge node {} (s12) edge node {} (s13) edge node {} (d2) edge node {} (s22) edge node {} (s23);
      \path (i2) edge node {} (i3) edge node {} (d1) edge node {} (s11) edge node {} (s13) edge node {} (d2) edge node {} (s21) edge node {} (s23);
      \path (i3) edge node {} (d1) edge node {} (s12) edge node {} (d2) edge node {} (s21);
      \path (d1) edge node {} (d2) edge node {} (s21) edge node {} (s22) edge node {} (s23);
      \path (s11) edge node {} (d2) edge node {} (s23);
      \path (s12) edge node {} (d2) edge node {} (s21) edge node {} (s23);
      \path (s13) edge node {} (d2) edge node {} (s21) edge node {} (s22);
    \end{tikzpicture}
  \end{adjustbox}
\end{frame}

\section{Algorithm selection}

\begin{frame}{Algorithm selection}
  \begin{definition}[\cite{DBLP:journals/ai/BischlKKLMFHHLT16}]
    Given a set $\mathcal{I}$ of problem instances, a space of algorithms
    $\mathcal{A}$, and a performance measure $m \colon \mathcal{I} \times
    \mathcal{A} \to \mathbb{R}$, the \emph{algorithm selection problem} is to
    find a mapping $s \colon \mathcal{I} \to \mathcal{A}$ that optimises
    $\mathbb{E}[m(i, s(i))]$.
  \end{definition}
  \pause
  \centering{\textsc{Llama} \parencite{kotthoff_llama_2013}}
  \begin{figure}
    \centering
    \includegraphics[scale=0.5]{llama.jpg}
  \end{figure}
\end{frame}

\section{Labelling}
\begin{frame}{Labelling}
  Data from \cite{foggia2001-2, DBLP:journals/prl/SantoFSV03} (81400 pairs
  of graphs)
  \pause
  \begin{definition}
    A \emph{vertex-labelled graph} is a 3-tuple $G = (V, E, \mu)$, where $\mu
    \colon V \to \{ 0, \dots, N - 1 \}$ is a vertex labelling function, for some
    $N \in \mathbb{N}$.
  \end{definition}
  \pause
  \begin{definition}
    A graph $G = (V, E, \mu)$ is said to have a \emph{$p\%$ (vertex) labelling} if
    \[ N = \max \left\{ 2^n : n \in \mathbb{N},\, 2^n < \left\lfloor \frac{p}{100\%}
          \times |V| \right\rfloor \right\}. \]
  \end{definition}
\end{frame}

\begin{frame}{Labelling}
  \begin{definition}
    A graph $G = (V, E, \mu)$ is said to have a \emph{$p\%$ (vertex) labelling} if
    \[ N = \max \left\{ 2^n : n \in \mathbb{N},\, 2^n < \left\lfloor \frac{p}{100\%}
          \times |V| \right\rfloor \right\}. \]
  \end{definition}
  \begin{itemize}
  \item 5\% labelling - 20 vertices per label on average
  \item 50\% labelling - 2 vertices per label on average
    \pause
  \item Typical values explored: 33\%, 50\%, 75\%
    \pause
  \item In my data: 5\%, 10\%, 15\%, 20\%, 25\%, 33\%, 50\%
    \pause
  \item 3 subproblems
    \begin{itemize}
    \item no labels
    \item vertex labels
    \item vertex and edge labels
    \end{itemize}
  \end{itemize}
\end{frame}

% \begin{frame}{Distribution of vertices per label}
%   \begin{figure}
%     \centering
%     \includegraphics[scale=0.4]{../dissertation/images/labelling_histogram.png}
%   \end{figure}
%   For each graph and label
%   \begin{itemize}
%   \item $C$ is the number of vertices with that label
%   \item $E(C)$ is the number we would expect from a (discrete) uniform distribution
%   \end{itemize}
% \end{frame}

\section{Features}
\begin{frame}{Features (34 in total)}
  1--8 are from \cite{DBLP:conf/lion/KotthoffMS16}
  \begin{enumerate}
  \item number of vertices
  \item number of edges
  \item mean/max degree
  \item density
  \item mean/max distance between pairs of vertices
  \item number of loops
  \item proportion of vertex pairs with distance $\ge$ 2, 3, 4
  \item connectedness
    \pause
  \item standard deviation of degrees
  \item labelling percentage
    \pause
  \item ratios of features 1--5
  \end{enumerate}
\end{frame}

\section{Random forests}

\begin{frame}{Random forests \parencite{DBLP:journals/ml/Breiman01}}
  \begin{figure}
    \centering
    \includegraphics[scale=0.5]{random_forests_2.png} \\
    {\tiny\color{gray}Source: Tae-Kyun Kim \& Bjorn Stenger, Intelligent Systems and Networks (ISN) Research Group,\\[-7pt] Imperial College London}
  \end{figure}
\end{frame}

\begin{frame}{Random forests \parencite{DBLP:journals/ml/Breiman01}}
  \begin{figure}
    \centering
    \includegraphics[scale=0.5]{rand-forest-1.jpg} \\
    {\tiny\color{gray}Source: Random Forests(r), Explained, Ilan Reinstein, KDnuggets}
  \end{figure}
\end{frame}

\section{Results}

\begin{frame}{Results}
  \begin{figure}
    \centering
    \includegraphics[width=\textwidth]{../dissertation/images/ecdf_unlabelled.png}
  \end{figure}
\end{frame}

\begin{frame}{Results (27\%)}
  \begin{figure}
    \centering
    \includegraphics[width=\textwidth]{../dissertation/images/ecdf_unlabelled_llama.png}
  \end{figure}
\end{frame}

\begin{frame}{Results}
  \begin{figure}
    \centering
    \includegraphics[width=\textwidth]{../dissertation/images/ecdf_vertex_labels.png}
  \end{figure}
\end{frame}

\begin{frame}{Results (86\%)}
  \begin{figure}
    \centering
    \includegraphics[width=\textwidth]{../dissertation/images/ecdf_vertex_labels_llama.png}
  \end{figure}
\end{frame}

\begin{frame}{Results}
  \begin{figure}
    \centering
    \includegraphics[width=\textwidth]{../dissertation/images/ecdf_both_labels.png}
  \end{figure}
\end{frame}

\begin{frame}{Results (88\%)}
  \begin{figure}
    \centering
    \includegraphics[width=\textwidth]{../dissertation/images/ecdf_both_labels_llama.png}
  \end{figure}
\end{frame}

% \begin{frame}{Other observations}
%   \begin{itemize}
%   \item Most important features
%     \begin{itemize}
%     \item labelling percentage
%     \item standard deviation of degrees (for both graphs)
%     \end{itemize}
%   \end{itemize}
% \end{frame}

\begin{frame}{Errors}
  \begin{itemize}
  \item Out-of-bag error
  \item For each algorithm
    \begin{itemize}
    \item $1 - \text{recall}$
    \end{itemize}
  \end{itemize}
  \begin{definition}
    For an algorithm $A$, \emph{recall} (sensitivity) is
    \[ \frac{\text{the number of instances that were correctly predicted as
          $A$}}{\text{the number of instances where $A$ is the correct
          prediction}}. \]
  \end{definition}
\end{frame}

\begin{frame}{Errors (\%)}
  \centering
  \begin{tabular}{l | c c c}
    \multirow{2}{*}{Error} & \multicolumn{3}{c}{Labelling} \\
                           & no & vertex & both \\
    \hline
    out-of-bag & 17 & 13 & 14 \\
    clique & 30 & 8 & 7 \\
    \textsc{McSplit} & 29 & 22 & 29 \\
    $\textsc{McSplit}\downarrow$ & 11 & 11 & 11 \\
    $k\downarrow$ & 80 & &
  \end{tabular}
\end{frame}

\begin{frame}{Convergence of errors for unlabelled graphs}
  \begin{figure}
    \centering
    \includegraphics[scale=0.5]{../dissertation/images/unlabelled_forest_errors.png}
  \end{figure}
\end{frame}

\section{What happens when labelling changes?}
\begin{frame}{What happens when labelling changes?} % total runtime is equivalent
  \begin{columns}
    \begin{column}{0.5\textwidth}
      \begin{figure}
        \centering
        \includegraphics[width=\textwidth]{../dissertation/images/vertex_labels_linechart.png}
        \visible<2>{\includegraphics[width=\textwidth]{../dissertation/images/vertex_labels_linechart3.png}}
      \end{figure}
    \end{column}
    \begin{column}{0.5\textwidth}
      \begin{figure}
        \centering
        \includegraphics[width=\textwidth]{../dissertation/images/both_labels_linechart.png}
        \visible<2>{\includegraphics[width=\textwidth]{../dissertation/images/both_labels_linechart3.png}}
      \end{figure}
    \end{column}
  \end{columns}
\end{frame}

\section{Future work}
\begin{frame}{Future work}
  \begin{itemize}
  \item Relationships between clique algorithm's performance and properties of
    the association graph
  \item How the association graph changes after making a decision
  \item Can $k\downarrow$ and clique work together?
  \end{itemize}
\end{frame}

% \begin{frame}{Margins}
%   \begin{definition}
%     Let $c_1, \dots, c_n$ be $n$ classes and let $p$ be a data point that belongs
%     to class $c_p$. Let $v_1, \dots, v_n$ denote the number of votes for each
%     class when given $p$ as input. The \emph{margin} of $p$ is
%     \[ \frac{v_p}{\sum_{i=1}^n v_i} - \max_{i \ne p} \frac{v_i}{\sum_{j=1}^n v_j}, \]
%     which is a number in $[-1, 1]$.
%   \end{definition}
% \end{frame}

\end{document}